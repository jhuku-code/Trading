{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "086cee3c-7b69-4e24-80b8-a3c60d204f46",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "058b23f5-6cf3-4fbb-aafe-1d16f7662e2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "#import cryptocompare\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.drawing.image import Image\n",
    "from itertools import combinations\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import ccxt\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dd9c24",
   "metadata": {},
   "source": [
    "Coinbase BTC Premium Index: https://www.coinglass.com/pro/i/coinbase-bitcoin-premium-index\n",
    "\n",
    "BTC Liquidation Map: https://www.coinglass.com/pro/futures/LiquidationMap\n",
    "\n",
    "Perp futures to spot volume: https://www.coinglass.com/pro/perpteual-spot-volume\n",
    "\n",
    "BTC 25 Delta Skew: https://www.theblock.co/data/crypto-markets/options/btc-option-skew-delta-25\n",
    "\n",
    "Crypto Greed Fear Index: https://alternative.me/crypto/fear-and-greed-index/\n",
    "\n",
    "Volmex BTC Volatility: https://charts.volmex.finance/symbol/BVIV\n",
    "\n",
    "Funding Rate Heatmap: https://www.coinglass.com/FundingRateHeatMap\n",
    "\n",
    "Liquidations Ratio and OI/Market Cap from Coinalyze.net\n",
    "\n",
    "Bitcoin Dominance Chart: https://in.tradingview.com/chart/7cGHhC4a/?symbol=CRYPTOCAP%3ABTC.D\n",
    "\n",
    "Meme Marketcap Chart: https://in.tradingview.com/chart/7cGHhC4a/?symbol=CRYPTOCAP%3AMEME.C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1d88d5-ac0f-43f2-a743-180b0fccd033",
   "metadata": {},
   "source": [
    "# Sentiment Tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d3b435-0f1b-4379-8942-d014400d5377",
   "metadata": {},
   "source": [
    "### Messari Sentiment Signals\n",
    "https://messari.io/signals?filter=curated&change=absolute&hide=wrapped-coins,stablecoins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a81172-3d1b-4ed3-a65c-357b520834fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55eb54c-ffe0-4c74-932b-3cb9fff32d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49f4043-d268-46df-8c1b-52ef336f3dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71f85cd9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Themes Tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713f396a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Daily performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ba034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set the dates\n",
    "start_dt = \"2024-12-31\"  ## yyyy-mm-dd format\n",
    "end_dt = \"2025-11-10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fac5eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the list of coins and themes\n",
    "\n",
    "# Path to the Excel file\n",
    "excel_file_path = r'C:\\Users\\jhuku\\OneDrive\\Documents\\2. Crypto Research\\Crypto Portfolio\\Themes_mapping.xlsx'\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "df = pd.read_excel(excel_file_path)\n",
    "\n",
    "\n",
    "## Extract the 'symbol' column and save it to the theme_list and theme column to themes\n",
    "theme_list = df['Symbol'].tolist()\n",
    "themes = df['Theme'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73d2273",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Map to Binance trading pairs\n",
    "binance_pairs = [f\"{sym}/USDT\" for sym in theme_list]\n",
    "\n",
    "# Filter valid pairs\n",
    "exchange = ccxt.kucoin()   ## binance was not working hence used kucoin\n",
    "markets = exchange.load_markets()\n",
    "available_pairs = set(markets.keys())\n",
    "valid_pairs = [pair for pair in binance_pairs if pair in available_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af722c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b10f1e9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fetch daily close prices\n",
    "timeframe = '1d'\n",
    "limit = 90\n",
    "sleep_seconds = 0.2\n",
    "\n",
    "import time\n",
    "\n",
    "frames = []\n",
    "\n",
    "for pair in valid_pairs:\n",
    "    try:\n",
    "        base = pair.split('/')[0]  # e.g. BTC\n",
    "        ohlcv = exchange.fetch_ohlcv(pair, timeframe=timeframe, limit=limit)\n",
    "        df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "        df = df[['timestamp', 'close']].rename(columns={'close': base})\n",
    "        frames.append(df.set_index('timestamp'))\n",
    "        time.sleep(sleep_seconds)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {pair}: {e}\")\n",
    "\n",
    "# Merge all into wide format\n",
    "price_theme = pd.concat(frames, axis=1).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9e871a",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_theme.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0567351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only show columns with at least one null - CHECK to see if there any null\n",
    "null_columns = price_theme.columns[price_theme.isnull().any()]\n",
    "print(null_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a163d8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop columns where all rows are NaN\n",
    "price_theme = price_theme.dropna(axis=1, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b916634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only show columns with at least one null - CHECK to see if there any null\n",
    "null_columns = price_theme.columns[price_theme.isnull().any()]\n",
    "print(null_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221007c6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Calculate Returns over Different Lookback periods\n",
    "\n",
    "# Lookback periods (in days)\n",
    "lookbacks = [1, 3, 5, 10, 15, 30, 60]\n",
    "\n",
    "# Choose a specific date to calculate returns up to (e.g. latest date)\n",
    "target_date = price_theme.index[-1]\n",
    "\n",
    "# Prepare output\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca0dad9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for lb in lookbacks:\n",
    "    current = price_theme.loc[target_date]\n",
    "    past_date = target_date - pd.Timedelta(days=lb)\n",
    "\n",
    "    # Find the closest available past date\n",
    "    if past_date in price_theme.index:\n",
    "        past = price_theme.loc[past_date]\n",
    "    else:\n",
    "        # Use the most recent available date before target\n",
    "        past = price_theme[:target_date].iloc[-lb]\n",
    "\n",
    "    returns = (current - past) / past\n",
    "    results[f'{lb}d'] = returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8bc214",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Final output\n",
    "returns_df = pd.DataFrame(results)\n",
    "returns_df.index.name = 'Coin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a710b55b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a mapping from original ticker to theme (w/o deletions)\n",
    "ticker_to_theme = dict(zip(theme_list, themes))\n",
    "remaining_tickers = returns_df.T.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b028912",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now map themes for remaining tickers\n",
    "theme_values = [ticker_to_theme[ticker] for ticker in remaining_tickers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1794300b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Adding back the Theme column to the dataframe\n",
    "returns_df['Theme'] = theme_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddec8ac",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "returns_df['Coin'] = returns_df.index  ## Adding the names of Coins as a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8961627c",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e32615",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Calculate Excess Return (vs. group avg) for each coin\n",
    "\n",
    "# Check for duplicates in the data first\n",
    "dupes = returns_df[returns_df.duplicated(subset=['Coin', 'Theme'], keep=False)]\n",
    "print(dupes)\n",
    "\n",
    "## Remove Duplicates\n",
    "returns_df = returns_df.drop_duplicates(subset=['Coin', 'Theme'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df250a82",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STEP 1: Base setup - calculating average return for each theme & excess return of each coin vs. category average\n",
    "lookback_cols = ['1d', '3d', '5d', '10d', '15d', '30d', '60d']\n",
    "\n",
    "df_long = returns_df.melt(id_vars=['Coin', 'Theme'], value_vars=lookback_cols,\n",
    "                          var_name='Period', value_name='Return')\n",
    "\n",
    "df_long['Theme_Avg'] = df_long.groupby(['Theme', 'Period'])['Return'].transform('median')\n",
    "df_long['Excess_Return'] = df_long['Return'] - df_long['Theme_Avg']\n",
    "\n",
    "returns_wide = df_long.pivot(index=['Coin', 'Theme'], columns='Period', values='Return')\n",
    "excess_wide = df_long.pivot(index=['Coin', 'Theme'], columns='Period', values='Excess_Return')\n",
    "\n",
    "returns_wide.columns = [f\"{col}\" for col in returns_wide.columns]\n",
    "excess_wide.columns = [f\"{col}_Excess\" for col in excess_wide.columns]\n",
    "\n",
    "final_df = pd.concat([returns_wide, excess_wide], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6218821c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STEP 2: Multiply by 100 and round\n",
    "final_df = final_df * 100\n",
    "final_df = final_df.round(2)\n",
    "\n",
    "# STEP 3: Reorder columns\n",
    "sorted_cols = []\n",
    "for col in lookback_cols:\n",
    "    sorted_cols.append(col)\n",
    "    sorted_cols.append(f\"{col}_Excess\")\n",
    "final_df = final_df[sorted_cols]\n",
    "\n",
    "# STEP 4: Reset and sort by Theme + Coin\n",
    "final_df = final_df.reset_index()\n",
    "final_df = final_df.sort_values(by=['Theme', 'Coin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328f6d70",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STEP 5: Add Theme Average Rows with Coin = '<Theme>_average'\n",
    "# For each theme, Excess return is calculated vs. Global Average\n",
    "\n",
    "# Theme-wise average returns\n",
    "theme_avg_returns = returns_df.groupby('Theme')[lookback_cols].mean() * 100\n",
    "theme_avg_returns = theme_avg_returns.round(2)\n",
    "\n",
    "# Global average returns\n",
    "global_avg = returns_df[lookback_cols].mean() * 100\n",
    "global_avg = global_avg.round(2)\n",
    "\n",
    "# Excess = Theme avg - Global avg\n",
    "theme_excess = theme_avg_returns.subtract(global_avg, axis=1)\n",
    "theme_excess.columns = [f\"{col}_Excess\" for col in theme_excess.columns]\n",
    "\n",
    "# Combine\n",
    "theme_combined = pd.concat([theme_avg_returns, theme_excess], axis=1)\n",
    "\n",
    "# Add 'Coin' = '<Theme>_average'\n",
    "theme_combined['Coin'] = theme_combined.index + '_average'\n",
    "theme_combined = theme_combined.reset_index()  # Keeps 'Theme' as a column\n",
    "\n",
    "# Reorder columns to match final_df\n",
    "theme_combined = theme_combined[['Coin', 'Theme'] + sorted_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cbff98",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STEP 6: Append to final_df\n",
    "final_df = pd.concat([final_df, theme_combined], ignore_index=True)\n",
    "\n",
    "# Sort to group properly again\n",
    "final_df = final_df.sort_values(by=['Theme', 'Coin']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f285baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe0df06",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save to Excel File on OneDrive\n",
    "file_path = r'C:\\Users\\jhuku\\OneDrive\\Documents\\2. Crypto Research\\Crypto Portfolio\\Output.xlsx'\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(file_path):\n",
    "    # File does NOT exist: create new workbook with the first sheet\n",
    "    with pd.ExcelWriter(file_path, engine='openpyxl', mode='w') as writer:\n",
    "        final_df.to_excel(writer, sheet_name='ThemeData', index=False)\n",
    "else:\n",
    "    # File EXISTS: append or replace sheet as needed\n",
    "    with pd.ExcelWriter(file_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "        final_df.to_excel(writer, sheet_name='ThemeData', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7389ea7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "800c6d10",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### 1H Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd3b489",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Read the list of coins and themes\n",
    "\n",
    "# Path to the Excel file\n",
    "excel_file_path = r'C:\\Users\\jhuku\\OneDrive\\Documents\\2. Crypto Research\\Crypto Portfolio\\Themes_mapping.xlsx'\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "df = pd.read_excel(excel_file_path)\n",
    "\n",
    "\n",
    "## Extract the 'symbol' column and save it to the theme_list and theme column to themes\n",
    "theme_list = df['Symbol'].tolist()\n",
    "themes = df['Theme'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfccbc7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Extract the 'symbol' column and save it to the theme_list and theme column to themes\n",
    "theme_list = df['Symbol'].tolist()\n",
    "themes = df['Theme'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88419869",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Map to Binance trading pairs\n",
    "binance_pairs = [f\"{sym}/USDT\" for sym in theme_list]\n",
    "\n",
    "# Filter valid pairs\n",
    "exchange = ccxt.kucoin()   ## binance was not working hence used kucoin\n",
    "markets = exchange.load_markets()\n",
    "available_pairs = set(markets.keys())\n",
    "valid_pairs = [pair for pair in binance_pairs if pair in available_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d696d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch hourly close prices\n",
    "timeframe = '1h'\n",
    "limit = 24\n",
    "sleep_seconds = 0.2\n",
    "\n",
    "import time\n",
    "\n",
    "frames = []\n",
    "\n",
    "for pair in valid_pairs:\n",
    "    try:\n",
    "        base = pair.split('/')[0]  # e.g. BTC\n",
    "        ohlcv = exchange.fetch_ohlcv(pair, timeframe=timeframe, limit=limit)\n",
    "        df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "        df = df[['timestamp', 'close']].rename(columns={'close': base})\n",
    "        frames.append(df.set_index('timestamp'))\n",
    "        time.sleep(sleep_seconds)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {pair}: {e}\")\n",
    "\n",
    "# Merge all into wide format\n",
    "price_theme_h = pd.concat(frames, axis=1).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97c4ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only show columns with at least one null - CHECK to see if there any null\n",
    "null_columns = price_theme_h.columns[price_theme_h.isnull().any()]\n",
    "print(null_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848ac55c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop columns where all rows are NaN\n",
    "price_theme_h = price_theme_h.dropna(axis=1, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94295f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only show columns with at least one null - CHECK to see if there any null\n",
    "null_columns = price_theme_h.columns[price_theme_h.isnull().any()]\n",
    "print(null_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbc46ff",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Calculate Returns over different time frames/Lookback period \n",
    "\n",
    "# Lookback periods (in 15mins)\n",
    "lookbacks_h = [1, 2, 3, 4, 6]\n",
    "\n",
    "# Choose a specific date to calculate returns up to (e.g. latest date)\n",
    "target_date_h = price_theme_h.index[-1]\n",
    "\n",
    "# Prepare output\n",
    "results_h = {}\n",
    "\n",
    "\n",
    "for lb in lookbacks_h:\n",
    "    current_h = price_theme_h.loc[target_date_h]\n",
    "    past_date_h = target_date_h - pd.Timedelta(days=lb)\n",
    "\n",
    "    # Find the closest available past date\n",
    "    if past_date_h in price_theme_h.index:\n",
    "        past_h = price_theme_h.loc[past_date_h]\n",
    "    else:\n",
    "        # Use the most recent available date before target\n",
    "        past_h = price_theme_h[:target_date_h].iloc[-lb]\n",
    "\n",
    "    returns_h = (current_h - past_h) / past_h\n",
    "    results_h[f'{lb}min'] = returns_h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f25acc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final output\n",
    "returns_df_h = pd.DataFrame(results_h)\n",
    "returns_df_h.index.name = 'Coin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5197bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from original ticker to theme (w/o deletions)\n",
    "ticker_to_theme = dict(zip(theme_list, themes))\n",
    "remaining_tickers = returns_df_h.T.columns.to_list()\n",
    "\n",
    "# Now map themes for remaining tickers\n",
    "theme_values_h = [ticker_to_theme[ticker] for ticker in remaining_tickers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb51300",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add as a new row or column\n",
    "returns_df_h['Theme'] = theme_values_h\n",
    "\n",
    "returns_df_h['Coin'] = returns_df_h.index  ## Adding the names of Coins as a column\n",
    "\n",
    "returns_df_h.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf09667",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Excess Return (vs. group avg) for each coin\n",
    "\n",
    "# STEP 1: Base setup - calculating average return for each theme & excess return of each coin vs. category average\n",
    "lookback_cols_h = ['1min', '2min', '3min', '4min', '6min']\n",
    "\n",
    "# Check for duplicates in the data first\n",
    "dupes = returns_df_h[returns_df_h.duplicated(subset=['Coin', 'Theme'], keep=False)]\n",
    "print(dupes)\n",
    "\n",
    "## Remove Duplicates\n",
    "returns_df_h = returns_df_h.drop_duplicates(subset=['Coin', 'Theme'])\n",
    "\n",
    "\n",
    "df_long_h = returns_df_h.melt(id_vars=['Coin', 'Theme'], value_vars=lookback_cols_h,\n",
    "                          var_name='Period', value_name='Return')\n",
    "\n",
    "df_long_h['Theme_Avg'] = df_long_h.groupby(['Theme', 'Period'])['Return'].transform('median')\n",
    "df_long_h['Excess_Return'] = df_long_h['Return'] - df_long_h['Theme_Avg']\n",
    "\n",
    "returns_wide_h = df_long_h.pivot(index=['Coin', 'Theme'], columns='Period', values='Return')\n",
    "excess_wide_h = df_long_h.pivot(index=['Coin', 'Theme'], columns='Period', values='Excess_Return')\n",
    "\n",
    "returns_wide_h.columns = [f\"{col}\" for col in returns_wide_h.columns]\n",
    "excess_wide_h.columns = [f\"{col}_Excess\" for col in excess_wide_h.columns]\n",
    "\n",
    "final_df_h = pd.concat([returns_wide_h, excess_wide_h], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6217166",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STEP 2: Multiply by 100 and round\n",
    "final_df_h = final_df_h * 100\n",
    "final_df_h = final_df_h.round(2)\n",
    "\n",
    "# STEP 3: Reorder columns\n",
    "sorted_cols = []\n",
    "for col in lookback_cols_h:\n",
    "    sorted_cols.append(col)\n",
    "    sorted_cols.append(f\"{col}_Excess\")\n",
    "final_df_h = final_df_h[sorted_cols]\n",
    "\n",
    "# STEP 4: Reset and sort by Theme + Coin\n",
    "final_df_h = final_df_h.reset_index()\n",
    "final_df_h = final_df_h.sort_values(by=['Theme', 'Coin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7947d42d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STEP 5: Add Theme Average Rows with Coin = '<Theme>_average'\n",
    "# For each theme, Excess return is calculated vs. Global Average\n",
    "\n",
    "# Theme-wise average returns\n",
    "theme_avg_returns_h = returns_df_h.groupby('Theme')[lookback_cols_h].mean() * 100\n",
    "theme_avg_returns_h = theme_avg_returns_h.round(2)\n",
    "\n",
    "# Global average returns\n",
    "global_avg_h = returns_df_h[lookback_cols_h].mean() * 100\n",
    "global_avg_h = global_avg_h.round(2)\n",
    "\n",
    "# Excess = Theme avg - Global avg\n",
    "theme_excess_h = theme_avg_returns_h.subtract(global_avg_h, axis=1)\n",
    "theme_excess_h.columns = [f\"{col}_Excess\" for col in theme_excess_h.columns]\n",
    "\n",
    "# Combine\n",
    "theme_combined_h = pd.concat([theme_avg_returns_h, theme_excess_h], axis=1)\n",
    "\n",
    "# Add 'Coin' = '<Theme>_average'\n",
    "theme_combined_h['Coin'] = theme_combined_h.index + '_average'\n",
    "theme_combined_h = theme_combined_h.reset_index()  # Keeps 'Theme' as a column\n",
    "\n",
    "# Reorder columns to match final_df_h\n",
    "theme_combined_h = theme_combined_h[['Coin', 'Theme'] + sorted_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd178ade",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STEP 6: Append to final_df_h\n",
    "final_df_h = pd.concat([final_df_h, theme_combined_h], ignore_index=True)\n",
    "\n",
    "# Sort to group properly again\n",
    "final_df_h = final_df_h.sort_values(by=['Theme', 'Coin']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3972e517",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_h.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ee2275",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Change the column names to meaningful ones\n",
    "final_df_h.columns = ['Coin', 'Theme', '1H', '1H_Excess', '2H', '2H_Excess', '3H', '3H_Excess', '4H', '4H_Excess', '6H', '6H_Excess']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b73a122",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save to Excel File on GoogleDrive\n",
    "file_path = r'C:\\Users\\jhuku\\OneDrive\\Documents\\2. Crypto Research\\Crypto Portfolio\\Output.xlsx'\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(file_path):\n",
    "    # File does NOT exist: create new workbook with the first sheet\n",
    "    with pd.ExcelWriter(file_path, engine='openpyxl', mode='w') as writer:\n",
    "        final_df_h.to_excel(writer, sheet_name='ThemeData_15min', index=False)\n",
    "else:\n",
    "    # File EXISTS: append or replace sheet as needed\n",
    "    with pd.ExcelWriter(file_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "        final_df_h.to_excel(writer, sheet_name='ThemeData_15min', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6789e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "016a41ce-af38-4889-83cc-fa3e630cc1dd",
   "metadata": {},
   "source": [
    "### Themes TS Cum. Perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293a45a7-6e24-43bf-a5a0-3a74774e6af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use price_theme as price data source as it has data for last 90 days\n",
    "price_ts = price_theme.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c14e6e-11d8-4167-9b45-321b595947f8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "returns_ts = price_ts.pct_change().dropna()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7744d31-7b0f-426b-8d28-39d569c9912d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tickers_ts = returns_ts.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cf1da9-93f3-4a12-a3e7-be7699986db7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "returns_ts_T = returns_ts.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec545c16-e6fe-4938-83b2-55f0afd844e9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Adding back the Theme column to the dataframe\n",
    "returns_ts_T['Theme'] = theme_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef189683-5a71-4b4b-be17-23a46f56e3cd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Generating dataframes for each theme\n",
    "def create_theme_dataframes_and_chart(returns_ts_T):\n",
    "    \"\"\"\n",
    "    Create separate dataframes for each theme and generate cumulative returns chart\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get unique themes\n",
    "    themes = returns_ts_T['Theme'].unique()\n",
    "    \n",
    "    # Get date columns (all columns except 'Theme')\n",
    "    date_columns = [col for col in returns_ts_T.columns if col != 'Theme']\n",
    "    \n",
    "    # Dictionary to store individual theme dataframes\n",
    "    theme_dataframes = {}\n",
    "    \n",
    "    # Create separate dataframe for each theme\n",
    "    for theme in themes:\n",
    "        # Filter data for current theme\n",
    "        theme_data = returns_ts_T[returns_ts_T['Theme'] == theme]\n",
    "        \n",
    "        # Calculate median for each date column\n",
    "        median_values = theme_data[date_columns].median()\n",
    "        \n",
    "        # Create dataframe with dates as columns and theme as index\n",
    "        df_theme = pd.DataFrame([median_values], index=[theme])\n",
    "        df_theme.columns = date_columns\n",
    "        \n",
    "        # Store in dictionary with naming convention df_theme\n",
    "        theme_dataframes[f'df_{theme.lower().replace(\" \", \"_\")}'] = df_theme\n",
    "        \n",
    "   \n",
    "    # Consolidate all theme dataframes into a single dataframe\n",
    "    consolidated_df = pd.concat(theme_dataframes.values(), ignore_index=False)\n",
    "    \n",
    " \n",
    "    # Create cumulative returns chart starting at 100\n",
    "    # Convert to numeric and handle any non-numeric values\n",
    "    numeric_df = consolidated_df.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Calculate cumulative returns starting at 100\n",
    "    # Assuming the data represents returns or price changes\n",
    "    cumulative_returns = pd.DataFrame(index=numeric_df.index, columns=numeric_df.columns)\n",
    "    \n",
    "    for theme in numeric_df.index:\n",
    "        # Convert returns to cumulative returns starting at 100\n",
    "        theme_data = numeric_df.loc[theme]\n",
    "        \n",
    "        # If data represents returns (percentage changes), use cumprod\n",
    "        # If data represents prices, use direct values\n",
    "        # Assuming returns data - adjust as needed\n",
    "        cumulative_returns.loc[theme] = (1 + theme_data/100).cumprod() * 100\n",
    "    \n",
    "    return theme_dataframes, consolidated_df, cumulative_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b2470b-5f8e-4e0f-b9e9-8ae7d5e6bb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_dataframes, consolidated_df, cumulative_returns = create_theme_dataframes_and_chart(returns_ts_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602fd67c-edc7-49e7-80f3-a2ab4c8f270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_dataframes['df_dex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60534b6-b654-4af9-98ba-e3380c266494",
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a762241f-e7a5-42d3-9948-962166cdcf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b7f0ac-0b0a-4494-9347-2926355ff133",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_returns_T = cumulative_returns.T\n",
    "\n",
    "## Adding the index as a column so that it gets printed in export\n",
    "cumulative_returns_T['Date'] = cumulative_returns_T.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd3cab8-ab05-4475-92d2-cdddc5dafcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save to Excel File on OneDrive\n",
    "file_path = r'C:\\Users\\jhuku\\OneDrive\\Documents\\2. Crypto Research\\Crypto Portfolio\\Output.xlsx'\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(file_path):\n",
    "    # File does NOT exist: create new workbook with the first sheet\n",
    "    with pd.ExcelWriter(file_path, engine='openpyxl', mode='w') as writer:\n",
    "        cumulative_returns_T.to_excel(writer, sheet_name='ThemeCumPerf', index=False)\n",
    "else:\n",
    "    # File EXISTS: append or replace sheet as needed\n",
    "    with pd.ExcelWriter(file_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "        cumulative_returns_T.to_excel(writer, sheet_name='ThemeCumPerf', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60253a36-e848-463e-b45c-267475bfb770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71545b24",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Multi-Strategy Trading Signals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7bb068-d39e-4093-a02d-195351a15a11",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4H Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7087de2-c81f-44ff-95ba-02a69e69331d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fetch 4H close prices for 180 days\n",
    "timeframe = '4h'\n",
    "limit = 1080\n",
    "sleep_seconds = 0.2\n",
    "\n",
    "import time\n",
    "\n",
    "frames = []\n",
    "\n",
    "for pair in valid_pairs:\n",
    "    try:\n",
    "        base = pair.split('/')[0]  # e.g. BTC\n",
    "        ohlcv = exchange.fetch_ohlcv(pair, timeframe=timeframe, limit=limit)\n",
    "        df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "        df = df[['timestamp', 'close']].rename(columns={'close': base})\n",
    "        frames.append(df.set_index('timestamp'))\n",
    "        time.sleep(sleep_seconds)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {pair}: {e}\")\n",
    "\n",
    "# Merge all into wide format\n",
    "price_h = pd.concat(frames, axis=1).sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9175d3-21ad-4aaf-8124-147686c77a96",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop columns where all rows are NaN\n",
    "price_h = price_h.dropna(axis=1, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abed6805-9de0-43ad-b6b6-a29d0f632518",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only show columns with at least one null - CHECK to see if there any null\n",
    "null_columns = price_h.columns[price_h.isnull().any()]\n",
    "print(null_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b045ed-c26a-42ec-9789-506025d86432",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_h = price_h.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da40fe06-8ef5-49b0-b2f8-a6f276238cbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9218d5e7-44a3-40db-8430-fb8e394c2252",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1129bf41-15a7-4d4d-b4e9-dcee7212b82c",
   "metadata": {},
   "source": [
    "## Altcoin Diffusion Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cfd7b5-40a9-43ce-8528-b6a87c40fca6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_altcoin_diffusion_index(prices, window=180):\n",
    "    \"\"\"\n",
    "    Calculates the altcoin diffusion index over a given rolling window.\n",
    "\n",
    "    :param prices: pd.DataFrame with columns as coin tickers and dates as index\n",
    "    :param window: Lookback window in days for measuring outperformance\n",
    "    :return: pd.Series of diffusion index values\n",
    "    \"\"\"\n",
    "    returns = prices.pct_change(window)\n",
    "    btc_returns = returns['BTC']\n",
    "\n",
    "    # Exclude BTC itself for index calculation\n",
    "    altcoins = [col for col in returns.columns if col != 'BTC']\n",
    "    outperform = (returns[altcoins].gt(btc_returns, axis=0)).sum(axis=1)\n",
    "    diffusion_index = 100 * outperform / len(altcoins)\n",
    "    return diffusion_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f297f1-3972-427a-b96c-d892eccf4673",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_index = calc_altcoin_diffusion_index(df_h, window=90)\n",
    "diffusion_index.plot(title='Altcoin Diffusion Index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b845ecc-c52e-4dff-8ad7-464a16e516f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "baabcfa3-1dd1-4c2e-961a-12ba236dbe7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Theme Rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c24150b-fa34-4554-a215-d432e2007a49",
   "metadata": {},
   "source": [
    "### Theme Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4ca1a0-79bc-4e79-81df-fd4f82361349",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_tickers_h = df_h.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f3c6a3-6166-4eae-9871-16f626c7a49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now map themes for remaining tickers\n",
    "theme_values_h = [ticker_to_theme[ticker] for ticker in remaining_tickers_h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714ad64b-8060-4cb6-b299-6ec6aae3c1b0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Create Theme daily returns using earlier code\n",
    "price_ts_h = df_h.copy()\n",
    "returns_ts_h = price_ts_h.pct_change().dropna()*100\n",
    "tickers_ts_h = returns_ts_h.columns.values.tolist()\n",
    "returns_ts_h_T = returns_ts_h.T\n",
    "\n",
    "## Adding back the Theme column to the dataframe\n",
    "returns_ts_h_T['Theme'] = theme_values_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc16d85-f92f-47d8-9df9-9f87e6edec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_ts_h_T.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b3893b-8ebf-40ed-a641-34bd8b428cec",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Run the function to get consolidated_df for further use\n",
    "theme_dataframes_h, consolidated_df_h, cumulative_returns_h = create_theme_dataframes_and_chart(returns_ts_h_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333afa05-1e63-4dff-b29f-30a9d4fc4397",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_theme = consolidated_df_h.T.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6c26ef-5007-4473-89e2-c215feca21cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_theme.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4d0c84-4f56-4b41-b0ae-94af5baeb654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aceefe7f-012f-4253-b38d-869581881c82",
   "metadata": {},
   "source": [
    "### Coins Performance for specific Themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d943cd-4dbc-4b6e-b178-a14c743a8c04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861a057f-7f1b-4886-a53d-ca0eede0dd9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1472bf91-98f7-45c5-acdb-f74612bd25de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd98b49-70a6-4006-9f13-cfc15bf184fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efcd652-501b-4f91-9d6d-5b489b885e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acf4004-dd6e-4966-b51d-e4e943e915a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480515a8-ec72-48a2-96d9-ba953cba9167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "451cf9f6-0263-4e0c-a390-423724e82f1d",
   "metadata": {},
   "source": [
    "## Momentum Signals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddad534-148f-455d-978c-8b94309d3df3",
   "metadata": {},
   "source": [
    "### Momentum Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c913d4-bf6a-449c-a308-e7176c3f727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uses Momentum Quality to filter out coins\n",
    "### Uses Momentum Quality as defined in: https://alphaarchitect.com/wp-content/uploads/2021/08/The_Quantitative_Momentum_Investing_Philosophy.pdf\n",
    "### Original paper: https://www3.nd.edu/~zda/Frog.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03756c09-cf02-4dc8-8bc7-4c747078c37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## df_h has the 4H price data of the coins\n",
    "returns_h = df_h.pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae3a4da-deb3-42ae-ae09-2129dc81fb22",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Clean the returns to take into account of any 0 values\n",
    "def clean_returns(returns_h, rolling_window=60):\n",
    "    cleaned_data = returns_h.copy()\n",
    "    if (cleaned_data.iloc[0] == 0).all():\n",
    "        cleaned_data = cleaned_data.iloc[1:].copy()\n",
    "    for col in cleaned_data.columns:\n",
    "        zero_mask = cleaned_data[col] == 0\n",
    "        if zero_mask.sum() > 0:\n",
    "            rolling_mean = cleaned_data[col].rolling(window=rolling_window, min_periods=1).mean()\n",
    "            cleaned_data.loc[zero_mask, col] = rolling_mean.loc[zero_mask]\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1268a7-00dd-4718-b3ee-fd26ab110977",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean the vol_scaled_returns data before processing\n",
    "returns_cleaned_h = clean_returns(returns_h).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa48803-a574-4189-8311-3336b8cc2eca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Calculate idmag and remove coins in top 20% of idmag\n",
    "\n",
    "def compute_idmag_weighted(returns_cleaned_h: pd.DataFrame,\n",
    "                           lookback: int = 30):\n",
    "    \"\"\"\n",
    "    Compute IDMAG as in Equation (2) of 'Frog in the Pan'.\n",
    "    \"\"\"\n",
    "\n",
    "    weights_by_quintile = {\n",
    "        0: 5/15,  # Q1 (smallest abs)\n",
    "        1: 4/15,\n",
    "        2: 3/15,\n",
    "        3: 2/15,\n",
    "        4: 1/15   # Q5 (largest abs)\n",
    "    }\n",
    "\n",
    "    idmag_all = {}\n",
    "\n",
    "    for coin in returns_cleaned_h.columns:\n",
    "        r = returns_cleaned_h[coin].dropna()\n",
    "\n",
    "        if len(r) < lookback:\n",
    "            idmag_all[coin] = np.nan\n",
    "            continue\n",
    "\n",
    "        # Rolling implementation\n",
    "        idmag_values = []\n",
    "\n",
    "        for i in range(lookback, len(r)):\n",
    "            window_returns = r.iloc[i - lookback:i]\n",
    "            abs_returns = window_returns.abs()\n",
    "\n",
    "            # Rank returns into 5 quintiles (labels 0-4)\n",
    "            quintile_labels = pd.qcut(abs_returns, 5, labels=False, duplicates='drop')\n",
    "\n",
    "            # Assign weights according to quintile\n",
    "            weights = quintile_labels.map(weights_by_quintile)\n",
    "\n",
    "            # Compute sgn(Return_i) * w_i\n",
    "            signed_weights = np.sign(window_returns) * weights.values\n",
    "\n",
    "            # Compute PRET\n",
    "            pret = window_returns.sum()\n",
    "\n",
    "            # IDMAG formula\n",
    "            sgn_pret = np.sign(pret) if pret != 0 else 0\n",
    "            idmag = - (1 / lookback) * sgn_pret * np.sum(signed_weights)\n",
    "            idmag_values.append(idmag)\n",
    "\n",
    "        # Average across all rolling windows\n",
    "        idmag_all[coin] = np.mean(idmag_values) if len(idmag_values) > 0 else np.nan\n",
    "\n",
    "    # Create dataframe\n",
    "    idmag_df = pd.DataFrame([\n",
    "        {'Name': coin, 'IDMAG': idmag_all[coin]}\n",
    "        for coin in idmag_all if not pd.isna(idmag_all[coin])\n",
    "    ])\n",
    "    idmag_df = idmag_df.sort_values(by='IDMAG', ascending=True)\n",
    "\n",
    "    # Filter top 80% (drop top 20%)\n",
    "    cutoff = int(len(idmag_df) * 0.8)\n",
    "    Mom_Qual = idmag_df.iloc[:cutoff]['Name'].tolist()\n",
    "\n",
    "    # Filter dataframes\n",
    "    returns_cleaned_h_filtered = returns_cleaned_h[Mom_Qual]\n",
    "\n",
    "    return {\n",
    "        'idmag_df': idmag_df,\n",
    "        'Mom_Qual': Mom_Qual,\n",
    "        'returns_cleaned_h_filtered': returns_cleaned_h_filtered,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e608ea-c36e-4e29-8100-3debc45da578",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = compute_idmag_weighted(\n",
    "    returns_cleaned_h=returns_cleaned_h,\n",
    "    lookback=360  # configurable\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3563f378-1058-4b1d-a74a-222febf5fd54",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "idmag_df = result['idmag_df']\n",
    "Mom_Qual = result['Mom_Qual']\n",
    "returns_cleaned_h_filtered = result['returns_cleaned_h_filtered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d9799d-0ed0-49cb-8c3a-d7c41ddf7bf8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Getting list of coins which had top 20% of idmag and will be removed from further analysis\n",
    "def get_high_idmag_list(idmag_df: pd.DataFrame, top_pct: float = 0.2):\n",
    "    \"\"\"\n",
    "    Returns list of coin names in the top `top_pct` by IDMAG value.\n",
    "    \"\"\"\n",
    "    top_cutoff = int(len(idmag_df) * top_pct)\n",
    "    high_idmag_df = idmag_df.iloc[-top_cutoff:]  # Top 20% â†’ end of ascending-sorted df\n",
    "    High_IDMAG_List = high_idmag_df['Name'].tolist()\n",
    "    return High_IDMAG_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe68c06-a5c5-4430-9ee5-e48cc4eb9eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "High_IDMAG_List = get_high_idmag_list(idmag_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efd7cbf-02b8-4246-863d-866edac62601",
   "metadata": {},
   "outputs": [],
   "source": [
    "High_IDMAG_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7924591-8681-450d-a06d-633f9629c4be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c78544b-b53c-49de-b0f7-410f773d6055",
   "metadata": {},
   "source": [
    "### Volatility Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cf1ecb-7c82-4178-bab2-82180ea43651",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_h = returns_cleaned_h_filtered.copy()\n",
    "\n",
    "# Calculate standard deviation of returns for each coin over the full period\n",
    "vol_full_period = returns_h.std()\n",
    "\n",
    "# Scale each coin's returns by its full-period volatility\n",
    "vol_scaled_returns = returns_h.divide(vol_full_period, axis='columns')\n",
    "\n",
    "# Optional cleanup: handle NaN and inf values\n",
    "vol_scaled_returns = vol_scaled_returns.replace([np.inf, -np.inf], np.nan).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08068ac0-6134-4463-9536-2a02e13f7e23",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_vol_scaled_returns(vol_scaled_returns, rolling_window=60):\n",
    "    cleaned_data = vol_scaled_returns.copy()\n",
    "    if (cleaned_data.iloc[0] == 0).all():\n",
    "        cleaned_data = cleaned_data.iloc[1:].copy()\n",
    "    for col in cleaned_data.columns:\n",
    "        zero_mask = cleaned_data[col] == 0\n",
    "        if zero_mask.sum() > 0:\n",
    "            rolling_mean = cleaned_data[col].rolling(window=rolling_window, min_periods=1).mean()\n",
    "            cleaned_data.loc[zero_mask, col] = rolling_mean.loc[zero_mask]\n",
    "        # Remove extreme outliers\n",
    "        mean_val = cleaned_data[col].mean()\n",
    "        std_val = cleaned_data[col].std()\n",
    "        if std_val > 0:\n",
    "            outlier_mask = np.abs(cleaned_data[col] - mean_val) > 5 * std_val\n",
    "            cleaned_data.loc[outlier_mask, col] = np.sign(cleaned_data.loc[outlier_mask, col]) * 5 * std_val + mean_val\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edb31c3-7c43-44aa-a05b-094c780a9c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the vol_scaled_returns data before processing\n",
    "vol_scaled_returns_cleaned = clean_vol_scaled_returns(vol_scaled_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7ba0a8-e428-45a6-a6a7-785f9f32405c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c32453d9-434c-4024-9907-58dd30ee813e",
   "metadata": {},
   "source": [
    "### Excess Momentum across Universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72f7cb2-474c-4a2c-b33d-e9e82bab7aff",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Calculate momentum scores for different lookback periods\n",
    "def calculate_momentum_scores(vol_scaled_returns, lookback=60, exclude_last=6, winsorize_limit=2):\n",
    "    shifted_returns = vol_scaled_returns.shift(exclude_last)\n",
    "    cum_returns = shifted_returns.rolling(window=lookback).sum()\n",
    "    for col in cum_returns.columns:\n",
    "        col_data = cum_returns[col].dropna()\n",
    "        if len(col_data) > 0:\n",
    "            q5 = col_data.quantile(0.05)\n",
    "            q95 = col_data.quantile(0.95)\n",
    "            cum_returns[col] = cum_returns[col].clip(lower=q5, upper=q95)\n",
    "    z_scores = (cum_returns - cum_returns.mean(axis=1, skipna=True).values[:, None]) / cum_returns.std(axis=1, skipna=True).values[:, None]\n",
    "    ranks = z_scores.rank(axis=1, ascending=False)\n",
    "    return z_scores, ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf9628d-af59-40f7-8ce7-86dc863d29af",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_signals(ranks, top_n=8, bottom_n=8):\n",
    "    buy_signals_dict = {}\n",
    "    sell_signals_dict = {}\n",
    "    for idx in ranks.index:\n",
    "        row = ranks.loc[idx].dropna()\n",
    "        if len(row) >= top_n:\n",
    "            buy_list = row.nsmallest(top_n).index.tolist()\n",
    "            sell_list = row.nlargest(bottom_n).index.tolist()\n",
    "            buy_signals_dict[idx] = buy_list\n",
    "            sell_signals_dict[idx] = sell_list\n",
    "    buy_signals = pd.Series(buy_signals_dict)\n",
    "    sell_signals = pd.Series(sell_signals_dict)\n",
    "    return buy_signals, sell_signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee08b283-cc4a-4f42-8fa1-9052a233851f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Weekly drawdown limit of 10%\n",
    "\n",
    "def backtest_with_weekly_drawdown_limit(\n",
    "    vol_scaled_returns, buy_signals, sell_signals, \n",
    "    max_position_size=0.1, stop_loss=-0.05, weekly_drawdown_limit=-0.1\n",
    "):\n",
    "    dates = vol_scaled_returns.index\n",
    "    capital = 1.0\n",
    "    capital_history = []\n",
    "    position_returns = []\n",
    "    date_history = []\n",
    "    trades_executed = 0\n",
    "    weekly_returns = []\n",
    "    periods_per_week = 6 * 7  # 4-hour bars, 6 per day, 7 days\n",
    "\n",
    "    for i in range(1, len(dates)):\n",
    "        current_date = dates[i]\n",
    "        prev_date = dates[i - 1]\n",
    "        # Update weekly returns window\n",
    "        if len(weekly_returns) >= periods_per_week:\n",
    "            weekly_returns.pop(0)\n",
    "        if len(position_returns) > 0:\n",
    "            weekly_returns.append(position_returns[-1])\n",
    "        # Compute rolling weekly capital and drawdown\n",
    "        weekly_capital = 1.0\n",
    "        for r in weekly_returns:\n",
    "            weekly_capital *= (1 + r)\n",
    "        weekly_drawdown = (weekly_capital - 1.0) / 1.0\n",
    "        # Enforce weekly drawdown limit\n",
    "        if weekly_drawdown < weekly_drawdown_limit:\n",
    "            capital_history.append(capital)\n",
    "            position_returns.append(0)\n",
    "            date_history.append(current_date)\n",
    "            continue\n",
    "        # Check for valid signals\n",
    "        if prev_date not in buy_signals.index:\n",
    "            capital_history.append(capital)\n",
    "            position_returns.append(0)\n",
    "            date_history.append(current_date)\n",
    "            continue\n",
    "        buy_list = buy_signals.loc[prev_date]\n",
    "        if not buy_list or len(buy_list) == 0:\n",
    "            capital_history.append(capital)\n",
    "            position_returns.append(0)\n",
    "            date_history.append(current_date)\n",
    "            continue\n",
    "        # Calculate returns for the position\n",
    "        try:\n",
    "            position_returns_raw = vol_scaled_returns.loc[current_date, buy_list]\n",
    "            valid_returns = position_returns_raw.dropna()\n",
    "            if len(valid_returns) == 0:\n",
    "                daily_returns = 0\n",
    "            else:\n",
    "                daily_returns = valid_returns.mean()\n",
    "            # Apply position sizing\n",
    "            if abs(daily_returns) > max_position_size:\n",
    "                daily_returns = np.sign(daily_returns) * max_position_size\n",
    "            # Apply stop-loss\n",
    "            if daily_returns < stop_loss:\n",
    "                daily_returns = stop_loss\n",
    "            capital *= (1 + daily_returns)\n",
    "            trades_executed += 1\n",
    "        except (KeyError, ValueError):\n",
    "            daily_returns = 0\n",
    "            capital *= (1 + daily_returns)\n",
    "        capital_history.append(capital)\n",
    "        position_returns.append(daily_returns)\n",
    "        date_history.append(current_date)\n",
    "    capital_series = pd.Series(capital_history, index=pd.Index(date_history))\n",
    "    returns_series = pd.Series(position_returns, index=pd.Index(date_history))\n",
    "    print(f\"Total trades executed: {trades_executed}\")\n",
    "    print(f\"Final capital: {capital:.4f}\")\n",
    "    if len(capital_history) > 0:\n",
    "        max_dd = ((min(capital_history) - max(capital_history)) / max(capital_history))\n",
    "        print(f\"Max drawdown: {max_dd:.4f}\")\n",
    "    return capital_series, returns_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c19ca5-e898-4249-9e44-0720e9fd19ec",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_results(equity_curve, returns_series, title=\"\"):\n",
    "    rolling_return = returns_series.rolling(24).mean()  # 4 days\n",
    "    drawdown = equity_curve / equity_curve.cummax() - 1\n",
    "    total_return = (equity_curve.iloc[-1] - 1) * 100\n",
    "    max_dd = drawdown.min() * 100\n",
    "    returns_std = returns_series.std()\n",
    "    sharpe = returns_series.mean() / returns_std * np.sqrt(365.25 * 6) if returns_std != 0 else 0\n",
    "    win_rate = (returns_series > 0).mean() * 100\n",
    "    fig, axs = plt.subplots(4, 1, figsize=(15, 12))\n",
    "    axs[0].plot(equity_curve.index, equity_curve.values, label='Equity Curve', linewidth=2)\n",
    "    axs[0].set_title(f'Equity Curve - {title}\\nTotal Return: {total_return:.2f}%, Max DD: {max_dd:.2f}%, Sharpe: {sharpe:.2f}')\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(True, alpha=0.3)\n",
    "    axs[1].plot(rolling_return.index, rolling_return.values, label='Rolling Return (4D)', color='orange')\n",
    "    axs[1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    axs[1].set_title('Rolling Return (4 Days)')\n",
    "    axs[1].legend()\n",
    "    axs[1].grid(True, alpha=0.3)\n",
    "    axs[2].fill_between(drawdown.index, drawdown.values, 0, alpha=0.3, color='red')\n",
    "    axs[2].plot(drawdown.index, drawdown.values, color='red', linewidth=1)\n",
    "    axs[2].set_title('Drawdown')\n",
    "    axs[2].set_ylabel('Drawdown %')\n",
    "    axs[2].grid(True, alpha=0.3)\n",
    "    axs[3].hist(returns_series.values, bins=50, alpha=0.7, edgecolor='black')\n",
    "    axs[3].axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
    "    axs[3].set_title(f'Returns Distribution - Win Rate: {win_rate:.1f}%')\n",
    "    axs[3].set_xlabel('Return')\n",
    "    axs[3].set_ylabel('Frequency')\n",
    "    axs[3].grid(True, alpha=0.3)\n",
    "    for ax in axs[:-1]:\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86d4e3d-7138-4663-a704-bfaf406321f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MAIN EXECUTION ===\n",
    "print(\"Starting momentum strategy with weekly drawdown control...\")\n",
    "\n",
    "vol_scaled_returns_cleaned = clean_vol_scaled_returns(vol_scaled_returns)\n",
    "\n",
    "## Lookback periods of 10/15/20 days\n",
    "\n",
    "lookbacks = [60]   \n",
    "for lb in lookbacks:\n",
    "    print(f\"\\n{'='*60}\\nTESTING LOOKBACK = {lb}\\n{'='*60}\")\n",
    "    z_scores, ranks = calculate_momentum_scores(vol_scaled_returns_cleaned, lookback=lb, exclude_last=6)\n",
    "    buy_signals, sell_signals = generate_signals(ranks, top_n=8, bottom_n=8)\n",
    "    if len(buy_signals) == 0:\n",
    "        print(f\"No valid signals generated for lookback {lb}\")\n",
    "        continue\n",
    "    equity_curve, returns_series = backtest_with_weekly_drawdown_limit(\n",
    "        vol_scaled_returns_cleaned, buy_signals, sell_signals,\n",
    "        max_position_size=0.1,  # 10% position size\n",
    "        stop_loss=-0.05,        # 5% stop loss\n",
    "        weekly_drawdown_limit=-0.1  # -10% weekly drawdown limit\n",
    "    )\n",
    "    if len(equity_curve) > 0:\n",
    "        plot_results(equity_curve, returns_series, title=f'Lookback = {lb}')\n",
    "    else:\n",
    "        print(f\"No data to plot for lookback {lb}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e23dba-c3aa-4aae-ab77-300caf779b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting the Buy Signals\n",
    "buy_signals.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d922eb3-5300-4d8c-a436-43fc9a0d4aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting the Sell Signals\n",
    "sell_signals.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fea9be-bba3-4c3b-a82b-2322d1ea79fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_signals.to_excel('buy_signals.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5867b7-8182-4154-9acc-47a5ac74668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sell_signals.to_excel('sell_signals.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0cc51e-52e1-49b5-8b60-ca8be6779be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUTURE WORK - NEED TO COMBINE SIGNALS FROM DIFFERENT LOOKBACK PERIODS AND TAKE WEIGHTED AVERAGE TO SELECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37b537e-82c2-413c-9979-8e1b4a52a0c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09de6b21-dd93-4d4d-82bd-6fc72e86e4e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b89948d-0634-4598-a46c-50e4a6cb6f01",
   "metadata": {},
   "source": [
    "### Excess Momentum For each Sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c356cef7-3262-434c-a9db-060ea3201b56",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Saving themes of filtered coins to list\n",
    "filtered_tickers_h = vol_scaled_returns_cleaned.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf5e08b-4bc6-434d-8687-f7f3f9df856e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now map themes for those tickers\n",
    "filtered_theme_values_h = [ticker_to_theme[ticker] for ticker in filtered_tickers_h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50d0936-8c1f-4fee-a697-5b47a14344c1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Adding back the Theme column to the dataframe.T\n",
    "vol_scaled_returns_cleaned_T = vol_scaled_returns_cleaned.T\n",
    "vol_scaled_returns_cleaned_T['Theme'] = filtered_theme_values_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5f8792-980b-4e7e-a2ae-4feb8e1ce7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_scaled_returns_cleaned_T.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2839a13f-a99a-4316-b98b-f89704ecf18a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Creating a list of themes - have removed any themes which do have less than 5 coins manually \n",
    "theme_list_short = ['DEX', 'AI', 'DEPIN', 'L2', 'GAMEFI', 'RWA', 'ZKPROOF', 'CROSS_CHAIN', 'MEME', 'LENDING']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91721b5c-006e-42b5-a0cb-ff5576e3cbe9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create separate datasets for each theme\n",
    "theme_datasets = {}\n",
    "\n",
    "for theme in theme_list_short:\n",
    "    # Filter dataframe for the current theme\n",
    "    filtered_df = vol_scaled_returns_cleaned_T[vol_scaled_returns_cleaned_T['Theme'] == theme]\n",
    "    \n",
    "    # Transpose the filtered dataframe\n",
    "    transposed_df = filtered_df.T\n",
    "    \n",
    "    # Remove the Theme row (since we transposed, Theme is now a row)\n",
    "    transposed_df = transposed_df.drop('Theme', axis=0)\n",
    "    \n",
    "    # Store in dictionary with naming convention\n",
    "    theme_datasets[f'{theme}_df_T'] = transposed_df\n",
    "    \n",
    "    # Optionally, create individual variables for each theme dataset\n",
    "    globals()[f'{theme}_df_T'] = transposed_df\n",
    "\n",
    "# Print summary of created datasets\n",
    "print(\"Created datasets:\")\n",
    "for theme in theme_list_short:\n",
    "    dataset_name = f'{theme}_df_T'\n",
    "    print(f\"- {dataset_name}: Shape {theme_datasets[dataset_name].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a359ed89-6f36-404b-abac-d0f44cbba5cc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Access a specific theme dataset\n",
    "dex_df_T = theme_datasets['DEX_df_T']  # if 'DEX' is in your theme_list_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271c60d1-03a9-4a31-aa48-81c0610b2ecc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ===== GENERATE BUY/SELL SIGNALS FOR ALL THEME DATASETS =====\n",
    "# ===== KEEPING ONLY THE SIGNALS FOR THE LAST DATE =====\n",
    "\n",
    "# Initialize list to store all signals\n",
    "all_signals = []\n",
    "\n",
    "# Process each theme dataset\n",
    "for theme_name, dataset in theme_datasets.items():\n",
    "    try:\n",
    "        # Run momentum analysis on current dataset\n",
    "        z_scores, ranks = calculate_momentum_scores(dataset, lookback=90, exclude_last=6)\n",
    "        buy_signals, sell_signals = generate_signals(ranks, top_n=3, bottom_n=3)\n",
    "        \n",
    "        # Get signals for the last date only\n",
    "        if isinstance(buy_signals, pd.DataFrame):\n",
    "            # If buy_signals is a DataFrame, get the last row\n",
    "            last_date = buy_signals.index[-1]\n",
    "            buy_signals = buy_signals.loc[last_date]\n",
    "        elif isinstance(buy_signals, pd.Series) and len(buy_signals.index) > 1:\n",
    "            # If it's a Series with multiple dates, get the last one\n",
    "            buy_signals = buy_signals.iloc[-1] if hasattr(buy_signals.iloc[-1], '__iter__') else [buy_signals.iloc[-1]]\n",
    "        \n",
    "        if isinstance(sell_signals, pd.DataFrame):\n",
    "            # If sell_signals is a DataFrame, get the last row\n",
    "            last_date = sell_signals.index[-1]\n",
    "            sell_signals = sell_signals.loc[last_date]\n",
    "        elif isinstance(sell_signals, pd.Series) and len(sell_signals.index) > 1:\n",
    "            # If it's a Series with multiple dates, get the last one\n",
    "            sell_signals = sell_signals.iloc[-1] if hasattr(sell_signals.iloc[-1], '__iter__') else [sell_signals.iloc[-1]]\n",
    "        \n",
    "        # Convert signals to comma-separated strings - handle different data types\n",
    "        # Handle buy signals\n",
    "        if isinstance(buy_signals, pd.Series):\n",
    "            buy_signals_str = ', '.join(buy_signals.astype(str)) if not buy_signals.empty else ''\n",
    "        elif isinstance(buy_signals, list):\n",
    "            buy_signals_str = ', '.join(buy_signals) if buy_signals else ''\n",
    "        elif isinstance(buy_signals, (pd.Index, np.ndarray)):\n",
    "            buy_signals_str = ', '.join(buy_signals.astype(str)) if len(buy_signals) > 0 else ''\n",
    "        else:\n",
    "            buy_signals_str = str(buy_signals) if buy_signals is not None else ''\n",
    "            \n",
    "        # Handle sell signals\n",
    "        if isinstance(sell_signals, pd.Series):\n",
    "            sell_signals_str = ', '.join(sell_signals.astype(str)) if not sell_signals.empty else ''\n",
    "        elif isinstance(sell_signals, list):\n",
    "            sell_signals_str = ', '.join(sell_signals) if sell_signals else ''\n",
    "        elif isinstance(sell_signals, (pd.Index, np.ndarray)):\n",
    "            sell_signals_str = ', '.join(sell_signals.astype(str)) if len(sell_signals) > 0 else ''\n",
    "        else:\n",
    "            sell_signals_str = str(sell_signals) if sell_signals is not None else ''\n",
    "        \n",
    "        # Store signals for this theme\n",
    "        signal_row = {\n",
    "            'Name': theme_name,\n",
    "            'Buy': buy_signals_str,\n",
    "            'Sell': sell_signals_str\n",
    "        }\n",
    "        all_signals.append(signal_row)\n",
    "        \n",
    "        print(f\"âœ“ Processed {theme_name}: {len(buy_signals) if hasattr(buy_signals, '__len__') else 1} buy, {len(sell_signals) if hasattr(sell_signals, '__len__') else 1} sell signals\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error processing {theme_name}: {str(e)}\")\n",
    "        # Add empty row for failed processing\n",
    "        all_signals.append({\n",
    "            'Name': theme_name,\n",
    "            'Buy': '',\n",
    "            'Sell': ''\n",
    "        })\n",
    "\n",
    "# Create consolidated signals dataframe\n",
    "consolidated_signals = pd.DataFrame(all_signals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f474ebda-a173-41c3-b110-7eff4b4676a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3db8d85-1329-46b3-a92b-3a24bca0e87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUTURE WORK - NEED TO COMBINE SIGNALS FROM DIFFERENT LOOKBACK PERIODS AND TAKE WEIGHTED AVERAGE TO SELECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fb1324-0821-4a46-9050-1290157a0b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc4ef8b0-e7d5-4dad-963a-1cb31ce7daaf",
   "metadata": {},
   "source": [
    "### Volatility Breakout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde560b6-53df-4383-8187-704e804a2625",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple, Dict, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403826ec-e356-46df-8161-eb73fee0b562",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VolatilityBreakoutSignal:\n",
    "    \"\"\"\n",
    "    Volatility breakout signal generator that considers both historical \n",
    "    and cross-sectional volatility patterns.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 historical_window: int = 30,\n",
    "                 volatility_window: int = 6,\n",
    "                 cross_sectional_window: int = 20,\n",
    "                 historical_threshold: float = 2.0,\n",
    "                 cross_sectional_threshold: float = 1.5):\n",
    "        \"\"\"\n",
    "        Initialize volatility breakout parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        historical_window : int\n",
    "            Lookback period for calculating historical volatility baseline (in 4-hour periods)\n",
    "        volatility_window : int  \n",
    "            Rolling window for current volatility calculation (in 4-hour periods)\n",
    "        cross_sectional_window : int\n",
    "            Window for cross-sectional volatility comparison\n",
    "        historical_threshold : float\n",
    "            Z-score threshold for historical volatility breakout\n",
    "        cross_sectional_threshold : float\n",
    "            Z-score threshold for cross-sectional volatility breakout\n",
    "        \"\"\"\n",
    "        self.historical_window = historical_window\n",
    "        self.volatility_window = volatility_window\n",
    "        self.cross_sectional_window = cross_sectional_window\n",
    "        self.historical_threshold = historical_threshold\n",
    "        self.cross_sectional_threshold = cross_sectional_threshold\n",
    "    \n",
    "    def calculate_returns(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Calculate log returns from price data.\"\"\"\n",
    "        return np.log(df / df.shift(1)).dropna()\n",
    "    \n",
    "    def calculate_rolling_volatility(self, returns: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Calculate rolling volatility (standard deviation of returns).\"\"\"\n",
    "        return returns.rolling(window=self.volatility_window).std()\n",
    "    \n",
    "    def calculate_historical_volatility_zscore(self, volatility: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calculate z-score of current volatility vs historical volatility for each coin.\n",
    "        \"\"\"\n",
    "        historical_mean = volatility.rolling(window=self.historical_window).mean()\n",
    "        historical_std = volatility.rolling(window=self.historical_window).std()\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        historical_std = historical_std.replace(0, np.nan)\n",
    "        \n",
    "        z_scores = (volatility - historical_mean) / historical_std\n",
    "        return z_scores\n",
    "    \n",
    "    def calculate_cross_sectional_zscore(self, volatility: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calculate z-score of each coin's volatility vs cross-sectional average.\n",
    "        \"\"\"\n",
    "        # Calculate cross-sectional mean and std for each timestamp\n",
    "        cross_sectional_mean = volatility.mean(axis=1)\n",
    "        cross_sectional_std = volatility.std(axis=1)\n",
    "        \n",
    "        # Broadcast to match original shape\n",
    "        cs_mean_matrix = np.broadcast_to(cross_sectional_mean.values.reshape(-1, 1), \n",
    "                                       volatility.shape)\n",
    "        cs_std_matrix = np.broadcast_to(cross_sectional_std.values.reshape(-1, 1), \n",
    "                                      volatility.shape)\n",
    "        \n",
    "        # Create DataFrames with same index/columns as input\n",
    "        cs_mean_df = pd.DataFrame(cs_mean_matrix, \n",
    "                                index=volatility.index, \n",
    "                                columns=volatility.columns)\n",
    "        cs_std_df = pd.DataFrame(cs_std_matrix, \n",
    "                               index=volatility.index, \n",
    "                               columns=volatility.columns)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        cs_std_df = cs_std_df.replace(0, np.nan)\n",
    "        \n",
    "        # Calculate z-scores\n",
    "        z_scores = (volatility - cs_mean_df) / cs_std_df\n",
    "        return z_scores\n",
    "    \n",
    "    def calculate_excess_volatility_score(self, \n",
    "                                        historical_zscore: pd.DataFrame,\n",
    "                                        cross_sectional_zscore: pd.DataFrame,\n",
    "                                        historical_weight: float = 0.6,\n",
    "                                        cross_sectional_weight: float = 0.4) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Combine historical and cross-sectional z-scores into excess volatility score.\n",
    "        \"\"\"\n",
    "        excess_score = (historical_weight * historical_zscore + \n",
    "                       cross_sectional_weight * cross_sectional_zscore)\n",
    "        return excess_score\n",
    "    \n",
    "    def generate_signals(self, df: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Generate volatility breakout signals and rankings.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pd.DataFrame\n",
    "            Price data with datetime index and coin symbols as columns\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        Dict containing:\n",
    "        - 'signals': Binary signals (1 for breakout, 0 otherwise)\n",
    "        - 'rankings': Percentile rankings of excess volatility\n",
    "        - 'excess_volatility': Raw excess volatility scores\n",
    "        - 'historical_zscore': Historical volatility z-scores\n",
    "        - 'cross_sectional_zscore': Cross-sectional volatility z-scores\n",
    "        \"\"\"\n",
    "        \n",
    "        # Step 1: Calculate returns\n",
    "        returns = self.calculate_returns(df)\n",
    "        \n",
    "        # Step 2: Calculate rolling volatility\n",
    "        volatility = self.calculate_rolling_volatility(returns)\n",
    "        \n",
    "        # Step 3: Calculate historical volatility z-scores\n",
    "        historical_zscore = self.calculate_historical_volatility_zscore(volatility)\n",
    "        \n",
    "        # Step 4: Calculate cross-sectional volatility z-scores\n",
    "        cross_sectional_zscore = self.calculate_cross_sectional_zscore(volatility)\n",
    "        \n",
    "        # Step 5: Calculate combined excess volatility score\n",
    "        excess_volatility = self.calculate_excess_volatility_score(\n",
    "            historical_zscore, cross_sectional_zscore\n",
    "        )\n",
    "        \n",
    "        # Step 6: Generate binary signals based on thresholds\n",
    "        historical_signals = (historical_zscore > self.historical_threshold).astype(int)\n",
    "        cross_sectional_signals = (cross_sectional_zscore > self.cross_sectional_threshold).astype(int)\n",
    "        \n",
    "        # Combined signal: either conditions must be met\n",
    "        combined_signals = (historical_signals | cross_sectional_signals).astype(int)\n",
    "        \n",
    "        # Step 7: Calculate rankings (percentile-based)\n",
    "        rankings = excess_volatility.rank(axis=1, pct=True) * 100\n",
    "        \n",
    "        # Step 8: Create strength-based rankings (0-100 scale)\n",
    "        strength_rankings = pd.DataFrame(index=excess_volatility.index, \n",
    "                                       columns=excess_volatility.columns)\n",
    "        \n",
    "        for timestamp in excess_volatility.index:\n",
    "            row_data = excess_volatility.loc[timestamp].dropna()\n",
    "            if len(row_data) > 0:\n",
    "                # Rank from highest (strongest) to lowest excess volatility\n",
    "                ranked = row_data.rank(ascending=False)\n",
    "                # Convert to 0-100 scale\n",
    "                max_rank = len(row_data)\n",
    "                strength_scores = ((max_rank - ranked + 1) / max_rank) * 100\n",
    "                strength_rankings.loc[timestamp, strength_scores.index] = strength_scores\n",
    "        \n",
    "        return {\n",
    "            'signals': combined_signals,\n",
    "            'rankings': rankings,\n",
    "            'strength_rankings': strength_rankings,\n",
    "            'excess_volatility': excess_volatility,\n",
    "            'historical_zscore': historical_zscore,\n",
    "            'cross_sectional_zscore': cross_sectional_zscore,\n",
    "            'volatility': volatility\n",
    "        }\n",
    "    \n",
    "    def get_top_signals(self, \n",
    "                       signals_dict: Dict[str, pd.DataFrame], \n",
    "                       timestamp: Optional[str] = None,\n",
    "                       top_n: int = 10,\n",
    "                       require_both_signals: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get top N coins with strongest volatility breakout signals.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        signals_dict : Dict\n",
    "            Output from generate_signals()\n",
    "        timestamp : str, optional\n",
    "            Specific timestamp to analyze. If None, uses latest.\n",
    "        top_n : int\n",
    "            Number of top coins to return\n",
    "        require_both_signals : bool\n",
    "            If True, requires both historical and cross-sectional signals.\n",
    "            If False, returns top coins by excess volatility score regardless of signals.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame with top coins and their metrics\n",
    "        \"\"\"\n",
    "        if timestamp is None:\n",
    "            timestamp = signals_dict['signals'].index[-1]\n",
    "        \n",
    "        # Get data for specific timestamp\n",
    "        signals = signals_dict['signals'].loc[timestamp]\n",
    "        excess_vol = signals_dict['excess_volatility'].loc[timestamp].dropna()\n",
    "        strength_rank = signals_dict['strength_rankings'].loc[timestamp]\n",
    "        hist_z = signals_dict['historical_zscore'].loc[timestamp]\n",
    "        cs_z = signals_dict['cross_sectional_zscore'].loc[timestamp]\n",
    "        \n",
    "        if require_both_signals:\n",
    "            # Filter for coins with signals (both conditions met)\n",
    "            signal_coins = signals[signals == 1].index\n",
    "            \n",
    "            if len(signal_coins) == 0:\n",
    "                print(f\"No coins meet both thresholds at {timestamp}\")\n",
    "                print(\"Historical signals:\", (signals_dict['historical_zscore'].loc[timestamp] > self.historical_threshold).sum())\n",
    "                print(\"Cross-sectional signals:\", (signals_dict['cross_sectional_zscore'].loc[timestamp] > self.cross_sectional_threshold).sum())\n",
    "                return pd.DataFrame()  # No signals\n",
    "            \n",
    "            valid_coins = signal_coins\n",
    "        else:\n",
    "            # Use all coins with valid excess volatility scores\n",
    "            valid_coins = excess_vol.index\n",
    "        \n",
    "        # Create summary DataFrame\n",
    "        summary = pd.DataFrame({\n",
    "            'coin': valid_coins,\n",
    "            'excess_volatility_score': excess_vol[valid_coins].values,\n",
    "            'strength_ranking': strength_rank[valid_coins].values,\n",
    "            'historical_zscore': hist_z[valid_coins].values,\n",
    "            'cross_sectional_zscore': cs_z[valid_coins].values,\n",
    "            'has_signal': signals[valid_coins].values if require_both_signals else 'N/A'\n",
    "        })\n",
    "        \n",
    "        # Add individual signal flags for debugging\n",
    "        hist_signals = (hist_z > self.historical_threshold).astype(int)\n",
    "        cs_signals = (cs_z > self.cross_sectional_threshold).astype(int)\n",
    "        \n",
    "        summary['historical_signal'] = hist_signals[valid_coins].values\n",
    "        summary['cross_sectional_signal'] = cs_signals[valid_coins].values\n",
    "        \n",
    "        # Sort by excess volatility score (descending)\n",
    "        summary = summary.sort_values('excess_volatility_score', ascending=False)\n",
    "        \n",
    "        return summary.head(top_n).reset_index(drop=True)\n",
    "\n",
    "    def diagnose_signals(self, \n",
    "                        signals_dict: Dict[str, pd.DataFrame], \n",
    "                        timestamp: Optional[str] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Diagnose why you might be getting few signals.\n",
    "        \n",
    "        Returns statistics about signal generation.\n",
    "        \"\"\"\n",
    "        if timestamp is None:\n",
    "            timestamp = signals_dict['signals'].index[-1]\n",
    "        \n",
    "        hist_z = signals_dict['historical_zscore'].loc[timestamp].dropna()\n",
    "        cs_z = signals_dict['cross_sectional_zscore'].loc[timestamp].dropna()\n",
    "        \n",
    "        hist_signals = (hist_z > self.historical_threshold).sum()\n",
    "        cs_signals = (cs_z > self.cross_sectional_threshold).sum()\n",
    "        both_signals = ((hist_z > self.historical_threshold) & \n",
    "                       (cs_z > self.cross_sectional_threshold)).sum()\n",
    "        \n",
    "        diagnostics = {\n",
    "            'timestamp': timestamp,\n",
    "            'total_coins': len(hist_z),\n",
    "            'historical_threshold': self.historical_threshold,\n",
    "            'cross_sectional_threshold': self.cross_sectional_threshold,\n",
    "            'coins_above_historical_threshold': hist_signals,\n",
    "            'coins_above_cross_sectional_threshold': cs_signals,\n",
    "            'coins_above_both_thresholds': both_signals,\n",
    "            'max_historical_zscore': hist_z.max(),\n",
    "            'max_cross_sectional_zscore': cs_z.max(),\n",
    "            'mean_historical_zscore': hist_z.mean(),\n",
    "            'mean_cross_sectional_zscore': cs_z.mean(),\n",
    "            'std_historical_zscore': hist_z.std(),\n",
    "            'std_cross_sectional_zscore': cs_z.std()\n",
    "        }\n",
    "        \n",
    "        return diagnostics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a6a20c-65ed-4622-9aec-9ac4d4a60ebc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize with your parameters\n",
    "vb_signal = VolatilityBreakoutSignal(\n",
    "    historical_window=90,      # 15 days of 4-hour data\n",
    "    volatility_window=6,       # 1 day rolling volatility\n",
    "    historical_threshold=1.5,   # 2 std devs above historical\n",
    "    cross_sectional_threshold=1.0  # 1.5 std devs above peers\n",
    ")\n",
    "\n",
    "# Generate all signals and metrics\n",
    "signals_dict = vb_signal.generate_signals(df_h)\n",
    "\n",
    "# Get top 11-20 strongest signals\n",
    "#top_signals = vb_signal.get_top_signals(signals_dict, top_n=10)\n",
    "\n",
    "top_signals = vb_signal.get_top_signals(signals_dict, top_n=20).iloc[10:20].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e85419-2ec1-49d1-a048-c2d06fbafe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f7bb48-121e-4510-9eaf-038743da7dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5568d1a9-e170-4732-bff9-4ef8898a64bf",
   "metadata": {},
   "source": [
    "### Price Range Breakout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e40be-e002-4255-8ce6-a6874830e8d6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fetch daily close prices - valid_pairs is defined earlier\n",
    "timeframe = '1d'\n",
    "limit = 815 # NEED TO MODIFY - from 1st June 2023 to today\n",
    "sleep_seconds = 0.2\n",
    "\n",
    "import time\n",
    "\n",
    "frames = []\n",
    "\n",
    "for pair in valid_pairs:\n",
    "    try:\n",
    "        base = pair.split('/')[0]  # e.g. BTC\n",
    "        ohlcv = exchange.fetch_ohlcv(pair, timeframe=timeframe, limit=limit)\n",
    "        df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "        df = df[['timestamp', 'close']].rename(columns={'close': base})\n",
    "        frames.append(df.set_index('timestamp'))\n",
    "        time.sleep(sleep_seconds)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {pair}: {e}\")\n",
    "\n",
    "# Merge all into wide format\n",
    "price_d = pd.concat(frames, axis=1).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3038a4b-08b1-4a07-acbc-79c07340689b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detect_sr_channels_coins_only(df, \n",
    "                                  prd=5,\n",
    "                                  loopback=200,\n",
    "                                  min_strength=1,\n",
    "                                  channel_w_start=0.1,\n",
    "                                  channel_w_max=0.3,\n",
    "                                  proximity_start=0.02,\n",
    "                                  proximity_max=0.1,\n",
    "                                  top_n=15):\n",
    "\n",
    "    def detect_once(df, prd, loopback, min_strength, channel_w, proximity):\n",
    "        results = { \"near_support\": [], \"near_resistance\": [],\n",
    "                    \"broke_support\": [], \"broke_resistance\": [] }\n",
    "\n",
    "        for coin in df.columns:\n",
    "            prices = df[coin].dropna()\n",
    "            if len(prices) < loopback + prd + 2:\n",
    "                continue\n",
    "\n",
    "            closes = prices.values\n",
    "            highs, lows = closes, closes\n",
    "\n",
    "            # Step 1: pivots\n",
    "            pivots = []\n",
    "            for i in range(prd, len(closes) - prd):\n",
    "                is_high = all(highs[i] >= highs[i - k] for k in range(1, prd+1)) and \\\n",
    "                          all(highs[i] >= highs[i + k] for k in range(1, prd+1))\n",
    "                is_low = all(lows[i] <= lows[i - k] for k in range(1, prd+1)) and \\\n",
    "                         all(lows[i] <= lows[i + k] for k in range(1, prd+1))\n",
    "                if is_high or is_low:\n",
    "                    pivots.append((i, closes[i]))\n",
    "\n",
    "            if not pivots:\n",
    "                continue\n",
    "\n",
    "            # Step 2: channels\n",
    "            max_width = (closes[-loopback:].max() - closes[-loopback:].min()) * channel_w\n",
    "            channels = []\n",
    "            for _, val in pivots:\n",
    "                lo, hi, count = val, val, 0\n",
    "                for _, val2 in pivots:\n",
    "                    if abs(val2 - val) <= max_width:\n",
    "                        lo, hi = min(lo, val2), max(hi, val2)\n",
    "                        count += 1\n",
    "                if count >= min_strength:\n",
    "                    channels.append([lo, hi, count])\n",
    "\n",
    "            if not channels:\n",
    "                continue\n",
    "\n",
    "            # Step 3: score by loopback\n",
    "            for ch in channels:\n",
    "                lo, hi, s = ch\n",
    "                for k in range(1, loopback+1):\n",
    "                    if len(closes)-k < 0: break\n",
    "                    if (lows[-k] <= hi and highs[-k] >= lo):\n",
    "                        ch[2] += 1\n",
    "\n",
    "            channels = sorted(channels, key=lambda x: x[2], reverse=True)[:3]\n",
    "\n",
    "            # Step 4: classify\n",
    "            last, prev = closes[-1], closes[-2]\n",
    "            support = [c for c in channels if c[0] <= last]\n",
    "            resistance = [c for c in channels if c[1] >= last]\n",
    "\n",
    "            if support:\n",
    "                lo, hi, s = max(support, key=lambda x: x[2])\n",
    "                if abs(last - lo)/last <= proximity:\n",
    "                    results[\"near_support\"].append((coin, s))\n",
    "                if prev >= lo and last < lo:\n",
    "                    results[\"broke_support\"].append((coin, s))\n",
    "\n",
    "            if resistance:\n",
    "                lo, hi, s = max(resistance, key=lambda x: x[2])\n",
    "                if abs(last - hi)/last <= proximity:\n",
    "                    results[\"near_resistance\"].append((coin, s))\n",
    "                if prev <= hi and last > hi:\n",
    "                    results[\"broke_resistance\"].append((coin, s))\n",
    "\n",
    "        for k in results:\n",
    "            results[k] = sorted(results[k], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return results\n",
    "\n",
    "    # Adaptive loop\n",
    "    channel_w, proximity = channel_w_start, proximity_start\n",
    "    while channel_w <= channel_w_max and proximity <= proximity_max:\n",
    "        res = detect_once(df, prd, loopback, min_strength, channel_w, proximity)\n",
    "\n",
    "        assigned = set()\n",
    "        unique_res = {k: [] for k in res}\n",
    "\n",
    "        # Priority: breakouts first, then near levels\n",
    "        for key in [\"broke_support\", \"broke_resistance\", \"near_support\", \"near_resistance\"]:\n",
    "            for coin, strength in res[key]:\n",
    "                if coin not in assigned:\n",
    "                    unique_res[key].append((coin, strength))\n",
    "                    assigned.add(coin)\n",
    "\n",
    "        # Limit to top_n and keep only coin names\n",
    "        for k in unique_res:\n",
    "            unique_res[k] = [c for c, _ in unique_res[k][:top_n]]\n",
    "\n",
    "        if any(len(unique_res[k]) > 0 for k in unique_res):\n",
    "            return unique_res\n",
    "\n",
    "        # Relax thresholds\n",
    "        channel_w *= 1.5\n",
    "        proximity *= 1.5\n",
    "\n",
    "    return unique_res  # return even if empty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45abb725-e2db-489b-ab2a-f2ffa6646259",
   "metadata": {},
   "outputs": [],
   "source": [
    "signals = detect_sr_channels_coins_only(price_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1191d76b-7abc-4f40-be15-0423543fdb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "signals['near_support']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d738740-4976-4cfc-90d2-d2b934f79576",
   "metadata": {},
   "outputs": [],
   "source": [
    "signals['near_resistance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2d4528-fc84-4b8a-8284-39d00972e564",
   "metadata": {},
   "outputs": [],
   "source": [
    "signals['broke_support']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d7dba8-3236-4cdc-8bbf-5be36ab52c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "signals['broke_resistance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a1ac40-fa4a-48c2-99d6-7934c5c72371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f50b729-76bc-4799-83d4-82ade3d2e90f",
   "metadata": {},
   "source": [
    "### Relative Price Oscillator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729f94c6-e3a0-4f6c-9163-5180b3a5d68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_scaled_returns_cleaned_T.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c1d3e1-090c-4308-872a-58f77ccf20fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NEED TO WORK ON THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797cbfe9-c019-4772-b772-9f7f82a24af7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaafbcb-3d8c-4c13-a923-e60da644f64a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fd8db3-6d65-4a67-8ae4-7ee846019f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28cc001-6b67-4e73-af28-e8cc97019e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07be5231-215f-40f2-b827-d065e3a30481",
   "metadata": {},
   "source": [
    "### Alpha Percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5817ed0a-811e-4848-9ce1-329b3a5187a3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Checking Alpha of each coin vs BTC and getting %tile ranks for latest obs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69491894-4ad9-4bce-9f43-dcb23beb039d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fetch 4H close prices for 360 days\n",
    "timeframe = '1d'\n",
    "limit = 360\n",
    "sleep_seconds = 0.2\n",
    "\n",
    "import time\n",
    "\n",
    "frames = []\n",
    "\n",
    "for pair in valid_pairs:\n",
    "    try:\n",
    "        base = pair.split('/')[0]  # e.g. BTC\n",
    "        ohlcv = exchange.fetch_ohlcv(pair, timeframe=timeframe, limit=limit)\n",
    "        df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "        df = df[['timestamp', 'close']].rename(columns={'close': base})\n",
    "        frames.append(df.set_index('timestamp'))\n",
    "        time.sleep(sleep_seconds)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {pair}: {e}\")\n",
    "\n",
    "# Merge all into wide format\n",
    "df_price_alpha = pd.concat(frames, axis=1).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0f00a0-f240-4498-be64-9eddbc5cc69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price_alpha.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f70a67-dcbf-4df9-af22-73a7b49db84b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Step 1: Compute returns ===\n",
    "df_ret = df_price_alpha.pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f04e180-ac33-44fc-b328-7ebaada832c0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Step 0: Clean df_ret ===\n",
    "# Drop columns (coins) that are entirely NaN\n",
    "df_ret = df_ret.dropna(axis=1, how=\"all\").copy()\n",
    "\n",
    "# Ensure BTC is present\n",
    "assert \"BTC\" in df_ret.columns, \"BTC column missing\"\n",
    "\n",
    "btc_ret = df_ret[\"BTC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63edb6c7-07be-493a-896d-befe3779921d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ret.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bcfbf8-6053-4eba-852e-67b0c41afbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ret.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4c326d-2eeb-41af-9ed2-effd0319edf2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Step 1: Function to compute alpha ===\n",
    "def calc_alpha(coin_window, btc_window):\n",
    "    mask = (~np.isnan(coin_window)) & (~np.isnan(btc_window))\n",
    "    if mask.sum() < 30:   # require at least 30 valid obs in 60-day window\n",
    "        return np.nan\n",
    "    X = btc_window[mask].reshape(-1, 1)\n",
    "    y = coin_window[mask].reshape(-1, 1)\n",
    "    model = LinearRegression().fit(X, y)\n",
    "    return model.intercept_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7757f88c-12e9-429a-bce7-2112a9d05576",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Step 2: Compute rolling 90-day alpha for all coins ===\n",
    "window = 90\n",
    "alpha_dict = {}\n",
    "\n",
    "for coin in df_ret.columns:\n",
    "    if coin == \"BTC\":\n",
    "        continue\n",
    "    values = []\n",
    "    for i in range(len(df_ret)):\n",
    "        if i < window - 1:\n",
    "            values.append(np.nan)\n",
    "        else:\n",
    "            coin_window = df_ret[coin].iloc[i-window+1:i+1].values\n",
    "            btc_window  = btc_ret.iloc[i-window+1:i+1].values\n",
    "            values.append(calc_alpha(coin_window, btc_window))\n",
    "    alpha_dict[coin] = values\n",
    "\n",
    "df_alpha = pd.DataFrame(alpha_dict, index=df_ret.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb721f7-9068-4421-b516-35bfbcf4b51e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_alpha = df_alpha*100\n",
    "df_alpha = df_alpha.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0145663f-2a66-4567-8cc7-7c16164b34d0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Step 3a: Historical percentiles (within-coin over history) ===\n",
    "df_alpha_hist_pct = df_alpha.apply(lambda x: x.rank(pct=True))\n",
    "\n",
    "# === Step 3b: Cross-sectional percentiles (within-day across coins) ===\n",
    "df_alpha_xsec_pct = df_alpha.rank(axis=1, pct=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd845d2-2fa1-44e9-b8c7-6bfb2996cf60",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Step 4: Last-day rankings ===\n",
    "last_hist_pct = df_alpha_hist_pct.iloc[-1].dropna()\n",
    "last_xsec_pct = df_alpha_xsec_pct.iloc[-1].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d508d07-ee79-47ee-8409-d60b1ed1f9f8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Top/bottom 15% (historical)\n",
    "top_20_hist = last_hist_pct.nlargest(max(1, int(len(last_hist_pct) * 0.20)))\n",
    "bottom_20_hist = last_hist_pct.nsmallest(max(1, int(len(last_hist_pct) * 0.20)))\n",
    "\n",
    "# Top/bottom 10% (cross-sectional)\n",
    "top_20_xsec = last_xsec_pct.nlargest(max(1, int(len(last_xsec_pct) * 0.20)))\n",
    "bottom_20_xsec = last_xsec_pct.nsmallest(max(1, int(len(last_xsec_pct) * 0.20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e816859-55b2-4b79-924a-d5c125108001",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_hist[17:]  ## Getting the 2nd decile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6670f95-933d-418b-82ff-c13e24eb7efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_20_hist[17:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d04e1d-824b-4de8-9c32-b3b7a9f957d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_xsec[17:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d777fe68-43ba-49db-9a97-18c8e1d6ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_20_xsec[17:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b312ff-5fb0-48ca-ae20-604fed03c534",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting rolling alpha for a specfic coin\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: pick one or more coins\n",
    "coins_to_plot = [\"BNB\"]  # <-- put your coin(s) here\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for coin in coins_to_plot:\n",
    "    if coin in df_alpha.columns:\n",
    "        series = df_alpha[coin].dropna()\n",
    "        plt.plot(series.index, series.values, label=coin)\n",
    "    else:\n",
    "        print(f\"âš ï¸ Coin {coin} not found in df_alpha\")\n",
    "\n",
    "plt.title(\"Rolling 90-Day Alpha vs BTC\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Alpha\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d0779d-ebbd-4e99-bb7c-92e66379c8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plotting rolling alpha cross-sectional %tile for the same coin\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for coin in coins_to_plot:\n",
    "    if coin in df_alpha_xsec_pct.columns:\n",
    "        series = df_alpha_xsec_pct[coin].dropna()\n",
    "        plt.plot(series.index, series.values, label=coin)\n",
    "    else:\n",
    "        print(f\"âš ï¸ Coin {coin} not found in df_alpha_xsec_pct\")\n",
    "\n",
    "plt.title(\"Rolling 90-Day Cross-sectional Alpha %tile vs BTC\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Alpha\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3318dc72-0dc7-498b-abf1-dc250a679554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4da2380-0197-4899-8581-c39dd6df1b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d59fbbc-1f62-4709-a19d-b8c3b533074d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d6c2dd1-098b-4eaf-87c1-31d6f06c40ca",
   "metadata": {},
   "source": [
    "## Trend Following"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79335f16-b985-401d-b143-07f210dd077f",
   "metadata": {},
   "source": [
    "### Trend Following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88edd959-3913-4024-b077-f2cdcca76757",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Filtered list of coins which has been filtered using momentum quality = filtered_tickers_h\n",
    "## Price dataframe = df_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016654e7-c88f-44f1-b48c-599d1b1e70f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Keeping only those coins in dataframe which are in the filtered list\n",
    "df_short_h = df_h[filtered_tickers_h].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580a1940-d8ad-4d82-b5d0-0164bbbc323b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "#nw_window = 48.0 # 8 * 6 times/day\n",
    "#nw_r = 48.0   # 8 * 6 times/day\n",
    "#nw_start = 150 # 25 * 6 times/day\n",
    "#ss_wma_period = 90 # 15days * 6times/day\n",
    "#aema_period = 84 # 14days * 6times/day\n",
    "#fatl_length = 120 # 20days * 6times/day\n",
    "#jma_length = 84  # 14days * 6times/day\n",
    "#phase = 0.5\n",
    "#hold_days = 18  # 3 days * 6times/day\n",
    "\n",
    "# Parameters\n",
    "base_length = 15\n",
    "nw_start = 150 # 25 * 6 times/day\n",
    "ss_wma_period = 90 # 15days * 6times/day\n",
    "aema_period = 6*base_length # 15days * 6times/day\n",
    "fatl_length = 6*base_length # 15days * 6times/day\n",
    "jma_length = 6*base_length  # 15days * 6times/day\n",
    "nw_window = 6*base_length # 14 * 6 times/day\n",
    "nw_r = 48.0   # 8 * 6 times/day\n",
    "phase = 0.5\n",
    "hold_days = 16  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f27363-6d89-4f35-b195-91172b5be6b5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Smoothed WMA (LazyLine)\n",
    "#def smooth_wma(series, length):\n",
    "#    w2 = int(round(length / 3))\n",
    "#    w1 = int(round((length - w2) / 2))\n",
    "#    w3 = int((length - w2) / 2)\n",
    "#    l1 = series.rolling(w1).mean()\n",
    "#    l2 = l1.rolling(w2).mean()\n",
    "#    l3 = l2.rolling(w3).mean()\n",
    "#    return l3\n",
    "\n",
    "# Adaptive EMA\n",
    "def adaptive_ema(series, period):\n",
    "    ema = series.copy()\n",
    "    noise = np.zeros_like(series.values)\n",
    "    for i in range(period, len(series)):\n",
    "        sig = abs(series.iloc[i] - series.iloc[i - period])\n",
    "        noise[i] = noise[i - 1] + abs(series.iloc[i] - series.iloc[i - 1]) - abs(series.iloc[i] - series.iloc[i - period])\n",
    "        noise_val = noise[i] if noise[i] != 0 else 1\n",
    "        efratio = sig / noise_val\n",
    "        slow_end = period * 5\n",
    "        fast_end = max(period / 2.0, 1)\n",
    "        avg_period = ((sig / noise_val) * (slow_end - fast_end)) + fast_end\n",
    "        alpha = 2.0 / (1.0 + avg_period)\n",
    "        ema.iloc[i] = ema.iloc[i - 1] + alpha * (series.iloc[i] - ema.iloc[i - 1])\n",
    "    return ema\n",
    "\n",
    "# JFATL\n",
    "def jfatl(series, fatl_len, jma_len, phase):\n",
    "    fatl = series.rolling(fatl_len).mean()\n",
    "    e = 0.5 * (phase + 1)\n",
    "    wma1 = fatl.rolling(jma_len).mean()\n",
    "    wma2 = fatl.rolling(jma_len // 2).mean()\n",
    "    return wma1 * e + wma2 * (1 - e)\n",
    "\n",
    "# Vectorized Nadaraya-Watson Estimator\n",
    "def nadaraya_watson_vectorized(series, h, r, start_regression_at_bar):\n",
    "    n = len(series)\n",
    "    smoothed = np.full(n, np.nan)\n",
    "    X = np.arange(n)\n",
    "\n",
    "    for t in range(start_regression_at_bar, n):\n",
    "        indices = np.arange(0, t)\n",
    "        distances = t - indices\n",
    "        weights = (1 + (distances**2 / ((h**2) * 2 * r))) ** (-r)\n",
    "        values = series.values[:t]\n",
    "        smoothed[t] = np.sum(values * weights) / np.sum(weights)\n",
    "\n",
    "    return pd.Series(smoothed, index=series.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bd9d33-b537-470f-9793-7769b1936d58",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Indicator Calculation and Signal Generation\n",
    "indicators = {}\n",
    "buy_list = []\n",
    "short_list = []\n",
    "\n",
    "for coin in df_short_h.columns:\n",
    "    price = df_short_h[coin]\n",
    "#    ss_wma = smooth_wma(price, ss_wma_period)\n",
    "    aema = adaptive_ema(price, aema_period)\n",
    "    jfatl_val = jfatl(price, fatl_length, jma_length, phase)\n",
    "    nw_val = nadaraya_watson_vectorized(price, nw_window, nw_r, nw_start)\n",
    "\n",
    "    indicators[coin] = {\n",
    "#        \"ss_wma\": ss_wma,\n",
    "        \"aema\": aema,\n",
    "        \"jfatl\": jfatl_val,\n",
    "        \"nw\": nw_val\n",
    "    }\n",
    "\n",
    "# Composite buy/sell logic with whipsaw reduction\n",
    "#    above = (price > ss_wma) & (price > aema) & (price > jfatl_val) & (price > nw_val)\n",
    "    above = (price > aema) & (price > jfatl_val) & (price > nw_val)\n",
    "#    below = (price < ss_wma) & (price < aema) & (price < jfatl_val) & (price < nw_val)\n",
    "    below = (price < aema) & (price < jfatl_val) & (price < nw_val)\n",
    "\n",
    "    above_rolling = above.rolling(hold_days).sum()\n",
    "    below_rolling = below.rolling(hold_days).sum()\n",
    "\n",
    "    if above_rolling.iloc[-1] == hold_days:\n",
    "        buy_list.append(coin)\n",
    "    elif below_rolling.iloc[-1] == hold_days:\n",
    "        short_list.append(coin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af3953d-3568-481e-b958-46f98fa399bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad001b71-7d72-43e3-a136-e435667bcb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e098f1-610c-4b39-94f5-02c9be62d4bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate 7-day and 30-day returns\n",
    "returns_7d = df_short_h.pct_change(42).iloc[-1] * 100\n",
    "returns_30d = df_short_h.pct_change(180).iloc[-1] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa12608b-2917-492f-9007-0ae62b2948d3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to generate return tables\n",
    "def get_return_table(ticker_list, label):\n",
    "    rows = []\n",
    "    for ticker in ticker_list:\n",
    "        if ticker in returns_7d and ticker in returns_30d:\n",
    "            rows.append([ticker, f\"{returns_7d[ticker]:.2f}%\", f\"{returns_30d[ticker]:.2f}%\"])\n",
    "    if \"BTC\" in returns_7d:\n",
    "        rows.append([\"BTC\", f\"{returns_7d['BTC']:.2f}%\", f\"{returns_30d['BTC']:.2f}%\"])\n",
    "    return tabulate(rows, headers=[f\"{label} Ticker\", \"7-Day Return\", \"30-Day Return\"], tablefmt=\"pretty\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f61b6ba-e33a-4f73-b402-c0ae6443a612",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "buy_return_table = get_return_table(buy_list, \"Buy\")\n",
    "short_return_table = get_return_table(short_list, \"Short\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a15d060-5035-43bc-b7b9-992877750640",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(buy_return_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc9dac8-95fc-4cdd-974a-2cc8d5a21007",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(short_return_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcfc1e6-81f4-4182-a486-38ab1fb610b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb746ca1-efbd-42b5-9afd-d8988a04096c",
   "metadata": {},
   "source": [
    "### Return-to-trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19aaa73-8971-466c-8ce5-4c1683e26fd3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_short_h = df_h[filtered_tickers_h].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c76c79-a609-4812-a4bd-558355d0c277",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def return_to_trend_signals_percentile(df_prices, ema_period=50, slope_window=20, \n",
    "                                       dev_pct_thresh=0.2, slope_pct_thresh=0.8):\n",
    "    \"\"\"\n",
    "    Adaptive Return-to-Trend Strategy using percentile ranks across coins.\n",
    "    \n",
    "    Parameters:\n",
    "    - df_prices: Price DataFrame with datetime index and coin symbols as columns\n",
    "    - ema_period: Period for EMA smoothing\n",
    "    - slope_window: Window to compute EMA slope\n",
    "    - dev_pct_thresh: Percentile threshold for deviation (e.g., 0.2 = bottom 20%)\n",
    "    - slope_pct_thresh: Percentile threshold for slope (e.g., 0.8 = top 20%)\n",
    "\n",
    "    Returns:\n",
    "    - signals: DataFrame with 1 (buy), -1 (short), 0 (no trade)\n",
    "    - ema: EMA DataFrame\n",
    "    - ema_slope: Slope of EMA\n",
    "    - deviation: % deviation from EMA\n",
    "    - dev_rank, slope_rank: percentile ranks used for filtering\n",
    "    \"\"\"\n",
    "    # Step 1: Compute EMA\n",
    "    ema = df_prices.ewm(span=ema_period, adjust=False).mean()\n",
    "\n",
    "    # Step 2: Compute EMA Slope\n",
    "    ema_slope = ema.diff(slope_window) / slope_window\n",
    "\n",
    "    # Step 3: Compute % Deviation\n",
    "    deviation = (df_prices - ema) / ema\n",
    "\n",
    "    # Step 4: Compute cross-sectional percentile ranks\n",
    "    dev_rank = deviation.rank(axis=1, pct=True)\n",
    "    slope_rank = ema_slope.rank(axis=1, pct=True)\n",
    "\n",
    "    # Step 5: Signals\n",
    "    signals = pd.DataFrame(0, index=df_prices.index, columns=df_prices.columns)\n",
    "\n",
    "    # Buy: pulled back but strong uptrend\n",
    "    buy_condition = (dev_rank < dev_pct_thresh) & (slope_rank > slope_pct_thresh)\n",
    "    \n",
    "    # Sell: extended but strong downtrend\n",
    "    short_condition = (dev_rank > (1 - dev_pct_thresh)) & (slope_rank < (1 - slope_pct_thresh))\n",
    "\n",
    "    signals[buy_condition] = 1\n",
    "    signals[short_condition] = -1\n",
    "\n",
    "    return signals, ema, ema_slope, deviation, dev_rank, slope_rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f120a887-3e21-44b6-8493-7de42b7605c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "signals, ema, ema_slope, deviation, dev_rank, slope_rank = return_to_trend_signals_percentile(\n",
    "    df_short_h,\n",
    "    ema_period=50,\n",
    "    slope_window=30,\n",
    "    dev_pct_thresh=0.3,\n",
    "    slope_pct_thresh=0.7\n",
    ")\n",
    "\n",
    "# Get last row signals\n",
    "last_row = signals.iloc[-1]\n",
    "buy_signals = last_row[last_row == 1].index.tolist()\n",
    "sell_signals = last_row[last_row == -1].index.tolist()\n",
    "\n",
    "print(\"Buy signals:\", buy_signals)\n",
    "print(\"Sell signals:\", sell_signals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d57dcb-3b05-48ee-ab1c-bc0d4739fe6c",
   "metadata": {},
   "source": [
    "| Behavior                | Thresholds                                           |\n",
    "| ----------------------- | ---------------------------------------------------- |\n",
    "| More signals            | `dev_pct_thresh = 0.3`, `slope_pct_thresh = 0.7`     |\n",
    "| Fewer, stronger signals | `dev_pct_thresh = 0.1`, `slope_pct_thresh = 0.9`     |\n",
    "| Symmetric longs/shorts  | Use same `dev_pct_thresh` and `1 - slope_pct_thresh` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b41c901-0f16-4770-bb73-98788b168695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ade9e4-92e4-4020-94da-530bf6c19285",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEED TO WORK ON THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddacbb4-b718-4fc2-bfba-b09bf1501300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcf27a0-c6b9-4d74-bb7b-1ef5fdaf99a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4933f20f-3d66-4b61-a5f5-a6e3f47fa613",
   "metadata": {},
   "source": [
    "### Trend of Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6f7a33-77db-4792-a91b-11f16c826e4b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def trend_of_trend_signals(\n",
    "    df_prices: pd.DataFrame,\n",
    "    trend_span: int = 55,\n",
    "    slope_smooth: int = 5,\n",
    "    accel_smooth: int = 5,\n",
    "    vol_lookback: int = 120\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute trend-of-trend components and continuous scores.\n",
    "    \"\"\"\n",
    "    df = df_prices.copy().astype(float)\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna(how=\"all\")\n",
    "    df = df.fillna(method=\"ffill\")\n",
    "\n",
    "    lp = np.log(df)\n",
    "\n",
    "    # Primary trend\n",
    "    ema = lp.ewm(span=trend_span, min_periods=trend_span, adjust=False).mean()\n",
    "    slope = ema.diff().ewm(span=slope_smooth, adjust=False).mean()\n",
    "    accel = slope.diff().ewm(span=accel_smooth, adjust=False).mean()\n",
    "\n",
    "    # Vol normalization\n",
    "    logret = lp.diff()\n",
    "    vol = logret.rolling(vol_lookback, min_periods=max(20, vol_lookback // 3)).std()\n",
    "\n",
    "    slope_z = slope / vol\n",
    "    accel_z = accel / vol\n",
    "\n",
    "    # Composite score (slope + accel)\n",
    "    tot_score = 0.6 * slope_z + 0.4 * accel_z\n",
    "\n",
    "    return {\n",
    "        \"slope_z\": slope_z,\n",
    "        \"accel_z\": accel_z,\n",
    "        \"tot_score\": tot_score\n",
    "    }\n",
    "\n",
    "def last_bar_entries_topN(signal_dict, top_n: int = 15):\n",
    "    \"\"\"\n",
    "    Select top N and bottom N coins by tot_score at the last bar.\n",
    "\n",
    "    Parameters:\n",
    "      top_n : number of coins to long/short\n",
    "\n",
    "    Returns:\n",
    "      timestamp, dict with 'long_entry' and 'short_entry' coin lists\n",
    "    \"\"\"\n",
    "    tot_score = signal_dict[\"tot_score\"]\n",
    "    last_time = tot_score.index[-1]\n",
    "    last_scores = tot_score.loc[last_time].dropna()\n",
    "\n",
    "    if last_scores.empty:\n",
    "        return last_time, {\"long_entry\": [], \"short_entry\": []}\n",
    "\n",
    "    # Rank by score\n",
    "    sorted_scores = last_scores.sort_values(ascending=False)\n",
    "\n",
    "    long_entry = sorted_scores.head(top_n).index.tolist()\n",
    "    short_entry = sorted_scores.tail(top_n).index.tolist()\n",
    "\n",
    "    return last_time, {\"long_entry\": long_entry, \"short_entry\": short_entry}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84907279-a608-4c39-9add-068a29289bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_short_h is your wide DataFrame of 4-hourly prices\n",
    "signals = trend_of_trend_signals(df_short_h)\n",
    "\n",
    "ts, entries = last_bar_entries_topN(signals, top_n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5e0078-1299-45b8-9e81-4e7ae03e027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Long Entry :\", entries[\"long_entry\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0cc33e-a107-468e-9de9-748fb43a9241",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Short Entry:\", entries[\"short_entry\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5099091-90c8-41b4-bd36-59d3dd53b026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccf56a0-0405-45d4-a25c-1f7a4c249aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58596e37-1ea2-4075-b806-1e51b0526be7",
   "metadata": {},
   "source": [
    "## Long/Short"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21a633b-4abc-4111-92dd-69c347ec4062",
   "metadata": {},
   "source": [
    "### Long/Short for Each Sector - CODE NOT WORKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1c79f3-fa5f-4885-bcf7-e6ee9987960e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from statsmodels.tsa.stattools import coint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66611c26-f46f-457d-a530-1cd82b63e032",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making copy of original price data\n",
    "df_coint = df_h.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98291d2-8c8c-43e3-acd3-6bd57139366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving prices of filtered coins to list -- filtered coins comes after momentum quality filter\n",
    "df_coint = df_coint.filter(items=filtered_tickers_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f08305e-4213-473f-80ef-291e5648bd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding back the Theme column to the dataframe\n",
    "df_coint_T = df_coint.T\n",
    "df_coint_T['Theme'] = filtered_theme_values_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dc4e06-3f55-42a7-9a1f-5f7c365c1817",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a list of themes - have removed any themes which do have less than 5 coins manually \n",
    "coint_list_short = ['DEX', 'AI', 'DEPIN', 'L2', 'GAMEFI', 'RWA', 'ZKPROOF', 'CROSS_CHAIN', 'MEME', 'LENDING']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95606ab-54da-45e1-b4bb-3ffb4ef63d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate datasets for each theme\n",
    "coint_datasets = {}\n",
    "\n",
    "for theme in coint_list_short:\n",
    "    # Filter dataframe for the current theme\n",
    "    filtered_df = df_coint_T[df_coint_T['Theme'] == theme]\n",
    "    \n",
    "    # Transpose the filtered dataframe\n",
    "    transposed_df = filtered_df.T\n",
    "    \n",
    "    # Remove the Theme row (since we transposed, Theme is now a row)\n",
    "    transposed_df = transposed_df.drop('Theme', axis=0)\n",
    "    \n",
    "    # Store in dictionary with naming convention\n",
    "    coint_datasets[f'{theme}_coint_T'] = transposed_df\n",
    "    \n",
    "    # Optionally, create individual variables for each theme dataset\n",
    "    globals()[f'{theme}_coint_T'] = transposed_df\n",
    "\n",
    "# Print summary of created datasets\n",
    "print(\"Created datasets:\")\n",
    "for theme in coint_list_short:\n",
    "    dataset_name = f'{theme}_coint_T'\n",
    "    print(f\"- {dataset_name}: Shape {coint_datasets[dataset_name].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f697ea-a520-4e32-9297-20479af907d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a list of sector dataframes to look thru later\n",
    "list_sector_dfs = [DEX_coint_T, AI_coint_T, DEPIN_coint_T, L2_coint_T, GAMEFI_coint_T, RWA_coint_T, ZKPROOF_coint_T,\n",
    "                   CROSS_CHAIN_coint_T, MEME_coint_T, LENDING_coint_T]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825f1232-950b-4924-bc8d-8439f40beae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOT WORKING Properly - possibly too many calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a51ab45-0f6a-4453-825b-49611cfd117d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6f9cd5-d45f-4b5f-9929-1176770cbf03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639eb222-f9e3-4138-8fc9-61a5268a8ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85aecb4a-9fdf-4e66-b674-baa22974efe0",
   "metadata": {},
   "source": [
    "## Value Buys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e87a9c-effc-4a3c-a123-eb12b3fb957f",
   "metadata": {},
   "source": [
    "### Fresh Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403f80c2-b3d0-4e92-b3af-8e0a619af027",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the fresh momentum function\n",
    "\n",
    "def fresh_momentum(close_prices, lookback_days=60, holding_period=90):\n",
    "    returns = close_prices.pct_change(lookback_days).shift(-holding_period)\n",
    "    # Sum positive returns over holding period and subtract the negative returns\n",
    "    fm = returns.clip(lower=0).sum() - returns.clip(upper=0).sum()\n",
    "    return fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffc9242-cdbc-4d1f-8758-0a8b29fc6f45",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate fresh momentum\n",
    "fresh_momentum_df_ST = pd.DataFrame(index=filtered_tickers_h, columns = ['fresh_mom_ST'])\n",
    "for ticker in filtered_tickers_h:\n",
    "    close_prices = df_short_h[ticker]\n",
    "    fm = fresh_momentum(close_prices)\n",
    "    fresh_momentum_df_ST.loc[ticker] = fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cdfbe2-9ed2-44dd-993c-67bafa22cbe1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Distance from ATH and ATL\n",
    "\n",
    "# Calculate all-time high, low, and associated dates\n",
    "ath_values = df_short_h.max()\n",
    "ath_dates = df_short_h.idxmax()\n",
    "\n",
    "atl_values = df_short_h.min()\n",
    "atl_dates = df_short_h.idxmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3076d61-b60c-4f46-aacd-598f4360def4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "latest_prices = df_short_h.iloc[-1]\n",
    "ath_diff = ((latest_prices - ath_values) / ath_values) * 100\n",
    "atl_diff = ((latest_prices - atl_values) / atl_values) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf44307-c9b5-467e-b50a-297b4e704a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames for both ATH and ATL distance\n",
    "ath_df = pd.DataFrame({\n",
    "    \"ATH Date\": ath_dates,\n",
    "    \"Return from ATH (%)\": ath_diff\n",
    "}).sort_values(by=\"Return from ATH (%)\", ascending=False).head(25)\n",
    "\n",
    "atl_df = pd.DataFrame({\n",
    "    \"ATL Date\": atl_dates,\n",
    "    \"Return from ATL (%)\": atl_diff\n",
    "}).sort_values(by=\"Return from ATL (%)\").head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e600335d-3beb-468f-b458-5d7d9f8322cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEED TO COMBINE THE DATAFRAMES TOGETHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093a4baf-3a0f-401f-a6b1-719ab8e40f93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f167b114-b54a-4e87-b0d8-1890835ca11e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5513249e-cefb-40b2-bbf3-dbfcbf20ca9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf5eeb51-a385-445e-9a84-7e2916446df0",
   "metadata": {},
   "source": [
    "## Technical Positioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3818bd45-deb7-4542-8cda-e2a5090da272",
   "metadata": {},
   "source": [
    "### Funding Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37c07295-fa57-488f-ae59-b2bffe8f9c96",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict\n",
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1bd766b-4666-4611-a0cc-a3b33b5f57c6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Getting Funding rates for last 90 days\n",
    "class BinanceFundingRateCollector:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://fapi.binance.com\"\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'Content-Type': 'application/json',\n",
    "            'User-Agent': 'Mozilla/5.0'\n",
    "        })\n",
    "\n",
    "    def get_funding_rate_history(self, symbol: str, start_time: int, end_time: int, limit: int = 1000) -> List[Dict]:\n",
    "        url = f\"{self.base_url}/fapi/v1/fundingRate\"\n",
    "        params = {\n",
    "            \"symbol\": symbol,\n",
    "            \"startTime\": start_time,\n",
    "            \"endTime\": end_time,\n",
    "            \"limit\": limit\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = self.session.get(url, params=params)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching data for {symbol}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_all_funding_rates(self, symbol: str, days_back: int = 30) -> List[Dict]:\n",
    "        end_time = int(time.time() * 1000)\n",
    "        start_time = end_time - (days_back * 24 * 60 * 60 * 1000)\n",
    "\n",
    "        all_data = []\n",
    "        current_start = start_time\n",
    "\n",
    "        while current_start < end_time:\n",
    "            batch_end = min(current_start + (333 * 24 * 60 * 60 * 1000), end_time)\n",
    "            batch_data = self.get_funding_rate_history(symbol, current_start, batch_end, 1000)\n",
    "\n",
    "            if not batch_data:\n",
    "                break\n",
    "\n",
    "            all_data.extend(batch_data)\n",
    "\n",
    "            if len(batch_data) < 1000:\n",
    "                break\n",
    "\n",
    "            current_start = batch_data[-1]['fundingTime'] + 1\n",
    "            time.sleep(0.1)\n",
    "\n",
    "        return all_data\n",
    "\n",
    "    def process_funding_data(self, raw_data: List[Dict]) -> pd.DataFrame:\n",
    "        if not raw_data:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df = pd.DataFrame(raw_data)\n",
    "        df['fundingTime'] = pd.to_datetime(df['fundingTime'], unit='ms')\n",
    "        df['fundingRate'] = df['fundingRate'].astype(float)\n",
    "        df = df.sort_values('fundingTime').reset_index(drop=True)\n",
    "        df['fundingRatePercent'] = df['fundingRate'] * 100\n",
    "        df['annualizedRate'] = df['fundingRate'] * 365 * 3\n",
    "        df['annualizedRatePercent'] = df['annualizedRate'] * 100\n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_active_futures_symbols(self) -> List[str]:\n",
    "        url = f\"{self.base_url}/fapi/v1/exchangeInfo\"\n",
    "        try:\n",
    "            response = self.session.get(url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            symbols = []\n",
    "            for symbol_info in data['symbols']:\n",
    "                if (symbol_info['status'] == 'TRADING' and \n",
    "                    symbol_info['contractType'] == 'PERPETUAL' and\n",
    "                    symbol_info['symbol'].endswith('USDT')):\n",
    "                    symbols.append(symbol_info['symbol'])\n",
    "\n",
    "            return sorted(symbols)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching symbols: {e}\")\n",
    "            return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68d7b3ea-f93e-4337-832c-e8f85a7973c2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyze_annualized_rate_percent(days_back=90, outlier_method=\"iqr\"):\n",
    "    collector = BinanceFundingRateCollector()\n",
    "\n",
    "    # Step 1: Get list of active USDT perpetual futures\n",
    "    all_symbols = collector.get_active_futures_symbols()\n",
    "    print(f\"\\nFound {len(all_symbols)} active USDT perpetual symbols.\")\n",
    "    print(\"First 10 symbols:\", all_symbols[:10])\n",
    "\n",
    "    # Step 2: Download and process data for all symbols\n",
    "    all_data = []\n",
    "    for symbol in all_symbols:\n",
    "        try:\n",
    "            raw_data = collector.get_all_funding_rates(symbol, days_back)\n",
    "            if raw_data:\n",
    "                df = collector.process_funding_data(raw_data)\n",
    "                df['symbol'] = symbol\n",
    "                all_data.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {symbol}: {e}\")\n",
    "\n",
    "    if not all_data:\n",
    "        print(\"No data collected.\")\n",
    "        return None\n",
    "\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    # Step 3: Outlier removal based on annualizedRatePercent\n",
    "    if outlier_method == \"iqr\":\n",
    "        q1 = combined_df['annualizedRatePercent'].quantile(0.25)\n",
    "        q3 = combined_df['annualizedRatePercent'].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        filtered_df = combined_df[(combined_df['annualizedRatePercent'] >= lower_bound) & (combined_df['annualizedRatePercent'] <= upper_bound)]\n",
    "    else:\n",
    "        filtered_df = combined_df.copy()\n",
    "\n",
    "    # Step 4: Get latest value per symbol\n",
    "    latest_df = (\n",
    "        filtered_df.sort_values(\"fundingTime\")\n",
    "        .groupby(\"symbol\")\n",
    "        .tail(1)\n",
    "        .set_index(\"symbol\")\n",
    "    )\n",
    "\n",
    "    # Step 5: Compute z-score\n",
    "    latest_df['zscore_annualized'] = zscore(latest_df['annualizedRatePercent'])\n",
    "\n",
    "    # Step 6: Top/Bottom 10\n",
    "    top_10 = latest_df.sort_values(\"zscore_annualized\", ascending=False).head(10)\n",
    "    bottom_10 = latest_df.sort_values(\"zscore_annualized\", ascending=True).head(10)\n",
    "\n",
    "    print(\"\\n=== Top 10 Coins by Z-Score of Annualized Rate (%) ===\")\n",
    "    display(top_10[['annualizedRatePercent', 'zscore_annualized']])\n",
    "\n",
    "    print(\"\\n=== Bottom 10 Coins by Z-Score of Annualized Rate (%) ===\")\n",
    "    display(bottom_10[['annualizedRatePercent', 'zscore_annualized']])\n",
    "\n",
    "    return filtered_df, latest_df, top_10, bottom_10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91a39558-980e-4c3b-ba15-bca98967e841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 536 active USDT perpetual symbols.\n",
      "First 10 symbols: ['0GUSDT', '1000000BOBUSDT', '1000000MOGUSDT', '1000BONKUSDT', '1000CATUSDT', '1000CHEEMSUSDT', '1000FLOKIUSDT', '1000LUNCUSDT', '1000PEPEUSDT', '1000RATSUSDT']\n",
      "\n",
      "=== Top 10 Coins by Z-Score of Annualized Rate (%) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annualizedRatePercent</th>\n",
       "      <th>zscore_annualized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symbol</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>THETAUSDT</th>\n",
       "      <td>9.547305</td>\n",
       "      <td>2.243886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LTCUSDT</th>\n",
       "      <td>9.444375</td>\n",
       "      <td>2.199120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IDUSDT</th>\n",
       "      <td>9.434520</td>\n",
       "      <td>2.194834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROSEUSDT</th>\n",
       "      <td>9.433425</td>\n",
       "      <td>2.194358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QTUMUSDT</th>\n",
       "      <td>9.319545</td>\n",
       "      <td>2.144829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XVGUSDT</th>\n",
       "      <td>9.299835</td>\n",
       "      <td>2.136257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORDERUSDT</th>\n",
       "      <td>8.984475</td>\n",
       "      <td>1.999101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GMXUSDT</th>\n",
       "      <td>8.942865</td>\n",
       "      <td>1.981004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TLMUSDT</th>\n",
       "      <td>8.880450</td>\n",
       "      <td>1.953858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACHUSDT</th>\n",
       "      <td>8.835555</td>\n",
       "      <td>1.934333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           annualizedRatePercent  zscore_annualized\n",
       "symbol                                             \n",
       "THETAUSDT               9.547305           2.243886\n",
       "LTCUSDT                 9.444375           2.199120\n",
       "IDUSDT                  9.434520           2.194834\n",
       "ROSEUSDT                9.433425           2.194358\n",
       "QTUMUSDT                9.319545           2.144829\n",
       "XVGUSDT                 9.299835           2.136257\n",
       "ORDERUSDT               8.984475           1.999101\n",
       "GMXUSDT                 8.942865           1.981004\n",
       "TLMUSDT                 8.880450           1.953858\n",
       "ACHUSDT                 8.835555           1.934333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Bottom 10 Coins by Z-Score of Annualized Rate (%) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annualizedRatePercent</th>\n",
       "      <th>zscore_annualized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symbol</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VELODROMEUSDT</th>\n",
       "      <td>-1.322760</td>\n",
       "      <td>-2.483713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PEOPLEUSDT</th>\n",
       "      <td>-1.305240</td>\n",
       "      <td>-2.476093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0GUSDT</th>\n",
       "      <td>-1.293195</td>\n",
       "      <td>-2.470854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARPAUSDT</th>\n",
       "      <td>-1.280055</td>\n",
       "      <td>-2.465139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IPUSDT</th>\n",
       "      <td>-1.261440</td>\n",
       "      <td>-2.457043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MELANIAUSDT</th>\n",
       "      <td>-1.257060</td>\n",
       "      <td>-2.455138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SNXUSDT</th>\n",
       "      <td>-1.227495</td>\n",
       "      <td>-2.442280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NMRUSDT</th>\n",
       "      <td>-1.225305</td>\n",
       "      <td>-2.441328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ETHFIUSDT</th>\n",
       "      <td>-1.218735</td>\n",
       "      <td>-2.438470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KMNOUSDT</th>\n",
       "      <td>-1.194645</td>\n",
       "      <td>-2.427993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               annualizedRatePercent  zscore_annualized\n",
       "symbol                                                 \n",
       "VELODROMEUSDT              -1.322760          -2.483713\n",
       "PEOPLEUSDT                 -1.305240          -2.476093\n",
       "0GUSDT                     -1.293195          -2.470854\n",
       "ARPAUSDT                   -1.280055          -2.465139\n",
       "IPUSDT                     -1.261440          -2.457043\n",
       "MELANIAUSDT                -1.257060          -2.455138\n",
       "SNXUSDT                    -1.227495          -2.442280\n",
       "NMRUSDT                    -1.225305          -2.441328\n",
       "ETHFIUSDT                  -1.218735          -2.438470\n",
       "KMNOUSDT                   -1.194645          -2.427993"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filtered_data, latest_data, top_10_z, bottom_10_z = analyze_annualized_rate_percent(days_back=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbacbe2e-e2c2-4f7f-b0cf-2900d88708a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "symbol\n",
       "VELODROMEUSDT   -1.322760\n",
       "PEOPLEUSDT      -1.305240\n",
       "0GUSDT          -1.293195\n",
       "ARPAUSDT        -1.280055\n",
       "IPUSDT          -1.261440\n",
       "                   ...   \n",
       "QTUMUSDT         9.319545\n",
       "ROSEUSDT         9.433425\n",
       "IDUSDT           9.434520\n",
       "LTCUSDT          9.444375\n",
       "THETAUSDT        9.547305\n",
       "Name: annualizedRatePercent, Length: 535, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Sorting the coins by latest APR\n",
    "latest_data['annualizedRatePercent'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2c99d93-c721-4b2d-9442-6baed44f76c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.to_excel('funding rate history.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c322bf-7f62-48a9-bde5-fa49eb462d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Identify coins which have Funding rates outside their historical +/1 SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5016a1f0-fc56-49e8-9806-97b1cb458b32",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Need to clean the data as fundingtime is slightly different for different coins (often differ by seconds)\n",
    "### So consolidating by date and hour for easy comparison\n",
    "\n",
    "def reshape_funding_data_by_hour(filtered_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reshapes filtered funding data into a time-indexed DataFrame with symbols as columns\n",
    "    and hourly funding rate values (annualizedRatePercent).\n",
    "    \"\"\"\n",
    "    if filtered_data.empty:\n",
    "        raise ValueError(\"Input DataFrame is empty.\")\n",
    "\n",
    "    # Ensure fundingTime is datetime\n",
    "    filtered_data['fundingTime'] = pd.to_datetime(filtered_data['fundingTime'])\n",
    "\n",
    "    # Round fundingTime to the hour\n",
    "    filtered_data['fundingHour'] = filtered_data['fundingTime'].dt.floor('H')\n",
    "\n",
    "    # Group by symbol and fundingHour, average in case of duplicates\n",
    "    grouped = (\n",
    "        filtered_data.groupby(['fundingHour', 'symbol'])['annualizedRatePercent']\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Pivot: rows = fundingHour, columns = symbol\n",
    "    pivot_df = grouped.pivot(index='fundingHour', columns='symbol', values='annualizedRatePercent')\n",
    "\n",
    "    # Optional: sort index and columns\n",
    "    pivot_df = pivot_df.sort_index().sort_index(axis=1)\n",
    "\n",
    "    return pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a6874b2-1c7e-4b40-8cbb-bd22a5c30b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>symbol</th>\n",
       "      <th>0GUSDT</th>\n",
       "      <th>1000000BOBUSDT</th>\n",
       "      <th>1000000MOGUSDT</th>\n",
       "      <th>1000BONKUSDT</th>\n",
       "      <th>1000CATUSDT</th>\n",
       "      <th>1000CHEEMSUSDT</th>\n",
       "      <th>1000FLOKIUSDT</th>\n",
       "      <th>1000LUNCUSDT</th>\n",
       "      <th>1000PEPEUSDT</th>\n",
       "      <th>1000RATSUSDT</th>\n",
       "      <th>...</th>\n",
       "      <th>ZETAUSDT</th>\n",
       "      <th>ZILUSDT</th>\n",
       "      <th>ZKCUSDT</th>\n",
       "      <th>ZKJUSDT</th>\n",
       "      <th>ZKUSDT</th>\n",
       "      <th>ZORAUSDT</th>\n",
       "      <th>ZRCUSDT</th>\n",
       "      <th>ZROUSDT</th>\n",
       "      <th>ZRXUSDT</th>\n",
       "      <th>å¸å®‰äººç”ŸUSDT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fundingHour</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-11-12 00:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.475</td>\n",
       "      <td>5.475</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.475</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.91104</td>\n",
       "      <td>5.475</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.475</td>\n",
       "      <td>3.80622</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.475</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-12 01:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-12 02:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-12 03:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-12 04:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.475</td>\n",
       "      <td>5.475</td>\n",
       "      <td>4.4676</td>\n",
       "      <td>5.475</td>\n",
       "      <td>5.475</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.475</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.475</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 535 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "symbol               0GUSDT  1000000BOBUSDT  1000000MOGUSDT  1000BONKUSDT  \\\n",
       "fundingHour                                                                 \n",
       "2025-11-12 00:00:00     NaN           5.475           5.475           NaN   \n",
       "2025-11-12 01:00:00     NaN             NaN             NaN           NaN   \n",
       "2025-11-12 02:00:00     NaN             NaN             NaN           NaN   \n",
       "2025-11-12 03:00:00     NaN             NaN             NaN           NaN   \n",
       "2025-11-12 04:00:00     NaN           5.475           5.475        4.4676   \n",
       "\n",
       "symbol               1000CATUSDT  1000CHEEMSUSDT  1000FLOKIUSDT  1000LUNCUSDT  \\\n",
       "fundingHour                                                                     \n",
       "2025-11-12 00:00:00          NaN           5.475            NaN           NaN   \n",
       "2025-11-12 01:00:00          NaN             NaN            NaN           NaN   \n",
       "2025-11-12 02:00:00          NaN             NaN            NaN           NaN   \n",
       "2025-11-12 03:00:00          NaN             NaN            NaN           NaN   \n",
       "2025-11-12 04:00:00        5.475           5.475            NaN           NaN   \n",
       "\n",
       "symbol               1000PEPEUSDT  1000RATSUSDT  ...  ZETAUSDT  ZILUSDT  \\\n",
       "fundingHour                                      ...                      \n",
       "2025-11-12 00:00:00       0.91104         5.475  ...       NaN      NaN   \n",
       "2025-11-12 01:00:00           NaN           NaN  ...       NaN      NaN   \n",
       "2025-11-12 02:00:00           NaN           NaN  ...       NaN      NaN   \n",
       "2025-11-12 03:00:00           NaN           NaN  ...       NaN      NaN   \n",
       "2025-11-12 04:00:00           NaN           NaN  ...       NaN      NaN   \n",
       "\n",
       "symbol               ZKCUSDT  ZKJUSDT   ZKUSDT  ZORAUSDT  ZRCUSDT  ZROUSDT  \\\n",
       "fundingHour                                                                  \n",
       "2025-11-12 00:00:00      NaN    5.475  3.80622       NaN    5.475      NaN   \n",
       "2025-11-12 01:00:00      NaN      NaN      NaN       NaN      NaN      NaN   \n",
       "2025-11-12 02:00:00      NaN      NaN      NaN       NaN      NaN      NaN   \n",
       "2025-11-12 03:00:00      NaN      NaN      NaN       NaN      NaN      NaN   \n",
       "2025-11-12 04:00:00      NaN    5.475      NaN       NaN    5.475      NaN   \n",
       "\n",
       "symbol               ZRXUSDT  å¸å®‰äººç”ŸUSDT  \n",
       "fundingHour                             \n",
       "2025-11-12 00:00:00      NaN     5.475  \n",
       "2025-11-12 01:00:00      NaN       NaN  \n",
       "2025-11-12 02:00:00      NaN       NaN  \n",
       "2025-11-12 03:00:00      NaN       NaN  \n",
       "2025-11-12 04:00:00      NaN     5.475  \n",
       "\n",
       "[5 rows x 535 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_annualized_df = reshape_funding_data_by_hour(filtered_data)\n",
    "\n",
    "# Display a preview\n",
    "hourly_annualized_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0ccd335-8200-4dc9-ae07-f430f825aeb2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "def compute_latest_zscores_and_highlight(df_hourly_annualized):\n",
    "    \"\"\"\n",
    "    Compute latest z-scores for each coin vs its own history,\n",
    "    and return coins with extreme high/low scores.\n",
    "    \"\"\"\n",
    "    # Compute z-scores column-wise (axis=0), ignoring NaNs\n",
    "    zscore_df = df_hourly_annualized.apply(lambda x: zscore(x.dropna()), axis=0)\n",
    "    \n",
    "    # Reindex back to original for proper alignment\n",
    "    zscore_df = zscore_df.reindex(df_hourly_annualized.index)\n",
    "\n",
    "    # Get the latest z-score for each coin (last non-NaN value)\n",
    "    latest_zscores = zscore_df.ffill().iloc[-1]\n",
    "\n",
    "    # Highlight coins\n",
    "    high_threshold = 0.9\n",
    "    low_threshold = -0.9\n",
    "\n",
    "    top_coins = latest_zscores[latest_zscores > high_threshold].sort_values(ascending=False)\n",
    "    bottom_coins = latest_zscores[latest_zscores < low_threshold].sort_values()\n",
    "\n",
    "    print(\"\\n=== Coins with High Z-Score ( > 0.9 ) ===\")\n",
    "    display(top_coins.to_frame(\"z-score\"))\n",
    "\n",
    "    print(\"\\n=== Coins with Low Z-Score ( < -0.9 ) ===\")\n",
    "    display(bottom_coins.to_frame(\"z-score\"))\n",
    "\n",
    "    return latest_zscores, top_coins, bottom_coins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12334484-5ed2-47f7-a892-1df4174e92ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Coins with High Z-Score ( > 0.9 ) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>z-score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symbol</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HIPPOUSDT</th>\n",
       "      <td>2.477735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZBTUSDT</th>\n",
       "      <td>2.443936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>API3USDT</th>\n",
       "      <td>2.049253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LTCUSDT</th>\n",
       "      <td>1.845779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QTUMUSDT</th>\n",
       "      <td>1.800281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XVGUSDT</th>\n",
       "      <td>1.767533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROSEUSDT</th>\n",
       "      <td>1.644634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARCUSDT</th>\n",
       "      <td>1.597592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IDUSDT</th>\n",
       "      <td>1.582578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORDERUSDT</th>\n",
       "      <td>1.460908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MTLUSDT</th>\n",
       "      <td>1.458718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZILUSDT</th>\n",
       "      <td>1.444965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>THETAUSDT</th>\n",
       "      <td>1.439828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALICEUSDT</th>\n",
       "      <td>1.432853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SUPERUSDT</th>\n",
       "      <td>1.400052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOTUSDT</th>\n",
       "      <td>1.390747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LINKUSDT</th>\n",
       "      <td>1.353045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGLDUSDT</th>\n",
       "      <td>1.345053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USDCUSDT</th>\n",
       "      <td>1.312211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RDNTUSDT</th>\n",
       "      <td>1.289912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MUSDT</th>\n",
       "      <td>1.289561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GMXUSDT</th>\n",
       "      <td>1.285955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BICOUSDT</th>\n",
       "      <td>1.269363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACHUSDT</th>\n",
       "      <td>1.212239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRVUSDT</th>\n",
       "      <td>1.190443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWARMSUSDT</th>\n",
       "      <td>1.189154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IOTAUSDT</th>\n",
       "      <td>1.187492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PHBUSDT</th>\n",
       "      <td>1.178295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZENUSDT</th>\n",
       "      <td>1.101239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MINAUSDT</th>\n",
       "      <td>1.073858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LRCUSDT</th>\n",
       "      <td>1.069564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MYXUSDT</th>\n",
       "      <td>1.067626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CFXUSDT</th>\n",
       "      <td>1.027050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MIRAUSDT</th>\n",
       "      <td>1.022761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TLMUSDT</th>\n",
       "      <td>1.020901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GTCUSDT</th>\n",
       "      <td>1.006636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANKRUSDT</th>\n",
       "      <td>1.003205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XMRUSDT</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DENTUSDT</th>\n",
       "      <td>0.980189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TURTLEUSDT</th>\n",
       "      <td>0.918238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             z-score\n",
       "symbol              \n",
       "HIPPOUSDT   2.477735\n",
       "ZBTUSDT     2.443936\n",
       "API3USDT    2.049253\n",
       "LTCUSDT     1.845779\n",
       "QTUMUSDT    1.800281\n",
       "XVGUSDT     1.767533\n",
       "ROSEUSDT    1.644634\n",
       "ARCUSDT     1.597592\n",
       "IDUSDT      1.582578\n",
       "ORDERUSDT   1.460908\n",
       "MTLUSDT     1.458718\n",
       "ZILUSDT     1.444965\n",
       "THETAUSDT   1.439828\n",
       "ALICEUSDT   1.432853\n",
       "SUPERUSDT   1.400052\n",
       "HOTUSDT     1.390747\n",
       "LINKUSDT    1.353045\n",
       "AGLDUSDT    1.345053\n",
       "USDCUSDT    1.312211\n",
       "RDNTUSDT    1.289912\n",
       "MUSDT       1.289561\n",
       "GMXUSDT     1.285955\n",
       "BICOUSDT    1.269363\n",
       "ACHUSDT     1.212239\n",
       "CRVUSDT     1.190443\n",
       "SWARMSUSDT  1.189154\n",
       "IOTAUSDT    1.187492\n",
       "PHBUSDT     1.178295\n",
       "ZENUSDT     1.101239\n",
       "MINAUSDT    1.073858\n",
       "LRCUSDT     1.069564\n",
       "MYXUSDT     1.067626\n",
       "CFXUSDT     1.027050\n",
       "MIRAUSDT    1.022761\n",
       "TLMUSDT     1.020901\n",
       "GTCUSDT     1.006636\n",
       "ANKRUSDT    1.003205\n",
       "XMRUSDT     1.000000\n",
       "DENTUSDT    0.980189\n",
       "TURTLEUSDT  0.918238"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Coins with Low Z-Score ( < -0.9 ) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>z-score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symbol</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BANANAS31USDT</th>\n",
       "      <td>-11.717709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CCUSDT</th>\n",
       "      <td>-7.937254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KASUSDT</th>\n",
       "      <td>-6.746988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NMRUSDT</th>\n",
       "      <td>-6.511674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VELODROMEUSDT</th>\n",
       "      <td>-6.427129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1INCHUSDT</th>\n",
       "      <td>-0.983484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOMEUSDT</th>\n",
       "      <td>-0.976185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOGEUSDT</th>\n",
       "      <td>-0.941720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IOTXUSDT</th>\n",
       "      <td>-0.926521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHRUSDT</th>\n",
       "      <td>-0.923208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 z-score\n",
       "symbol                  \n",
       "BANANAS31USDT -11.717709\n",
       "CCUSDT         -7.937254\n",
       "KASUSDT        -6.746988\n",
       "NMRUSDT        -6.511674\n",
       "VELODROMEUSDT  -6.427129\n",
       "...                  ...\n",
       "1INCHUSDT      -0.983484\n",
       "BOMEUSDT       -0.976185\n",
       "DOGEUSDT       -0.941720\n",
       "IOTXUSDT       -0.926521\n",
       "CHRUSDT        -0.923208\n",
       "\n",
       "[114 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "latest_z, top_z, bottom_z = compute_latest_zscores_and_highlight(hourly_annualized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de8fb30-3432-4e21-a345-ffad3195c044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82df1514-6cd3-4603-a6e1-6aa86a6b74cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Open Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47d65b43-3d39-40e5-ae40-5d676f44cf88",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "from typing import List, Dict, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ddfb490-7397-4b4e-9e8d-f7ff9af3638b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BinanceOpenInterestCollector:\n",
    "    def __init__(self, sleep_between_calls: float = 0.12):\n",
    "        self.base_url = \"https://fapi.binance.com\"\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"User-Agent\": \"Mozilla/5.0\"\n",
    "        })\n",
    "        self.sleep_between_calls = sleep_between_calls\n",
    "        # endpoint for historical open interest\n",
    "        self.oi_endpoint = \"/futures/data/openInterestHist\"\n",
    "\n",
    "    def _safe_get(self, url: str, params: dict, timeout: int = 15) -> Optional[requests.Response]:\n",
    "        \"\"\"\n",
    "        lightweight retry for transient failures and rate-limit (429/418).\n",
    "        Keeps behaviour simple â€” you can replace with a more robust backoff if needed.\n",
    "        \"\"\"\n",
    "        max_retries = 5\n",
    "        backoff_base = 1.8\n",
    "        attempt = 0\n",
    "        while attempt <= max_retries:\n",
    "            try:\n",
    "                r = self.session.get(url, params=params, timeout=timeout)\n",
    "            except Exception as e:\n",
    "                attempt += 1\n",
    "                sleep_for = backoff_base ** attempt\n",
    "                print(f\"[{datetime.now(timezone.utc).isoformat()}] Request exception: {e}. retry in {sleep_for:.1f}s\")\n",
    "                time.sleep(sleep_for)\n",
    "                continue\n",
    "\n",
    "            if 200 <= r.status_code < 300:\n",
    "                return r\n",
    "\n",
    "            if r.status_code in (429, 418):\n",
    "                retry_after = r.headers.get(\"Retry-After\")\n",
    "                if retry_after:\n",
    "                    try:\n",
    "                        wait = float(retry_after)\n",
    "                    except Exception:\n",
    "                        wait = backoff_base ** (attempt + 1)\n",
    "                else:\n",
    "                    wait = backoff_base ** (attempt + 1)\n",
    "                print(f\"[{datetime.now(timezone.utc).isoformat()}] Rate limited ({r.status_code}). waiting {wait:.1f}s (attempt {attempt}/{max_retries})\")\n",
    "                attempt += 1\n",
    "                time.sleep(wait)\n",
    "                continue\n",
    "\n",
    "            if 500 <= r.status_code < 600:\n",
    "                wait = backoff_base ** (attempt + 1)\n",
    "                print(f\"[{datetime.now(timezone.utc).isoformat()}] Server error {r.status_code}. wait {wait:.1f}s\")\n",
    "                attempt += 1\n",
    "                time.sleep(wait)\n",
    "                continue\n",
    "\n",
    "            # other client error: return response so caller can inspect\n",
    "            print(f\"[{datetime.now(timezone.utc).isoformat()}] HTTP {r.status_code}: {r.text[:400]}\")\n",
    "            return r\n",
    "\n",
    "        print(f\"[{datetime.now(timezone.utc).isoformat()}] Exceeded retries for {params.get('symbol')}\")\n",
    "        return None\n",
    "\n",
    "    def get_open_interest_history(self,\n",
    "                                  symbol: str,\n",
    "                                  period: str = \"1d\",\n",
    "                                  start_time: Optional[int] = None,\n",
    "                                  end_time: Optional[int] = None,\n",
    "                                  limit: int = 1000) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Call Binance /futures/data/openInterestHist\n",
    "\n",
    "        params:\n",
    "          - symbol: e.g. \"SOLUSDT\"\n",
    "          - period: \"5m\",\"15m\",\"1h\",\"4h\",\"1d\" (depends on API availability)\n",
    "          - start_time, end_time: milliseconds since epoch (optional)\n",
    "          - limit: number of records (max depends on endpoint; 1000 is typical)\n",
    "        \"\"\"\n",
    "        url = self.base_url + self.oi_endpoint\n",
    "        params = {\"symbol\": symbol, \"period\": period, \"limit\": limit}\n",
    "        if start_time is not None:\n",
    "            params[\"startTime\"] = int(start_time)\n",
    "        if end_time is not None:\n",
    "            params[\"endTime\"] = int(end_time)\n",
    "\n",
    "        resp = self._safe_get(url, params)\n",
    "        if resp is None:\n",
    "            return []\n",
    "        try:\n",
    "            return resp.json()\n",
    "        except Exception as e:\n",
    "            print(f\"Error decoding JSON for {symbol}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_all_open_interest(self, symbol: str, days_back: int = 30, period: str = \"1d\") -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Collect historical open interest for `symbol` going back `days_back`.\n",
    "        Uses startTime/endTime windowing and paginates by advancing start to last timestamp + 1 ms.\n",
    "        \"\"\"\n",
    "        end_time = int(time.time() * 1000)\n",
    "        start_time = end_time - (days_back * 24 * 60 * 60 * 1000)\n",
    "        all_data: List[Dict] = []\n",
    "        current_start = start_time\n",
    "\n",
    "        # We use a conservative batch window size (in ms): fetch up to 1000 records per call\n",
    "        # The endpoint returns aggregated entries depending on `period`. We advance using last timestamp.\n",
    "        while current_start < end_time:\n",
    "            batch_end = min(current_start + (333 * 24 * 60 * 60 * 1000), end_time)\n",
    "            batch = self.get_open_interest_history(\n",
    "                symbol=symbol,\n",
    "                period=period,\n",
    "                start_time=current_start,\n",
    "                end_time=batch_end,\n",
    "                limit=1000\n",
    "            )\n",
    "\n",
    "            if not batch:\n",
    "                # either no data in this window or an error occurred\n",
    "                break\n",
    "\n",
    "            all_data.extend(batch)\n",
    "\n",
    "            # if fewer than limit returned, we likely reached end for this range\n",
    "            if len(batch) < 1000:\n",
    "                break\n",
    "\n",
    "            # try to advance using the last returned timestamp; different responses may use different keys\n",
    "            last = batch[-1]\n",
    "            # common timestamp keys: 'timestamp', 'time', 'startTime'\n",
    "            ts_keys = ['timestamp', 'time', 'startTime']\n",
    "            last_ts = None\n",
    "            for k in ts_keys:\n",
    "                if k in last:\n",
    "                    try:\n",
    "                        last_ts = int(last[k])\n",
    "                        break\n",
    "                    except Exception:\n",
    "                        pass\n",
    "            if last_ts is None:\n",
    "                # fallback: if response had 'sumOpenInterest' only with no timestamp we must stop to avoid infinite loop\n",
    "                print(f\"No timestamp in last batch for {symbol}, stopping pagination.\")\n",
    "                break\n",
    "\n",
    "            # advance one ms beyond last_ts to avoid repeating\n",
    "            current_start = last_ts + 1\n",
    "            time.sleep(self.sleep_between_calls)\n",
    "\n",
    "        return all_data\n",
    "\n",
    "    def process_oi_data(self, raw_data: List[Dict], symbol: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convert raw openInterestHist JSON list into a DataFrame with sane columns.\n",
    "        Handles common field names returned by Binance's endpoint.\n",
    "        \"\"\"\n",
    "        if not raw_data:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df = pd.DataFrame(raw_data)\n",
    "\n",
    "        # determine timestamp column (ms)\n",
    "        ts_col = None\n",
    "        for c in ['timestamp', 'time', 'startTime', 'openTime']:\n",
    "            if c in df.columns:\n",
    "                ts_col = c\n",
    "                break\n",
    "\n",
    "        if ts_col is not None:\n",
    "            df['ts'] = pd.to_datetime(df[ts_col].astype(int), unit='ms', utc=True)\n",
    "        else:\n",
    "            # If there is no timestamp column, create an index-based timestamp (unlikely)\n",
    "            df['ts'] = pd.NaT\n",
    "\n",
    "        # common OI fields: sumOpenInterest, sumOpenInterestValue, avgOpenInterest\n",
    "        # fallback possible: 'openInterest' (if endpoint variation)\n",
    "        if 'sumOpenInterest' in df.columns:\n",
    "            df['sumOpenInterest'] = pd.to_numeric(df['sumOpenInterest'], errors='coerce')\n",
    "        elif 'openInterest' in df.columns:\n",
    "            df['sumOpenInterest'] = pd.to_numeric(df['openInterest'], errors='coerce')\n",
    "\n",
    "        if 'sumOpenInterestValue' in df.columns:\n",
    "            df['sumOpenInterestValue'] = pd.to_numeric(df['sumOpenInterestValue'], errors='coerce')\n",
    "        elif 'openInterestValue' in df.columns:\n",
    "            df['sumOpenInterestValue'] = pd.to_numeric(df['openInterestValue'], errors='coerce')\n",
    "\n",
    "        # optional average open interest\n",
    "        if 'avgOpenInterest' in df.columns:\n",
    "            df['avgOpenInterest'] = pd.to_numeric(df['avgOpenInterest'], errors='coerce')\n",
    "\n",
    "        # attach symbol if provided (helpful when concatenating multiple symbols)\n",
    "        if symbol:\n",
    "            df['symbol'] = symbol\n",
    "        elif 'symbol' not in df.columns:\n",
    "            # ensure symbol column exists to keep downstream code simple\n",
    "            df['symbol'] = None\n",
    "\n",
    "        # sort and reset\n",
    "        df = df.sort_values('ts').reset_index(drop=True)\n",
    "\n",
    "        # helpful derived metrics (example): convert sumOpenInterestValue to float, compute per-contract USD if available\n",
    "        # keep only useful columns\n",
    "        keep_cols = [c for c in ['symbol', 'ts', 'sumOpenInterest', 'avgOpenInterest', 'sumOpenInterestValue'] if c in df.columns]\n",
    "        return df[keep_cols].copy()\n",
    "\n",
    "    def get_active_futures_symbols(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Same code as your funding collector: returns USDT perpetuals that are TRADING.\n",
    "        \"\"\"\n",
    "        url = f\"{self.base_url}/fapi/v1/exchangeInfo\"\n",
    "        try:\n",
    "            resp = self.session.get(url, timeout=15)\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()\n",
    "            symbols = []\n",
    "            for s in data.get('symbols', []):\n",
    "                if (s.get('status') == 'TRADING'\n",
    "                        and s.get('contractType') == 'PERPETUAL'\n",
    "                        and s.get('symbol', '').endswith('USDT')):\n",
    "                    symbols.append(s['symbol'])\n",
    "            return sorted(symbols)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching symbols: {e}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "def collect_all_symbols_oi(days_back: int = 30, period: str = \"1d\") -> (pd.DataFrame, pd.DataFrame):\n",
    "    \"\"\"\n",
    "    High level function similar to your analyze_annualized_rate_percent:\n",
    "    - find active symbols\n",
    "    - fetch historical OI for each\n",
    "    - return combined_df and latest_df (latest row per symbol)\n",
    "    \"\"\"\n",
    "    collector = BinanceOpenInterestCollector()\n",
    "    all_symbols = collector.get_active_futures_symbols()\n",
    "    print(f\"Found {len(all_symbols)} active USDT perpetual symbols. Sample: {all_symbols[:10]}\")\n",
    "\n",
    "    all_dfs = []\n",
    "    for symbol in all_symbols:\n",
    "        try:\n",
    "            raw = collector.get_all_open_interest(symbol, days_back=days_back, period=period)\n",
    "            if raw:\n",
    "                df = collector.process_oi_data(raw, symbol=symbol)\n",
    "                if not df.empty:\n",
    "                    all_dfs.append(df)\n",
    "            # polite pause between symbols (avoid bursts)\n",
    "            time.sleep(0.05)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {symbol} due to error: {e}\")\n",
    "\n",
    "    if not all_dfs:\n",
    "        print(\"No open interest data collected.\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "    # get latest row per symbol\n",
    "    latest_df = (\n",
    "        combined_df.sort_values(\"ts\")\n",
    "        .groupby(\"symbol\", as_index=False)\n",
    "        .last()\n",
    "        .set_index(\"symbol\")\n",
    "    )\n",
    "\n",
    "    return combined_df, latest_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df469661-c7e2-4899-8c71-e522cf0ce05c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 536 active USDT perpetual symbols. Sample: ['0GUSDT', '1000000BOBUSDT', '1000000MOGUSDT', '1000BONKUSDT', '1000CATUSDT', '1000CHEEMSUSDT', '1000FLOKIUSDT', '1000LUNCUSDT', '1000PEPEUSDT', '1000RATSUSDT']\n",
      "Combined shape: (94535, 4)\n",
      "Latest (first 10):\n",
      "                                             ts  sumOpenInterest  \\\n",
      "symbol                                                             \n",
      "0GUSDT         1970-01-23 22:56:48.640000+00:00     1.905355e+07   \n",
      "1000000BOBUSDT 1970-01-23 22:56:48.640000+00:00     3.427406e+07   \n",
      "1000000MOGUSDT 1970-01-23 22:56:48.640000+00:00     3.351486e+06   \n",
      "1000BONKUSDT   1970-01-23 22:56:48.640000+00:00     1.913374e+09   \n",
      "1000CATUSDT    1970-01-23 22:56:48.640000+00:00     2.770911e+08   \n",
      "1000CHEEMSUSDT 1970-01-23 22:56:48.640000+00:00     1.008620e+10   \n",
      "1000FLOKIUSDT  1970-01-23 22:56:48.640000+00:00     1.311590e+08   \n",
      "1000LUNCUSDT   1970-01-23 22:56:48.640000+00:00     8.005249e+07   \n",
      "1000PEPEUSDT   1970-01-23 22:56:48.640000+00:00     1.639576e+10   \n",
      "1000RATSUSDT   1970-01-23 22:56:48.640000+00:00     1.673818e+08   \n",
      "\n",
      "                sumOpenInterestValue  \n",
      "symbol                                \n",
      "0GUSDT                  2.689218e+07  \n",
      "1000000BOBUSDT          1.534857e+06  \n",
      "1000000MOGUSDT          1.193464e+06  \n",
      "1000BONKUSDT            2.376921e+07  \n",
      "1000CATUSDT             1.200082e+06  \n",
      "1000CHEEMSUSDT          1.220138e+07  \n",
      "1000FLOKIUSDT           7.967907e+06  \n",
      "1000LUNCUSDT            2.844048e+06  \n",
      "1000PEPEUSDT            9.613934e+07  \n",
      "1000RATSUSDT            6.721199e+06  \n"
     ]
    }
   ],
   "source": [
    "from pandas.api.types import is_datetime64_any_dtype, is_datetime64tz_dtype\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage: collect 30 days of daily OI\n",
    "    combined, latest = collect_all_symbols_oi(days_back=30, period=\"4h\")\n",
    "    print(\"Combined shape:\", combined.shape)\n",
    "    print(\"Latest (first 10):\")\n",
    "    print(latest.head(10))\n",
    "    \n",
    "    # Ensure datetimes are timezone-naive before writing to Excel\n",
    "    if 'ts' in combined.columns:\n",
    "        combined['ts'] = combined['ts'].dt.tz_convert('UTC').dt.tz_localize(None)\n",
    "    if 'ts' in latest.columns:\n",
    "        latest['ts'] = latest['ts'].dt.tz_convert('UTC').dt.tz_localize(None)\n",
    "\n",
    "    # Save to parquet/csv as needed\n",
    "    combined.to_excel(\"binance_oi_combined.xlsx\", index=False)\n",
    "    latest.to_csv(\"binance_oi_latest.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "333876c5-777d-4024-8ea8-773c7e824a4e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Getting total OI values and market share for each date\n",
    "# 1) ensure ts is datetime and sort\n",
    "combined = combined.copy()\n",
    "combined['ts'] = pd.to_datetime(combined['ts'], utc=True)   # keep timezone if present\n",
    "combined = combined.sort_values(['symbol', 'ts']).reset_index(drop=True)\n",
    "\n",
    "# 2) tot_OI_val = total sumOpenInterestValue across all symbols for each ts\n",
    "combined['tot_OI_val'] = combined.groupby('ts')['sumOpenInterestValue'].transform('sum')\n",
    "\n",
    "# 3) %_OI_val = share (%) of each symbol's sumOpenInterestValue for that ts\n",
    "#    If tot_OI_val == 0 (rare), produce 0 to avoid divide-by-zero\n",
    "combined['%_OI_val'] = combined.apply(\n",
    "    lambda r: 0.0 if pd.isna(r['tot_OI_val']) or r['tot_OI_val'] == 0 else (r['sumOpenInterestValue'] / r['tot_OI_val']) * 100,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# optional: round % to e.g. 6 decimal places (comment out if you prefer raw)\n",
    "combined['%_OI_val'] = combined['%_OI_val'].round(6)\n",
    "combined['tot_OI_val'] = combined['tot_OI_val'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd6a2533-3485-4b5d-b542-37ab815418a0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4) Create market_share_last: last available row per symbol\n",
    "# Use groupby + idxmax or last after sorting. We'll use groupby(...).last()\n",
    "last_rows = (combined.sort_values('ts')\n",
    "                      .groupby('symbol', as_index=False)\n",
    "                      .last()[['symbol', 'ts', 'sumOpenInterest', 'sumOpenInterestValue', 'tot_OI_val', '%_OI_val']])\n",
    "\n",
    "# name the last-date % as market_share_last_pct for clarity\n",
    "last_rows = last_rows.rename(columns={\n",
    "    'sumOpenInterest': 'sumOpenInterest_last',\n",
    "    'sumOpenInterestValue': 'sumOpenInterestValue_last',\n",
    "    'tot_OI_val': 'tot_OI_val_at_last',\n",
    "    '%_OI_val': 'market_share_last_pct'   # percent on that last date\n",
    "})\n",
    "\n",
    "# 5) Compute market_share_avg using each symbol's last 29 observations\n",
    "def sum_last_n_per_group(df, n=29):\n",
    "    # returns DataFrame with columns ['symbol', 'last_n_sum']\n",
    "    res = (df.sort_values('ts')\n",
    "             .groupby('symbol')\n",
    "             .apply(lambda g: g.tail(n)['sumOpenInterestValue'].sum())\n",
    "             .reset_index(name='last29_sum'))\n",
    "    return res\n",
    "\n",
    "last29 = sum_last_n_per_group(combined, n=29)\n",
    "\n",
    "# total across all coins of their last29 sums (denominator)\n",
    "total_last29_sum = last29['last29_sum'].sum()\n",
    "# handle case where total is 0 to avoid division by zero\n",
    "if total_last29_sum == 0:\n",
    "    last29['market_share_avg'] = 0.0\n",
    "else:\n",
    "    last29['market_share_avg'] = (last29['last29_sum'] / total_last29_sum) * 100\n",
    "\n",
    "# merge market_share_avg into last_rows\n",
    "market_share_last = last_rows.merge(last29[['symbol', 'last29_sum', 'market_share_avg']], on='symbol', how='left')\n",
    "\n",
    "# Fill NaN market_share_avg with 0 if a symbol had no rows (shouldn't normally happen)\n",
    "market_share_last['market_share_avg'] = market_share_last['market_share_avg'].fillna(0.0)\n",
    "\n",
    "# 6) compute market_share_diff = market_share_last_pct - market_share_avg\n",
    "market_share_last['market_share_last_pct'] = market_share_last['market_share_last_pct'].astype(float)\n",
    "market_share_last['market_share_diff'] = market_share_last['market_share_last_pct'] - market_share_last['market_share_avg']\n",
    "\n",
    "# optional rounding\n",
    "market_share_last['market_share_last_pct'] = market_share_last['market_share_last_pct'].round(6)\n",
    "market_share_last['market_share_avg'] = market_share_last['market_share_avg'].round(6)\n",
    "market_share_last['market_share_diff'] = market_share_last['market_share_diff'].round(6)\n",
    "\n",
    "# reorder columns for readability\n",
    "market_share_last = market_share_last[[\n",
    "    'symbol', 'ts',\n",
    "    'sumOpenInterest_last', 'sumOpenInterestValue_last', 'tot_OI_val_at_last',\n",
    "    'market_share_last_pct',\n",
    "    'last29_sum', 'market_share_avg', 'market_share_diff'\n",
    "]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f55819e-c785-4333-850e-651160d73d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>ts</th>\n",
       "      <th>sumOpenInterest</th>\n",
       "      <th>sumOpenInterestValue</th>\n",
       "      <th>tot_OI_val</th>\n",
       "      <th>%_OI_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0GUSDT</td>\n",
       "      <td>1969-12-25 02:56:48.640000+00:00</td>\n",
       "      <td>6660355.0</td>\n",
       "      <td>1.508059e+07</td>\n",
       "      <td>2.409650e+10</td>\n",
       "      <td>0.062584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0GUSDT</td>\n",
       "      <td>1969-12-25 06:56:48.640000+00:00</td>\n",
       "      <td>6601983.0</td>\n",
       "      <td>1.481211e+07</td>\n",
       "      <td>2.351452e+10</td>\n",
       "      <td>0.062991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0GUSDT</td>\n",
       "      <td>1969-12-25 10:56:48.640000+00:00</td>\n",
       "      <td>6689714.0</td>\n",
       "      <td>1.517294e+07</td>\n",
       "      <td>2.357094e+10</td>\n",
       "      <td>0.064371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0GUSDT</td>\n",
       "      <td>1969-12-25 14:56:48.640000+00:00</td>\n",
       "      <td>6684473.0</td>\n",
       "      <td>1.561055e+07</td>\n",
       "      <td>2.409773e+10</td>\n",
       "      <td>0.064780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0GUSDT</td>\n",
       "      <td>1969-12-25 18:56:48.640000+00:00</td>\n",
       "      <td>6733841.0</td>\n",
       "      <td>1.583732e+07</td>\n",
       "      <td>2.403357e+10</td>\n",
       "      <td>0.065897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   symbol                               ts  sumOpenInterest  \\\n",
       "0  0GUSDT 1969-12-25 02:56:48.640000+00:00        6660355.0   \n",
       "1  0GUSDT 1969-12-25 06:56:48.640000+00:00        6601983.0   \n",
       "2  0GUSDT 1969-12-25 10:56:48.640000+00:00        6689714.0   \n",
       "3  0GUSDT 1969-12-25 14:56:48.640000+00:00        6684473.0   \n",
       "4  0GUSDT 1969-12-25 18:56:48.640000+00:00        6733841.0   \n",
       "\n",
       "   sumOpenInterestValue    tot_OI_val  %_OI_val  \n",
       "0          1.508059e+07  2.409650e+10  0.062584  \n",
       "1          1.481211e+07  2.351452e+10  0.062991  \n",
       "2          1.517294e+07  2.357094e+10  0.064371  \n",
       "3          1.561055e+07  2.409773e+10  0.064780  \n",
       "4          1.583732e+07  2.403357e+10  0.065897  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63c2455d-7cb0-43f3-b1dc-3dbc9601277f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>ts</th>\n",
       "      <th>sumOpenInterest_last</th>\n",
       "      <th>sumOpenInterestValue_last</th>\n",
       "      <th>tot_OI_val_at_last</th>\n",
       "      <th>market_share_last_pct</th>\n",
       "      <th>last29_sum</th>\n",
       "      <th>market_share_avg</th>\n",
       "      <th>market_share_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0GUSDT</td>\n",
       "      <td>1970-01-23 22:56:48.640000+00:00</td>\n",
       "      <td>1.905355e+07</td>\n",
       "      <td>2.689218e+07</td>\n",
       "      <td>2.271071e+10</td>\n",
       "      <td>0.118412</td>\n",
       "      <td>8.374683e+08</td>\n",
       "      <td>0.123625</td>\n",
       "      <td>-0.005213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000000BOBUSDT</td>\n",
       "      <td>1970-01-23 22:56:48.640000+00:00</td>\n",
       "      <td>3.427406e+07</td>\n",
       "      <td>1.534857e+06</td>\n",
       "      <td>2.271071e+10</td>\n",
       "      <td>0.006758</td>\n",
       "      <td>2.224384e+07</td>\n",
       "      <td>0.003284</td>\n",
       "      <td>0.003474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000000MOGUSDT</td>\n",
       "      <td>1970-01-23 22:56:48.640000+00:00</td>\n",
       "      <td>3.351486e+06</td>\n",
       "      <td>1.193464e+06</td>\n",
       "      <td>2.271071e+10</td>\n",
       "      <td>0.005255</td>\n",
       "      <td>3.721204e+07</td>\n",
       "      <td>0.005493</td>\n",
       "      <td>-0.000238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000BONKUSDT</td>\n",
       "      <td>1970-01-23 22:56:48.640000+00:00</td>\n",
       "      <td>1.913374e+09</td>\n",
       "      <td>2.376921e+07</td>\n",
       "      <td>2.271071e+10</td>\n",
       "      <td>0.104661</td>\n",
       "      <td>6.835836e+08</td>\n",
       "      <td>0.100909</td>\n",
       "      <td>0.003752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000CATUSDT</td>\n",
       "      <td>1970-01-23 22:56:48.640000+00:00</td>\n",
       "      <td>2.770911e+08</td>\n",
       "      <td>1.200082e+06</td>\n",
       "      <td>2.271071e+10</td>\n",
       "      <td>0.005284</td>\n",
       "      <td>3.658135e+07</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>-0.000116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           symbol                               ts  sumOpenInterest_last  \\\n",
       "0          0GUSDT 1970-01-23 22:56:48.640000+00:00          1.905355e+07   \n",
       "1  1000000BOBUSDT 1970-01-23 22:56:48.640000+00:00          3.427406e+07   \n",
       "2  1000000MOGUSDT 1970-01-23 22:56:48.640000+00:00          3.351486e+06   \n",
       "3    1000BONKUSDT 1970-01-23 22:56:48.640000+00:00          1.913374e+09   \n",
       "4     1000CATUSDT 1970-01-23 22:56:48.640000+00:00          2.770911e+08   \n",
       "\n",
       "   sumOpenInterestValue_last  tot_OI_val_at_last  market_share_last_pct  \\\n",
       "0               2.689218e+07        2.271071e+10               0.118412   \n",
       "1               1.534857e+06        2.271071e+10               0.006758   \n",
       "2               1.193464e+06        2.271071e+10               0.005255   \n",
       "3               2.376921e+07        2.271071e+10               0.104661   \n",
       "4               1.200082e+06        2.271071e+10               0.005284   \n",
       "\n",
       "     last29_sum  market_share_avg  market_share_diff  \n",
       "0  8.374683e+08          0.123625          -0.005213  \n",
       "1  2.224384e+07          0.003284           0.003474  \n",
       "2  3.721204e+07          0.005493          -0.000238  \n",
       "3  6.835836e+08          0.100909           0.003752  \n",
       "4  3.658135e+07          0.005400          -0.000116  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "market_share_last.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59c24460-be68-4fb9-a370-8487e5228619",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 7) Sort market_share_last by market_share_diff (largest to smallest)\n",
    "market_share_last_sorted = market_share_last.sort_values(\n",
    "    by='market_share_diff', ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# 8) Extract top and bottom 20 (symbol + value)\n",
    "top_20 = market_share_last_sorted.head(20)[['symbol', 'market_share_diff']]\n",
    "bottom_20 = market_share_last_sorted.tail(20)[['symbol', 'market_share_diff']]\n",
    "\n",
    "# Convert to lists of tuples for easy further use\n",
    "top_20_list = list(top_20.itertuples(index=False, name=None))\n",
    "bottom_20_list = list(bottom_20.itertuples(index=False, name=None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24e4f06f-a9df-4e42-868d-0af6d3301a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BTCUSDT', 0.788426),\n",
       " ('UNIUSDT', 0.295123),\n",
       " ('XRPUSDT', 0.085914),\n",
       " ('WLFIUSDT', 0.068061),\n",
       " ('LSKUSDT', 0.056209),\n",
       " ('BCHUSDT', 0.049367),\n",
       " ('BNBUSDT', 0.047193),\n",
       " ('ALLOUSDT', 0.031535),\n",
       " ('TRXUSDT', 0.029345),\n",
       " ('POPCATUSDT', 0.02542),\n",
       " ('ALCHUSDT', 0.019655),\n",
       " ('METUSDT', 0.018276),\n",
       " ('LTCUSDT', 0.016597),\n",
       " ('HIPPOUSDT', 0.016577),\n",
       " ('AEROUSDT', 0.015429),\n",
       " ('RESOLVUSDT', 0.015384),\n",
       " ('1000PEPEUSDT', 0.013488),\n",
       " ('MELANIAUSDT', 0.013297),\n",
       " ('JCTUSDT', 0.012125),\n",
       " ('PROMUSDT', 0.012036)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_20_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e767aa6f-c908-4d88-a8b6-ce6583dccb25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('TIAUSDT', -0.016967),\n",
       " ('GIGGLEUSDT', -0.017476),\n",
       " ('PAXGUSDT', -0.017923),\n",
       " ('ARUSDT', -0.017986),\n",
       " ('ZEREBROUSDT', -0.018568),\n",
       " ('VIRTUALUSDT', -0.01977),\n",
       " ('DASHUSDT', -0.019919),\n",
       " ('AAVEUSDT', -0.020207),\n",
       " ('ZENUSDT', -0.021343),\n",
       " ('XPLUSDT', -0.027417),\n",
       " ('AIAUSDT', -0.027474),\n",
       " ('ETCUSDT', -0.031013),\n",
       " ('DOTUSDT', -0.035193),\n",
       " ('ICPUSDT', -0.039308),\n",
       " ('PUMPUSDT', -0.039633),\n",
       " ('SUIUSDT', -0.053047),\n",
       " ('ZECUSDT', -0.087028),\n",
       " ('FILUSDT', -0.107054),\n",
       " ('NEARUSDT', -0.109134),\n",
       " ('ETHUSDT', -0.584454)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottom_20_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8afc0f-c9aa-473e-a844-9ed742721c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d400c8a-f477-4730-8a7f-7b067080c20b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e60028c-a7a8-4d02-bbd0-b1febcf92961",
   "metadata": {},
   "source": [
    "### Liquidations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8ea138-394d-419f-acc1-886e1c556950",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WORKING ON CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9c8c9a-22a2-4b16-9490-d0403317e75a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2273ad5-fa7e-4a66-99b4-dbc90311f434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff4dad6-8076-4160-a1b1-67771e62aa00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962905fc-30ca-483f-9e6a-085e6f901106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65803196-1f8a-4f13-9f4b-f7945339fda6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58619681-68a7-4f23-b8de-a688e0b9b3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3798e96-87e4-49f0-96b2-213de3434e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473d0850-20cd-493d-95f9-c61bae999088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d324712-b376-467c-805e-59474348c58f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318e3657-06d9-4499-adad-c139706fc914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c044556-9b2e-4d41-9682-0bce2275227b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8ba760-d978-4395-ad51-78eae5637bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235f34cf-e31d-4643-b281-dafba169403a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2905f85a-3e5c-439c-b102-cecd1df7e71f",
   "metadata": {},
   "source": [
    "### Institutional vs. Retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3b7ca1-d0c1-47b5-ac13-157d8088a531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bde01b-01a4-4bb7-94ad-8b8f6043f0e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f93922-8a83-4ad1-a76b-27f2cffd5888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53275d2-cbb7-47eb-a931-bb46516c39f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d056000d-b5db-4455-bf93-a67eac197cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1c461d-8920-419d-8cbe-f9b9d9225140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c7fb7e-18e9-4b0f-bb9b-ed854bfe13f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bb32771-b6b2-4ed1-a77c-1dac1a790ff6",
   "metadata": {},
   "source": [
    "## Fundamental Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5c6e0e-bfd8-4bd1-bdeb-5658d939d48e",
   "metadata": {},
   "source": [
    "### TVL Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48f31bf-4b24-46dd-b808-ee8225b771d6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from defillama2 import DefiLlama\n",
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "from collections import Counter\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2da79f-95b4-4fa3-9f7d-2e46862ac175",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### PROTOCOL ANALYSIS ############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d578009-121c-4e6c-a5f6-58ac516bea5a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## This code can be run once a quarter to update the list of coins\n",
    "# Create a DefiLlama instance\n",
    "#obj = DefiLlama()\n",
    "\n",
    "## Get fundamentals for all protocols\n",
    "#df_protocol = obj.get_protocols_fundamentals()\n",
    "#df_protocol = df_protocol.sort_values('tvl', ascending=False, ignore_index=True)\n",
    "#df_protocol.to_excel('defillama_test.xlsx')\n",
    "\n",
    "#df_prot = df_protocol[['name', 'symbol', 'category', 'tvl','change_7d']]\n",
    "\n",
    "## Exclude Protocols with TVL less than USD 15mn\n",
    "#df_prot = df_prot[df_prot['tvl'] > 15000000]\n",
    "\n",
    "## Exclude Protocols where symbols are '-'\n",
    "#df_prot_shortlst = df_prot[df_prot['symbol'] != '-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15f6a01-2e22-4dbe-80cc-af4aaa9c1d5b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Read the list of protocol names from excel - this includes the following filter:\n",
    "## 1. TVL > 50mn\n",
    "## 2. Delete protocols without coins\n",
    "## 3. Keep categories: CDP, Derivatives, Dexs, Lending, Liquid Staking, Restaking, Yield, Yield Aggregator\n",
    "## 4. Merge CDP & Lending into one category, Liquid Staking and Restaking together, Yield and Yield Aggregator, Derivatives and DEX together\n",
    "## 5. Manually delete coins which are unknown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddcab0b-8fc7-47e0-8c67-4c9d688a7bd5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Note: It is possible to get the historical TVL for a protocol using the API format below for each protocol\n",
    "## url = https://api.llama.fi/protocol/lido\n",
    "## responses = requests.get(url)\n",
    "## data = responses.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cac76ea-45c8-4f4e-ab5f-36f9eef1144b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path to the Excel file\n",
    "protocol_file_path = r'C:\\Users\\jhuku\\OneDrive\\Documents\\2. Crypto Research\\Crypto Portfolio\\Protocol_links.xlsx'\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "df_links = pd.read_excel(protocol_file_path, sheet_name = 'TVL')\n",
    "\n",
    "# Step 2: Define the 6-month cutoff date\n",
    "cutoff_date = pd.Timestamp.now() - relativedelta(months=6)\n",
    "\n",
    "# Step 3: Initialize the combined DataFrame\n",
    "combined_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00725fe-edd0-4a18-a837-d09429437921",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Iterate through the protocols with tqdm progress bar\n",
    "for _, row in tqdm(df_links.iterrows(), total=len(df_links), desc=\"Downloading Protocol Data\"):\n",
    "    symbol = row['Symbol']\n",
    "    url = row['Url']\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        # Extract the TVL data\n",
    "        tvl_data = data.get(\"tvl\", [])\n",
    "        df = pd.DataFrame(tvl_data)\n",
    "\n",
    "        # Handle possible alternate column names\n",
    "        if 'totalLiquidityUSD' not in df.columns and 'totalLiquidity' in df.columns:\n",
    "            df.rename(columns={'totalLiquidity': 'totalLiquidityUSD'}, inplace=True)\n",
    "\n",
    "        if df.empty or 'date' not in df.columns or 'totalLiquidityUSD' not in df.columns:\n",
    "            continue\n",
    "\n",
    "        # Convert date and filter\n",
    "        df['date'] = pd.to_datetime(df['date'], unit='s')\n",
    "        df = df[df['date'] >= cutoff_date]\n",
    "\n",
    "        # Keep only date and liquidity, rename liquidity to symbol\n",
    "        df = df[['date', 'totalLiquidityUSD']].rename(columns={'totalLiquidityUSD': symbol})\n",
    "        df.set_index('date', inplace=True)\n",
    "\n",
    "        # Merge into combined dataframe\n",
    "        combined_df = combined_df.join(df, how='outer')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {symbol} ({url}): {e}\")\n",
    "\n",
    "    # Step 5: Sleep to avoid overloading API\n",
    "    time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25892316-2a1b-4f5f-830d-291e9f8cc0b7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Final formatting\n",
    "combined_df = combined_df.sort_index().reset_index()\n",
    "combined_df.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d706dd-7357-4e79-adb8-ea119e6d9049",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get current date at midnight UTC\n",
    "#current_date_midnight = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "\n",
    "# Get yesterday's date at midnight UTC\n",
    "yesterday_midnight = (datetime.now() - timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "\n",
    "# Filter the dataframe\n",
    "#combined_df = combined_df[combined_df.index <= current_date_midnight]\n",
    "combined_df = combined_df[combined_df.index <= yesterday_midnight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6868cc59-be6d-4ea5-a5fc-526c71ae2a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9ae42d-86af-463c-b85d-255e22bf18fe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_list = combined_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cf3f5b-b359-42c5-8dbb-5fe1b56e5472",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a dictionary from Symbol to Category for all coins\n",
    "symbol_to_category = dict(zip(df_links['Symbol'], df_links['Category']))\n",
    "\n",
    "# Lookup each symbol in combined_list using the dictionary - in case data is not downloaded for some coins\n",
    "df_categories = [symbol_to_category[symbol] for symbol in combined_list if symbol in symbol_to_category]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f612dbd7-9cd8-44e5-91d7-d277c181b160",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Adding back the Theme column to the dataframe.T\n",
    "df_T = combined_df.T\n",
    "df_T['category'] = df_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5beb21-a900-4b7d-acc1-6c92967482ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_T.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedc4d58-196c-4b8b-a6ef-f21a0342e20c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Getting unique category values\n",
    "category_list_short = list(set(df_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8460d77c-478d-47db-a275-b04ef53cdab9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create separate datasets for each category\n",
    "category_datasets = {}\n",
    "\n",
    "for category in category_list_short:\n",
    "    # Filter dataframe for the current category\n",
    "    filtered_df = df_T[df_T['category'] == category]\n",
    "    \n",
    "    # Transpose the filtered dataframe\n",
    "    transposed_df = filtered_df.T\n",
    "    \n",
    "    # Remove the category row (since we transposed, category is now a row)\n",
    "    transposed_df = transposed_df.drop('category', axis=0)\n",
    "    \n",
    "    # Store in dictionary with naming convention\n",
    "    category_datasets[f'{category}_df_T'] = transposed_df\n",
    "    \n",
    "    # Optionally, create individual variables for each category dataset\n",
    "    globals()[f'{category}_df_T'] = transposed_df\n",
    "\n",
    "# Print summary of created datasets\n",
    "print(\"Created datasets:\")\n",
    "for category in category_list_short:\n",
    "    dataset_name = f'{category}_df_T'\n",
    "    print(f\"- {dataset_name}: Shape {category_datasets[dataset_name].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba3ce74-c313-40de-89f7-faf5c65a4aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access a specific category dataset\n",
    "dex_df_T = category_datasets['Dexs_df_T']  # if 'DEX' is in your category_list_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b0041f-ec1c-442c-a523-70e826e414a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dex_df_T.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4e27ae-2b63-4ff1-8bd2-2013f4c012f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your dictionary of DataFrames (example naming convention: category_df_T)\n",
    "# category_datasets = {\n",
    "#     'dex': df_dex,\n",
    "#     'lending': df_lending,\n",
    "#     ...\n",
    "# }\n",
    "\n",
    "# Dictionary to store processed DataFrames\n",
    "percent_tvl_dfs = {}\n",
    "\n",
    "# Iterate through each category DataFrame\n",
    "for category, df in category_datasets.items():\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Calculate total TVL for each date\n",
    "    df['total_tvl'] = df.sum(axis=1)\n",
    "    \n",
    "    # Calculate % TVL for each coin\n",
    "    percent_df = df.drop(columns='total_tvl').div(df['total_tvl'], axis=0) * 100\n",
    "    \n",
    "    # Store the result\n",
    "    percent_tvl_dfs[category] = percent_df\n",
    "\n",
    "# Export to Excel: each sheet is a category\n",
    "with pd.ExcelWriter(\"category_percent_tvl.xlsx\") as writer:\n",
    "    for category, percent_df in percent_tvl_dfs.items():\n",
    "        percent_df.to_excel(writer, sheet_name=category[:31])  # Excel sheet names limited to 31 characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359f13d1-69ec-4ba2-85a8-f5ab171530b3",
   "metadata": {},
   "source": [
    "### Valuations - MarketCap/TVL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e183b975-0f35-4ea7-afdd-5f0289383346",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca0f6d1-6ae5-44b7-9923-a81342651136",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path to the Excel file\n",
    "protocol_file_path = r'C:\\Users\\jhuku\\OneDrive\\Documents\\2. Crypto Research\\Crypto Portfolio\\Protocol_links.xlsx'\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "df_links = pd.read_excel(protocol_file_path, sheet_name = 'Mcap')\n",
    "\n",
    "# Step 2: Define the 6-month cutoff date\n",
    "cutoff_date = pd.Timestamp.now() - relativedelta(months=6)\n",
    "\n",
    "# Step 3: Initialize the combined DataFrame\n",
    "combined_df_m = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4744c004-db9b-4273-815b-71b8904f62be",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "defillama_symbols = df_links['Symbol'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce8a817-36c8-4269-a46a-5e4abe3044a6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "coin_list = requests.get(\"https://api.coingecko.com/api/v3/coins/list\").json()\n",
    "symbol_to_id = {c['symbol']: c['id'] for c in coin_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece93549-56cb-4b86-bac7-4ab84a7a0565",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ids = [symbol_to_id[s.lower()] for s in defillama_symbols if s.lower() in symbol_to_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1283ea-a1a7-41e8-891c-e02e10cc6161",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if ids:\n",
    "    params = {'vs_currency':'usd', 'ids':','.join(ids), 'order':'market_cap_desc', 'sparkline':False}\n",
    "    resp = requests.get(\"https://api.coingecko.com/api/v3/coins/markets\", params=params)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "else:\n",
    "    raise ValueError(\"No valid CoinGecko IDs found for provided symbols.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fcd724-8821-4298-8ea7-078e35f4edc1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df_market_test = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95e1d5a-c71c-43cf-8a3d-2fa41f2658f1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Keeping only the symbol and market cap columns\n",
    "df_market_caps = df_market_test[['symbol','market_cap']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8399fdff-85a5-4957-b7f4-3f5adbeed5e6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the 'Name' column to uppercase\n",
    "df_market_caps['symbol'] = df_market_caps['symbol'].str.upper()\n",
    "\n",
    "## Replacing the index with symbol column\n",
    "df_market_caps.set_index(['symbol'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e789fed-844e-480c-9666-08099652f994",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Getting the latest TVL values previously calculated\n",
    "TVL_latest = combined_df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4677aff2-76ca-42c8-b7bc-751027d04a80",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Renaming the index of TVL df\n",
    "df_renamed_axis = TVL_latest.T.rename_axis('symbol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34807d2e-19d4-4eda-ab06-ac7c6ec3aebe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Merge TVL and Market Cap dfs\n",
    "merged_df = pd.merge(df_renamed_axis, df_market_caps, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89e69ae-4117-4bc9-b890-b1c549389189",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Renaming TVL column properly and removing any Nan values\n",
    "merged_df.rename(columns={merged_df.columns[0]: 'TVL'}, inplace=True)\n",
    "merged_df = merged_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a7c16a-b33d-4114-ab39-938ba59a7ac1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Calculating valuation - Marketcap/TVL\n",
    "merged_df['mcap/TVL'] = merged_df['market_cap']/merged_df['TVL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac0dc1d-3518-4c3b-af8b-418ce964ec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.sort_values(by='mcap/TVL', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c6896c-0e12-4bfe-a7c8-ce48f5aa5352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77127822-b594-4d8b-ba29-5b367c112684",
   "metadata": {},
   "source": [
    "### DefiLlama Fees Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da421c89-285e-49e9-9c7f-49c456fb6c5f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = 'https://api.llama.fi/overview/fees?excludeTotalDataChart=true&excludeTotalDataChartBreakdown=true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2edcf73-aebf-459c-ba68-97baa4093a05",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6457f5d2-b7ff-421f-96dd-8c122adddbf4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract protocol data\n",
    "protocols = data.get(\"protocols\", [])\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_fees = pd.DataFrame(protocols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb95f77-49c2-4b4c-9b6a-9773a7e4a705",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Step 1: Split into chain_fees and protocol_fees based on protocolType ===\n",
    "chain_fees = df_fees[df_fees['protocolType'] == 'chain'].copy()\n",
    "protocol_fees = df_fees[df_fees['protocolType'] == 'protocol'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815575f2-66b8-40d2-ad83-a0fcc80af348",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Step 2: Clean 'parentProtocol' column in protocol_fees ===\n",
    "protocol_fees['parentProtocol'] = protocol_fees['parentProtocol'].str.replace('parent#', '', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cdf00c-b709-4a04-9098-f5235a9ef96f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Step 3: Replace blanks in parentProtocol with slug values ===\n",
    "protocol_fees['parentProtocol'] = protocol_fees['parentProtocol'].fillna(protocol_fees['slug'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f7fb0e-d743-4d29-8ae2-258b22a2b9eb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Step 4: Keep only required columns ===\n",
    "keep_cols = ['total24h', 'total7d', 'total30d', 'name', 'category', 'slug', 'parentProtocol']\n",
    "chain_fees = chain_fees[keep_cols]\n",
    "protocol_fees = protocol_fees[keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28df4d11-3a0b-4489-bfcd-6d1e4431d053",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sort by parentProtocol and total30d descending (so highest total30d per group is first)\n",
    "protocol_fees_sorted = protocol_fees.sort_values(\n",
    "    by=[\"parentProtocol\", \"total30d\"], ascending=[True, False]\n",
    ")\n",
    "\n",
    "# Group by parentProtocol and aggregate\n",
    "protocol_fees_consolidated = (\n",
    "    protocol_fees_sorted.groupby(\"parentProtocol\", as_index=False).agg({\n",
    "        \"total24h\": \"sum\",\n",
    "        \"total7d\": \"sum\",\n",
    "        \"total30d\": \"sum\",\n",
    "        \"name\": \"first\",       # take from row with highest total30d (since sorted)\n",
    "        \"category\": \"first\",\n",
    "        \"slug\": \"first\"\n",
    "    })\n",
    ")\n",
    "\n",
    "# Optional: reset index just in case\n",
    "protocol_fees_consolidated.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6ecbc1-b0ad-4951-81db-4a0caefd0791",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Step 5: Chain fees market share calculations ===\n",
    "for col in ['total24h', 'total7d', 'total30d']:\n",
    "    total_sum = chain_fees[col].sum()\n",
    "    chain_fees[f\"{col}_share\"] = chain_fees[col] / total_sum\n",
    "\n",
    "# Market share change: 7d - 30d\n",
    "chain_fees['share_change'] = chain_fees['total7d_share'] - chain_fees['total30d_share']\n",
    "\n",
    "# Top 20 chains by share_change\n",
    "top20_chains = chain_fees.sort_values('share_change', ascending=False).head(20)['name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242fdd1b-52da-4baa-97f8-049806893fbb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Step 6: Protocol fees market share calculations ===\n",
    "for col in ['total24h', 'total7d', 'total30d']:\n",
    "    total_sum = protocol_fees_consolidated[col].sum()\n",
    "    protocol_fees_consolidated[f\"{col}_share\"] = protocol_fees_consolidated[col] / total_sum\n",
    "\n",
    "# Market share change: 7d - 30d\n",
    "protocol_fees_consolidated['share_change'] = protocol_fees_consolidated['total7d_share']- protocol_fees_consolidated['total30d_share']\n",
    "\n",
    "# Top 20 protocols by share_change (using parentProtocol)\n",
    "top20_protocols = protocol_fees_consolidated.sort_values('share_change', ascending=False).head(20)['parentProtocol'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9c7782-808b-40ff-877d-5ed97f982343",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 20 Chains:\", top20_chains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91efc45-0bd2-4242-95eb-2fb2e981de46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 20 Protocols:\", top20_protocols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b980fa-c171-4fc8-8e3e-a25229ebe87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the values for a specific Chain\n",
    "input_str = \"eth\" ##Chain name\n",
    "chain_fees_filter = chain_fees[chain_fees[\"name\"].str.contains(input_str, case=False, na=False)]\n",
    "chain_fees_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee76c185-eab1-4efb-994a-6f68c1e31891",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the values for a specific protocol\n",
    "input_str = \"jupiter\" ##Protocol name\n",
    "protocol_fees_filter = protocol_fees_consolidated[protocol_fees_consolidated[\"parentProtocol\"].\n",
    "                                                  str.contains(input_str, case=False, na=False)]\n",
    "protocol_fees_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed363bb-9b77-435d-9e61-6051f768e405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95afb89c-9395-4e01-8ce2-72085228106c",
   "metadata": {},
   "source": [
    "### DefiLlama Revenue Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f0efb7-6c6f-4cb1-af92-d6093aa3c590",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = 'https://api.llama.fi/overview/fees?excludeTotalDataChart=true&excludeTotalDataChartBreakdown=true&dataType=dailyRevenue'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95285c9-38a4-4e71-abb4-985143c25186",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc234149-ae25-4f29-bb84-f5841243da49",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract protocol data\n",
    "protocols = data.get(\"protocols\", [])\n",
    "# Convert to DataFrame\n",
    "df_rev = pd.DataFrame(protocols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9570ab-8690-44d5-a0b6-1c87085debff",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Step 1: Split into chain_rev and protocol_rev based on protocolType ===\n",
    "chain_rev = df_rev[df_rev['protocolType'] == 'chain'].copy()\n",
    "protocol_rev = df_rev[df_rev['protocolType'] == 'protocol'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2dbb7a-31c6-4c37-8d53-8e4da2869213",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Step 2: Clean 'parentProtocol' column in protocol_rev ===\n",
    "protocol_rev['parentProtocol'] = protocol_rev['parentProtocol'].str.replace('parent#', '', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26b4ef3-9fc8-42ac-a67c-b4f1513b1d43",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Step 3: Replace blanks in parentProtocol with slug values ===\n",
    "protocol_rev['parentProtocol'] = protocol_rev['parentProtocol'].fillna(protocol_rev['slug'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd8f96a-8436-4cb3-bef7-124db6b9f4f7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Step 4: Keep only required columns ===\n",
    "keep_cols = ['total24h', 'total7d', 'total30d', 'name', 'category', 'slug', 'parentProtocol']\n",
    "chain_rev = chain_rev[keep_cols]\n",
    "protocol_rev = protocol_rev[keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9097f53b-04ee-42c1-b193-b47d2243f113",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sort by parentProtocol and total30d descending (so highest total30d per group is first)\n",
    "protocol_rev_sorted = protocol_rev.sort_values(\n",
    "    by=[\"parentProtocol\", \"total30d\"], ascending=[True, False]\n",
    ")\n",
    "\n",
    "# Group by parentProtocol and aggregate\n",
    "protocol_rev_consolidated = (\n",
    "    protocol_rev_sorted.groupby(\"parentProtocol\", as_index=False).agg({\n",
    "        \"total24h\": \"sum\",\n",
    "        \"total7d\": \"sum\",\n",
    "        \"total30d\": \"sum\",\n",
    "        \"name\": \"first\",       # take from row with highest total30d (since sorted)\n",
    "        \"category\": \"first\",\n",
    "        \"slug\": \"first\"\n",
    "    })\n",
    ")\n",
    "\n",
    "# Optional: reset index just in case\n",
    "protocol_rev_consolidated.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fde6bf4-2ee4-455d-b327-b503508f71b3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Step 5: Chain rev market share calculations ===\n",
    "for col in ['total24h', 'total7d', 'total30d']:\n",
    "    total_sum = chain_rev[col].sum()\n",
    "    chain_rev[f\"{col}_share\"] = chain_rev[col] / total_sum\n",
    "    \n",
    "# Market share change: 7d - 30d\n",
    "chain_rev['share_change'] = chain_rev['total7d_share'] - chain_rev['total30d_share']\n",
    "\n",
    "# Top 20 chains by share_change\n",
    "top20_chains = chain_rev.sort_values('share_change', ascending=False).head(20)['name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab4e6e3-8b42-4d62-a916-6270f12a5576",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Step 6: Protocol rev market share calculations ===\n",
    "for col in ['total24h', 'total7d', 'total30d']:\n",
    "    total_sum = protocol_rev_consolidated[col].sum()\n",
    "    protocol_rev_consolidated[f\"{col}_share\"] = protocol_rev_consolidated[col] / total_sum\n",
    "\n",
    "# Market share change: 7d - 30d\n",
    "protocol_rev_consolidated['share_change'] = protocol_rev_consolidated['total7d_share'] - protocol_rev_consolidated['total30d_share']\n",
    "\n",
    "# Top 20 protocols by share_change (using parentProtocol)\n",
    "top20_protocols = protocol_rev_consolidated.sort_values('share_change', ascending=False).head(20)['parentProtocol'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e1840c-daa8-4a52-a19b-5d59f1060709",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 20 Chains:\", top20_chains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8524ecaf-3fb5-4f90-a008-efc07e205a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 20 Protocols:\", top20_protocols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a638e78-881a-4de5-a58d-5733925c1913",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the values for a specific Chain\n",
    "input_str = \"bnb\" ##Chain name\n",
    "chain_rev_filter = chain_rev[chain_rev[\"name\"].str.contains(input_str, case=False, na=False)]\n",
    "chain_rev_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df92da5e-e248-4c4c-8820-bae2fa091a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the values for a specific protocol\n",
    "input_str = \"jupiter\" ##Protocol name\n",
    "protocol_rev_filter = protocol_rev_consolidated[protocol_rev_consolidated[\"parentProtocol\"].str.contains(input_str, case=False, na=False)]\n",
    "protocol_rev_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e5e760-1d10-4d98-8bbf-3a3337c3f28e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdedc1fe-e231-4e5e-a16d-2960cff094f2",
   "metadata": {},
   "source": [
    "### Whale vs. Retail Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b43079-747d-4a3e-87a5-66d06056d56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://app.alphractal.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5baf84-cb9c-402e-962c-cc8c49550821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd187fd-9ff6-4a92-9fbe-495c24176361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9b6391-3712-4dba-a83c-c2fef8e5ede4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30fb7e2-5915-4f6d-bfd9-fa210611aaf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01de1f29-c27c-4a1a-a10e-9ce0d87e9cef",
   "metadata": {},
   "source": [
    "### Artemis/Onchain Wallet Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6c6b39-a2f7-4d24-9e92-88e404f85d33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbd2df0-becd-442d-86d9-9aeeb8a65c58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a323e65f-923f-41d2-a25d-940a057d4e38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc14ac1d-dabb-4adb-a8a4-48e4787ddfbc",
   "metadata": {},
   "source": [
    "## Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb61b8c-3243-490e-96c1-abfbea0e79a8",
   "metadata": {},
   "source": [
    "### Tail Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ecf08e-6138-474f-afb0-6388f40ce41a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1752bcf0-dfa9-4e09-a5ee-e0cf9346136b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d1c9c0-59ff-428c-9a24-63f3f787697a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0030f0b-eb1c-4beb-9e81-341b1491db3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d16fc1d-2330-4092-b80e-12d66e91b1d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007ebd29-9e17-4dd7-b227-6a10baab62a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d62f2e1b-5883-4392-8c2f-b301e26e9b0c",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Premia Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9fcad7-bb57-47da-b20a-0a5820182b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8281ea75-fcc7-4a51-95b6-dbda4f6a63ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c95363-eace-4712-8d29-b13b76473f08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0934aaa-6253-4a45-b6c1-58bcb433fc1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48b4b87-7c0f-4948-8f7c-f1ef75f81cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0f56cd-8e63-4304-ab27-63cc876785b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a7a5562-8568-47f2-9083-cc87b8690e04",
   "metadata": {},
   "source": [
    "### Term Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3672690-4eeb-4b13-9e19-e930fd5262ab",
   "metadata": {},
   "source": [
    "#### Current 25D Implied Vol and 25D skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f043180-0464-45b4-b206-8727aa7e0eaa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import math\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "BASE_URL = \"https://www.deribit.com/api/v2\"\n",
    "\n",
    "def safe_get(url, params=None):\n",
    "    r = requests.get(url, params=params)\n",
    "    data = r.json()\n",
    "    if \"error\" in data:\n",
    "        raise ValueError(f\"Deribit API error: {data['error']}\")\n",
    "    if \"result\" not in data:\n",
    "        raise ValueError(f\"Unexpected response: {data}\")\n",
    "    return data[\"result\"]\n",
    "\n",
    "def get_instruments(currency=\"BTC\"):\n",
    "    return safe_get(f\"{BASE_URL}/public/get_instruments\",\n",
    "                    {\"currency\": currency, \"kind\": \"option\", \"expired\": \"false\"})\n",
    "\n",
    "def get_index_price(currency=\"BTC\"):\n",
    "    res = safe_get(f\"{BASE_URL}/public/get_index_price\",\n",
    "                   {\"index_name\": f\"{currency.lower()}_usd\"})\n",
    "    return res[\"index_price\"]\n",
    "\n",
    "def get_order_book(instrument_name):\n",
    "    res = safe_get(f\"{BASE_URL}/public/get_order_book\",\n",
    "                   {\"instrument_name\": instrument_name})\n",
    "    return res\n",
    "\n",
    "def find_atm_iv(instruments, expiry_ts, spot):\n",
    "    \"\"\"Find ATM option (closest strike) and get its IV.\"\"\"\n",
    "    candidates = [inst for inst in instruments if inst[\"expiration_timestamp\"] == expiry_ts]\n",
    "    if not candidates:\n",
    "        raise ValueError(f\"No options found for expiry {expiry_ts}\")\n",
    "    \n",
    "    closest = min(candidates, key=lambda x: abs(x[\"strike\"] - spot))\n",
    "    ob = get_order_book(closest[\"instrument_name\"])\n",
    "    iv = ob.get(\"mark_iv\")\n",
    "    \n",
    "    if iv is None:\n",
    "        raise ValueError(f\"No mark_iv for {closest['instrument_name']}\")\n",
    "    \n",
    "    return iv / 100  # convert % â†’ decimal\n",
    "\n",
    "def find_25delta_options(instruments, expiry_ts):\n",
    "    \"\"\"Find 25-delta put and call options and get their IVs.\"\"\"\n",
    "    candidates = [inst for inst in instruments if inst[\"expiration_timestamp\"] == expiry_ts]\n",
    "    if not candidates:\n",
    "        raise ValueError(f\"No options found for expiry {expiry_ts}\")\n",
    "    \n",
    "    # Separate puts and calls\n",
    "    puts = [inst for inst in candidates if inst[\"option_type\"] == \"put\"]\n",
    "    calls = [inst for inst in candidates if inst[\"option_type\"] == \"call\"]\n",
    "    \n",
    "    # Find option closest to 25 delta for puts and calls\n",
    "    # For Deribit, we look for options with greek data\n",
    "    put_25d = None\n",
    "    call_25d = None\n",
    "    min_put_diff = float('inf')\n",
    "    min_call_diff = float('inf')\n",
    "    \n",
    "    for put in puts:\n",
    "        ob = get_order_book(put[\"instrument_name\"])\n",
    "        greeks = ob.get(\"greeks\")\n",
    "        if greeks and greeks.get(\"delta\") is not None:\n",
    "            delta = abs(greeks[\"delta\"])\n",
    "            diff = abs(delta - 0.25)\n",
    "            if diff < min_put_diff:\n",
    "                min_put_diff = diff\n",
    "                put_25d = (put[\"instrument_name\"], ob.get(\"mark_iv\"))\n",
    "    \n",
    "    for call in calls:\n",
    "        ob = get_order_book(call[\"instrument_name\"])\n",
    "        greeks = ob.get(\"greeks\")\n",
    "        if greeks and greeks.get(\"delta\") is not None:\n",
    "            delta = abs(greeks[\"delta\"])\n",
    "            diff = abs(delta - 0.25)\n",
    "            if diff < min_call_diff:\n",
    "                min_call_diff = diff\n",
    "                call_25d = (call[\"instrument_name\"], ob.get(\"mark_iv\"))\n",
    "    \n",
    "    if put_25d is None or call_25d is None or put_25d[1] is None or call_25d[1] is None:\n",
    "        raise ValueError(f\"Could not find 25-delta options with valid IVs for expiry {expiry_ts}\")\n",
    "    \n",
    "    return put_25d[1] / 100, call_25d[1] / 100  # convert % â†’ decimal\n",
    "\n",
    "def get_vx30(currency=\"BTC\", target_days=30):\n",
    "    \"\"\"Calculate constant maturity 30-day implied volatility and 25-delta skew.\"\"\"\n",
    "    instruments = get_instruments(currency)\n",
    "    spot = get_index_price(currency)\n",
    "    \n",
    "    now = datetime.now(timezone.utc).timestamp() * 1000\n",
    "    expiries = sorted(list({inst[\"expiration_timestamp\"] for inst in instruments}))\n",
    "    expiries_days = [(ts, (ts - now) / 1000 / 86400) for ts in expiries]\n",
    "    \n",
    "    before = [e for e in expiries_days if e[1] < target_days]\n",
    "    after = [e for e in expiries_days if e[1] > target_days]\n",
    "    \n",
    "    if not before or not after:\n",
    "        raise ValueError(\"No expiries available around target maturity\")\n",
    "    \n",
    "    T1, d1 = before[-1]\n",
    "    T2, d2 = after[0]\n",
    "    \n",
    "    # Get ATM IVs\n",
    "    iv1 = find_atm_iv(instruments, T1, spot)\n",
    "    iv2 = find_atm_iv(instruments, T2, spot)\n",
    "    \n",
    "    # Get 25-delta IVs for skew calculation\n",
    "    print(f\"Fetching 25-delta options for {d1:.1f}d expiry...\")\n",
    "    put_iv1, call_iv1 = find_25delta_options(instruments, T1)\n",
    "    \n",
    "    print(f\"Fetching 25-delta options for {d2:.1f}d expiry...\")\n",
    "    put_iv2, call_iv2 = find_25delta_options(instruments, T2)\n",
    "    \n",
    "    # Calculate 25-delta skew (Put IV - Call IV)\n",
    "    skew1 = put_iv1 - call_iv1\n",
    "    skew2 = put_iv2 - call_iv2\n",
    "    \n",
    "    # Interpolate skew to 30 days\n",
    "    w = (target_days - d1) / (d2 - d1)\n",
    "    skew30 = skew1 * (1 - w) + skew2 * w\n",
    "    \n",
    "    # Variance interpolation for ATM IV\n",
    "    var1 = (iv1**2) * (d1 / 365)\n",
    "    var2 = (iv2**2) * (d2 / 365)\n",
    "    \n",
    "    var30 = var1 * (1 - w) + var2 * w\n",
    "    iv30 = math.sqrt(var30 * (365 / target_days))\n",
    "    \n",
    "    return iv30, iv1, d1, iv2, d2, skew30, skew1, skew2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fda43d2d-fe9c-439c-9a07-1ea340ea59f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating BTC Volatility Metrics...\n",
      "==================================================\n",
      "Fetching 25-delta options for 16.1d expiry...\n",
      "Fetching 25-delta options for 44.1d expiry...\n",
      "\n",
      "       CONSTANT MATURITY METRICS (30 days)        \n",
      "==================================================\n",
      "VX30 (ATM IV):         43.35%\n",
      "25-Delta Skew:          4.54%\n",
      "\n",
      "               NEARBY EXPIRIES USED               \n",
      "==================================================\n",
      "\n",
      "Shorter Expiry (16.1 days):\n",
      "  ATM IV:              41.36%\n",
      "  25-Delta Skew:        4.85%\n",
      "\n",
      "Longer Expiry (44.1 days):\n",
      "  ATM IV:              44.07%\n",
      "  25-Delta Skew:        4.23%\n",
      "\n",
      "==================================================\n",
      "Note: 25-Delta Skew = 25Î” Put IV - 25Î” Call IV\n",
      "      Positive skew indicates downside protection premium\n"
     ]
    }
   ],
   "source": [
    "# --- Run everything ---\n",
    "try:\n",
    "    print(\"Calculating BTC Volatility Metrics...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    vx30, iv1, d1, iv2, d2, skew30, skew1, skew2 = get_vx30()\n",
    "    \n",
    "    print(f\"\\n{'CONSTANT MATURITY METRICS (30 days)':^50}\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"VX30 (ATM IV):        {vx30*100:>6.2f}%\")\n",
    "    print(f\"25-Delta Skew:        {skew30*100:>6.2f}%\")\n",
    "    \n",
    "    print(f\"\\n{'NEARBY EXPIRIES USED':^50}\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"\\nShorter Expiry ({d1:.1f} days):\")\n",
    "    print(f\"  ATM IV:             {iv1*100:>6.2f}%\")\n",
    "    print(f\"  25-Delta Skew:      {skew1*100:>6.2f}%\")\n",
    "    \n",
    "    print(f\"\\nLonger Expiry ({d2:.1f} days):\")\n",
    "    print(f\"  ATM IV:             {iv2*100:>6.2f}%\")\n",
    "    print(f\"  25-Delta Skew:      {skew2*100:>6.2f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Note: 25-Delta Skew = 25Î” Put IV - 25Î” Call IV\")\n",
    "    print(\"      Positive skew indicates downside protection premium\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123f6b0f-6007-42e4-9275-6b0aa98271bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a54233f-6b36-4cb7-8784-8d1fbf54d49a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92cd2e61-bdb6-4e83-b3da-9fb751290e9e",
   "metadata": {},
   "source": [
    "#### Historical BTC Skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1e3e167-1b9f-4042-80a0-78811a59d9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "BASE_URL = \"https://www.deribit.com/api/v2\"\n",
    "CSV_FILE = \"btc_skew_history.csv\"\n",
    "\n",
    "# ==========================================================\n",
    "# --- Utility functions ---\n",
    "# ==========================================================\n",
    "def safe_get(url, params=None):\n",
    "    r = requests.get(url, params=params)\n",
    "    data = r.json()\n",
    "    if \"error\" in data:\n",
    "        raise ValueError(f\"Deribit API error: {data['error']}\")\n",
    "    if \"result\" not in data:\n",
    "        raise ValueError(f\"Unexpected response: {data}\")\n",
    "    return data[\"result\"]\n",
    "\n",
    "def get_instruments(currency=\"BTC\"):\n",
    "    return safe_get(f\"{BASE_URL}/public/get_instruments\",\n",
    "                    {\"currency\": currency, \"kind\": \"option\", \"expired\": \"false\"})\n",
    "\n",
    "def get_index_price(currency=\"BTC\"):\n",
    "    res = safe_get(f\"{BASE_URL}/public/get_index_price\",\n",
    "                   {\"index_name\": f\"{currency.lower()}_usd\"})\n",
    "    return res[\"index_price\"]\n",
    "\n",
    "def get_order_book(instrument_name):\n",
    "    res = safe_get(f\"{BASE_URL}/public/get_order_book\",\n",
    "                   {\"instrument_name\": instrument_name})\n",
    "    return res\n",
    "\n",
    "# ==========================================================\n",
    "# --- IV & skew extraction ---\n",
    "# ==========================================================\n",
    "def find_atm_iv(instruments, expiry_ts, spot):\n",
    "    \"\"\"Find ATM option (closest strike) and get its IV.\"\"\"\n",
    "    candidates = [inst for inst in instruments if inst[\"expiration_timestamp\"] == expiry_ts]\n",
    "    if not candidates:\n",
    "        raise ValueError(f\"No options found for expiry {expiry_ts}\")\n",
    "    \n",
    "    closest = min(candidates, key=lambda x: abs(x[\"strike\"] - spot))\n",
    "    ob = get_order_book(closest[\"instrument_name\"])\n",
    "    iv = ob.get(\"mark_iv\")\n",
    "    \n",
    "    if iv is None:\n",
    "        raise ValueError(f\"No mark_iv for {closest['instrument_name']}\")\n",
    "    \n",
    "    return iv / 100  # convert % â†’ decimal\n",
    "\n",
    "def find_25delta_options(instruments, expiry_ts):\n",
    "    \"\"\"Find 25-delta put and call options and get their IVs.\"\"\"\n",
    "    candidates = [inst for inst in instruments if inst[\"expiration_timestamp\"] == expiry_ts]\n",
    "    if not candidates:\n",
    "        raise ValueError(f\"No options found for expiry {expiry_ts}\")\n",
    "    \n",
    "    puts = [inst for inst in candidates if inst[\"option_type\"] == \"put\"]\n",
    "    calls = [inst for inst in candidates if inst[\"option_type\"] == \"call\"]\n",
    "    \n",
    "    put_25d = None\n",
    "    call_25d = None\n",
    "    min_put_diff = float('inf')\n",
    "    min_call_diff = float('inf')\n",
    "    \n",
    "    for put in puts:\n",
    "        ob = get_order_book(put[\"instrument_name\"])\n",
    "        greeks = ob.get(\"greeks\")\n",
    "        if greeks and greeks.get(\"delta\") is not None:\n",
    "            delta = abs(greeks[\"delta\"])\n",
    "            diff = abs(delta - 0.25)\n",
    "            if diff < min_put_diff:\n",
    "                min_put_diff = diff\n",
    "                put_25d = (put[\"instrument_name\"], ob.get(\"mark_iv\"))\n",
    "    \n",
    "    for call in calls:\n",
    "        ob = get_order_book(call[\"instrument_name\"])\n",
    "        greeks = ob.get(\"greeks\")\n",
    "        if greeks and greeks.get(\"delta\") is not None:\n",
    "            delta = abs(greeks[\"delta\"])\n",
    "            diff = abs(delta - 0.25)\n",
    "            if diff < min_call_diff:\n",
    "                min_call_diff = diff\n",
    "                call_25d = (call[\"instrument_name\"], ob.get(\"mark_iv\"))\n",
    "    \n",
    "    if put_25d is None or call_25d is None or put_25d[1] is None or call_25d[1] is None:\n",
    "        raise ValueError(f\"Could not find 25-delta options with valid IVs for expiry {expiry_ts}\")\n",
    "    \n",
    "    return put_25d[1] / 100, call_25d[1] / 100  # convert % â†’ decimal\n",
    "\n",
    "# ==========================================================\n",
    "# --- 30D Constant-Maturity Calculation ---\n",
    "# ==========================================================\n",
    "def get_vx30(currency=\"BTC\", target_days=30):\n",
    "    \"\"\"Calculate constant maturity 30-day implied volatility and 25-delta skew.\"\"\"\n",
    "    instruments = get_instruments(currency)\n",
    "    spot = get_index_price(currency)\n",
    "    \n",
    "    now = datetime.now(timezone.utc).timestamp() * 1000\n",
    "    expiries = sorted(list({inst[\"expiration_timestamp\"] for inst in instruments}))\n",
    "    expiries_days = [(ts, (ts - now) / 1000 / 86400) for ts in expiries]\n",
    "    \n",
    "    before = [e for e in expiries_days if e[1] < target_days]\n",
    "    after = [e for e in expiries_days if e[1] > target_days]\n",
    "    \n",
    "    if not before or not after:\n",
    "        raise ValueError(\"No expiries available around target maturity\")\n",
    "    \n",
    "    T1, d1 = before[-1]\n",
    "    T2, d2 = after[0]\n",
    "    \n",
    "    iv1 = find_atm_iv(instruments, T1, spot)\n",
    "    iv2 = find_atm_iv(instruments, T2, spot)\n",
    "    \n",
    "    print(f\"Fetching 25-delta options for {d1:.1f}d expiry...\")\n",
    "    put_iv1, call_iv1 = find_25delta_options(instruments, T1)\n",
    "    print(f\"Fetching 25-delta options for {d2:.1f}d expiry...\")\n",
    "    put_iv2, call_iv2 = find_25delta_options(instruments, T2)\n",
    "    \n",
    "    skew1 = put_iv1 - call_iv1\n",
    "    skew2 = put_iv2 - call_iv2\n",
    "    \n",
    "    w = (target_days - d1) / (d2 - d1)\n",
    "    skew30 = skew1 * (1 - w) + skew2 * w\n",
    "    \n",
    "    var1 = (iv1**2) * (d1 / 365)\n",
    "    var2 = (iv2**2) * (d2 / 365)\n",
    "    var30 = var1 * (1 - w) + var2 * w\n",
    "    iv30 = math.sqrt(var30 * (365 / target_days))\n",
    "    \n",
    "    return iv30, skew30\n",
    "\n",
    "# ==========================================================\n",
    "# --- Logging Function ---\n",
    "# ==========================================================\n",
    "def log_skew_to_csv(currency=\"BTC\"):\n",
    "    iv30, skew30 = get_vx30(currency)\n",
    "    \n",
    "    now = datetime.now(timezone.utc)\n",
    "    row = {\n",
    "        \"timestamp\": now.isoformat(),\n",
    "        \"vx30_atm_iv\": iv30 * 100,\n",
    "        \"skew_25d\": skew30 * 100\n",
    "    }\n",
    "    \n",
    "    # Append or create CSV\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        df = pd.read_csv(CSV_FILE)\n",
    "        df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
    "    else:\n",
    "        df = pd.DataFrame([row])\n",
    "    \n",
    "    df.drop_duplicates(subset=[\"timestamp\"], inplace=True)\n",
    "    df.to_csv(CSV_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\n--- Constant Maturity Metrics (30 days) ---\")\n",
    "    print(f\"Timestamp:         {now.strftime('%Y-%m-%d %H:%M UTC')}\")\n",
    "    print(f\"VX30 (ATM IV):     {iv30*100:.2f}%\")\n",
    "    print(f\"25Î” Skew:          {skew30*100:.2f}%\")\n",
    "    print(f\"\\nLogged to: {CSV_FILE}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3b84647-2765-4597-830f-5a7cd3259dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching BTC 30-Day Constant Maturity Metrics...\n",
      "Fetching 25-delta options for 16.1d expiry...\n",
      "Fetching 25-delta options for 44.1d expiry...\n",
      "\n",
      "--- Constant Maturity Metrics (30 days) ---\n",
      "Timestamp:         2025-11-12 05:04 UTC\n",
      "VX30 (ATM IV):     43.34%\n",
      "25Î” Skew:          4.45%\n",
      "\n",
      "Logged to: btc_skew_history.csv\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# --- MAIN EXECUTION ---\n",
    "# ==========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        print(\"Fetching BTC 30-Day Constant Maturity Metrics...\")\n",
    "        log_skew_to_csv(\"BTC\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329bee42-8886-478e-92ca-042015234c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81459c2-110d-401b-a49f-ce8efc4f2ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e1923a-36ca-460f-9d80-40d4a17d5b45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dd96a3-424d-4768-aefb-91e2468beee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "daf9bc44-cdbd-4913-895d-fa99b705b78e",
   "metadata": {},
   "source": [
    "# Risk Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b357dcd6-104f-4d26-b5e0-9b86e1722161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466711ad-f3ac-464a-bcba-ca300befad8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7e1f10-8ffb-4435-9412-56b2dfd5a79f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e919cd5b-ef42-4dff-89de-72c61bde6218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5208d25f-7aea-49e7-b63f-3ae18a799173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf763f41-762c-4f1a-929c-fc586dbe82a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873811c7-17ca-4b37-8a31-c619b3f42242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b029ec0-0f7b-4311-a721-4637bcbab549",
   "metadata": {},
   "source": [
    "# Backtester"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03e15b5-1ed2-445e-9d43-af8d14ace841",
   "metadata": {},
   "source": [
    "## Momentum Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfa3296-28c8-42e5-8612-c2b317fe9a44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfed50e-e882-4ee5-bfa9-134a072e5731",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9e008d-6dee-46b1-9c96-599583eb9f07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0856d7-0f7a-4ffe-a010-4d11255e5208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb912599-e0dc-4c91-9362-37e8a7fe0a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe1e8cbb-3337-4e8d-b9fa-604d4abb3350",
   "metadata": {},
   "source": [
    "## Trend Following"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f86aec-2951-4847-944e-dde0ffa32df4",
   "metadata": {},
   "source": [
    "### Trend Following"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e855e389-8456-44de-8803-524612e8540f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Define Function/Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21038f9d-15a7-4bd9-b140-1a5ba8359cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bff4c8a-7582-4b84-bee8-7faf8dc9303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedTradingStrategyOptimizer:\n",
    "    def __init__(self, df_h: pd.DataFrame, filtered_tickers_h: List[str]):\n",
    "        \"\"\"\n",
    "        Initialize the optimizer with data and filtered tickers\n",
    "        \n",
    "        Args:\n",
    "            df_h: DataFrame with datetime index and coin prices as columns\n",
    "            filtered_tickers_h: List of coin tickers to analyze\n",
    "        \"\"\"\n",
    "        self.df_h = df_h\n",
    "        self.filtered_tickers_h = filtered_tickers_h\n",
    "        self.df_short_h = df_h[filtered_tickers_h].copy()\n",
    "        self.results_df = pd.DataFrame()\n",
    "        \n",
    "    def adaptive_ema(self, series: pd.Series, period: int) -> pd.Series:\n",
    "        \"\"\"Adaptive EMA calculation\"\"\"\n",
    "        ema = series.copy()\n",
    "        noise = np.zeros_like(series.values)\n",
    "        \n",
    "        for i in range(period, len(series)):\n",
    "            sig = abs(series.iloc[i] - series.iloc[i - period])\n",
    "            noise[i] = noise[i - 1] + abs(series.iloc[i] - series.iloc[i - 1]) - abs(series.iloc[i] - series.iloc[i - period])\n",
    "            noise_val = noise[i] if noise[i] != 0 else 1\n",
    "            efratio = sig / noise_val\n",
    "            slow_end = period * 5\n",
    "            fast_end = max(period / 2.0, 1)\n",
    "            avg_period = ((sig / noise_val) * (slow_end - fast_end)) + fast_end\n",
    "            alpha = 2.0 / (1.0 + avg_period)\n",
    "            ema.iloc[i] = ema.iloc[i - 1] + alpha * (series.iloc[i] - ema.iloc[i - 1])\n",
    "            \n",
    "        return ema\n",
    "\n",
    "    def jfatl(self, series: pd.Series, fatl_len: int, jma_len: int, phase: float) -> pd.Series:\n",
    "        \"\"\"JFATL calculation\"\"\"\n",
    "        fatl = series.rolling(fatl_len).mean()\n",
    "        e = 0.5 * (phase + 1)\n",
    "        wma1 = fatl.rolling(jma_len).mean()\n",
    "        wma2 = fatl.rolling(jma_len // 2).mean()\n",
    "        return wma1 * e + wma2 * (1 - e)\n",
    "\n",
    "    def nadaraya_watson_vectorized(self, series: pd.Series, h: int, r: float, start_regression_at_bar: int) -> pd.Series:\n",
    "        \"\"\"Vectorized Nadaraya-Watson Estimator\"\"\"\n",
    "        n = len(series)\n",
    "        smoothed = np.full(n, np.nan)\n",
    "        X = np.arange(n)\n",
    "\n",
    "        for t in range(start_regression_at_bar, n):\n",
    "            indices = np.arange(0, t)\n",
    "            distances = t - indices\n",
    "            weights = (1 + (distances**2 / ((h**2) * 2 * r))) ** (-r)\n",
    "            values = series.values[:t]\n",
    "            smoothed[t] = np.sum(values * weights) / np.sum(weights)\n",
    "\n",
    "        return pd.Series(smoothed, index=series.index)\n",
    "\n",
    "    def generate_signals(self, coin: str, base_length: int, hold_days: int) -> Tuple[pd.Series, pd.Series]:\n",
    "        \"\"\"Generate buy/sell signals for a given coin, base_length, and hold_days\"\"\"\n",
    "        # Parameters based on base_length\n",
    "        nw_start = 150\n",
    "        aema_period = 6 * base_length\n",
    "        fatl_length = 6 * base_length\n",
    "        jma_length = 6 * base_length\n",
    "        nw_window = 6 * base_length\n",
    "        nw_r = 48.0\n",
    "        phase = 0.5\n",
    "        \n",
    "        price = self.df_short_h[coin]\n",
    "        \n",
    "        # Calculate indicators\n",
    "        aema = self.adaptive_ema(price, aema_period)\n",
    "        jfatl_val = self.jfatl(price, fatl_length, jma_length, phase)\n",
    "        nw_val = self.nadaraya_watson_vectorized(price, nw_window, nw_r, nw_start)\n",
    "        \n",
    "        # Generate signals\n",
    "        above = (price > aema) & (price > jfatl_val) & (price > nw_val)\n",
    "        below = (price < aema) & (price < jfatl_val) & (price < nw_val)\n",
    "        \n",
    "        above_rolling = above.rolling(hold_days).sum()\n",
    "        below_rolling = below.rolling(hold_days).sum()\n",
    "        \n",
    "        buy_signals = (above_rolling == hold_days)\n",
    "        sell_signals = (below_rolling == hold_days)\n",
    "        \n",
    "        return buy_signals, sell_signals\n",
    "\n",
    "    def calculate_forward_returns(self, coin: str, signals: pd.Series, signal_type: str, forward_period: int = 90) -> List[float]:\n",
    "        \"\"\"Calculate forward returns for given signals\"\"\"\n",
    "        price = self.df_short_h[coin]\n",
    "        returns = []\n",
    "        \n",
    "        signal_dates = signals[signals].index\n",
    "        \n",
    "        for date in signal_dates:\n",
    "            try:\n",
    "                current_idx = price.index.get_loc(date)\n",
    "                if current_idx + forward_period < len(price):\n",
    "                    current_price = price.iloc[current_idx]\n",
    "                    future_price = price.iloc[current_idx + forward_period]\n",
    "                    \n",
    "                    if signal_type == 'buy':\n",
    "                        ret = (future_price - current_price) / current_price\n",
    "                    else:  # sell signal\n",
    "                        ret = (current_price - future_price) / current_price\n",
    "                    \n",
    "                    returns.append(ret)\n",
    "            except (KeyError, IndexError):\n",
    "                continue\n",
    "                \n",
    "        return returns\n",
    "\n",
    "    def run_optimization_iteration(self, base_length: int, hold_days: int, iterations: int = 500) -> Dict:\n",
    "        \"\"\"Run optimization for a single combination of base_length and hold_days\"\"\"\n",
    "        print(f\"Running optimization for base_length: {base_length}, hold_days: {hold_days}\")\n",
    "        \n",
    "        buy_returns_all = []\n",
    "        sell_returns_all = []\n",
    "        total_returns_all = []\n",
    "        \n",
    "        for iteration in range(iterations):\n",
    "            # Select random coin\n",
    "            random_coin = random.choice(self.filtered_tickers_h)\n",
    "            \n",
    "            try:\n",
    "                # Generate signals\n",
    "                buy_signals, sell_signals = self.generate_signals(random_coin, base_length, hold_days)\n",
    "                \n",
    "                # Calculate returns\n",
    "                buy_returns = self.calculate_forward_returns(random_coin, buy_signals, 'buy')\n",
    "                sell_returns = self.calculate_forward_returns(random_coin, sell_signals, 'sell')\n",
    "                \n",
    "                # Store returns\n",
    "                buy_returns_all.extend(buy_returns)\n",
    "                sell_returns_all.extend(sell_returns)\n",
    "                total_returns_all.extend(buy_returns + sell_returns)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in iteration {iteration} for coin {random_coin}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Calculate statistics\n",
    "        results = {\n",
    "            'base_length': base_length,\n",
    "            'hold_days': hold_days,\n",
    "            'median_buy_return': np.median(buy_returns_all) if buy_returns_all else 0,\n",
    "            'median_sell_return': np.median(sell_returns_all) if sell_returns_all else 0,\n",
    "            'median_total_return': np.median(total_returns_all) if total_returns_all else 0,\n",
    "            'num_buy_signals': len(buy_returns_all),\n",
    "            'num_sell_signals': len(sell_returns_all),\n",
    "            'total_signals': len(total_returns_all)\n",
    "        }\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        if total_returns_all:\n",
    "            returns_array = np.array(total_returns_all)\n",
    "            positive_returns = returns_array[returns_array > 0]\n",
    "            negative_returns = returns_array[returns_array < 0]\n",
    "            \n",
    "            # Win-Loss Ratio\n",
    "            win_rate = len(positive_returns) / len(returns_array) if len(returns_array) > 0 else 0\n",
    "            loss_rate = len(negative_returns) / len(returns_array) if len(returns_array) > 0 else 0\n",
    "            results['win_loss_ratio'] = win_rate / loss_rate if loss_rate > 0 else float('inf')\n",
    "            \n",
    "            # Profit Factor\n",
    "            total_profits = np.sum(positive_returns) if len(positive_returns) > 0 else 0\n",
    "            total_losses = abs(np.sum(negative_returns)) if len(negative_returns) > 0 else 0\n",
    "            results['profit_factor'] = total_profits / total_losses if total_losses > 0 else float('inf')\n",
    "            \n",
    "            # Max Drawdown (simplified calculation)\n",
    "            cumulative_returns = np.cumsum(returns_array)\n",
    "            running_max = np.maximum.accumulate(cumulative_returns)\n",
    "            drawdown = (cumulative_returns - running_max)\n",
    "            results['max_drawdown'] = np.min(drawdown) if len(drawdown) > 0 else 0\n",
    "        else:\n",
    "            results['win_loss_ratio'] = 0\n",
    "            results['profit_factor'] = 0\n",
    "            results['max_drawdown'] = 0\n",
    "            \n",
    "        return results\n",
    "\n",
    "    def optimize_parameters(self, base_length_range: range, hold_days_range: range, iterations: int = 500):\n",
    "        \"\"\"Run optimization across multiple base_length and hold_days values\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for base_length in base_length_range:\n",
    "            for hold_days in hold_days_range:\n",
    "                result = self.run_optimization_iteration(base_length, hold_days, iterations)\n",
    "                results.append(result)\n",
    "        \n",
    "        self.results_df = pd.DataFrame(results)\n",
    "        return self.results_df\n",
    "\n",
    "    def create_heatmaps(self):\n",
    "        \"\"\"Create heatmaps of median returns vs parameters\"\"\"\n",
    "        if self.results_df.empty:\n",
    "            print(\"No results to plot. Run optimization first.\")\n",
    "            return None\n",
    "        \n",
    "        # Create pivot tables for heatmaps\n",
    "        buy_pivot = self.results_df.pivot(index='hold_days', columns='base_length', values='median_buy_return')\n",
    "        sell_pivot = self.results_df.pivot(index='hold_days', columns='base_length', values='median_sell_return')\n",
    "        total_pivot = self.results_df.pivot(index='hold_days', columns='base_length', values='median_total_return')\n",
    "        \n",
    "        # Create subplots\n",
    "        fig = make_subplots(\n",
    "            rows=3, cols=1,\n",
    "            subplot_titles=('Buy Signal Median Returns Heatmap',\n",
    "                          'Sell Signal Median Returns Heatmap',\n",
    "                          'Total (Buy + Sell) Median Returns Heatmap'),\n",
    "            vertical_spacing=0.1\n",
    "        )\n",
    "        \n",
    "        # Buy returns heatmap\n",
    "        fig.add_trace(\n",
    "            go.Heatmap(\n",
    "                z=buy_pivot.values,\n",
    "                x=buy_pivot.columns,\n",
    "                y=buy_pivot.index,\n",
    "                colorscale='RdYlGn',\n",
    "                name='Buy Returns',\n",
    "                text=np.round(buy_pivot.values, 4),\n",
    "                texttemplate='%{text}',\n",
    "                textfont={\"size\": 10},\n",
    "                hovertemplate='Base Length: %{x}<br>Hold Days: %{y}<br>Median Buy Return: %{z:.4f}<extra></extra>'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Sell returns heatmap\n",
    "        fig.add_trace(\n",
    "            go.Heatmap(\n",
    "                z=sell_pivot.values,\n",
    "                x=sell_pivot.columns,\n",
    "                y=sell_pivot.index,\n",
    "                colorscale='RdYlGn',\n",
    "                name='Sell Returns',\n",
    "                text=np.round(sell_pivot.values, 4),\n",
    "                texttemplate='%{text}',\n",
    "                textfont={\"size\": 10},\n",
    "                hovertemplate='Base Length: %{x}<br>Hold Days: %{y}<br>Median Sell Return: %{z:.4f}<extra></extra>'\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Total returns heatmap\n",
    "        fig.add_trace(\n",
    "            go.Heatmap(\n",
    "                z=total_pivot.values,\n",
    "                x=total_pivot.columns,\n",
    "                y=total_pivot.index,\n",
    "                colorscale='RdYlGn',\n",
    "                name='Total Returns',\n",
    "                text=np.round(total_pivot.values, 4),\n",
    "                texttemplate='%{text}',\n",
    "                textfont={\"size\": 10},\n",
    "                hovertemplate='Base Length: %{x}<br>Hold Days: %{y}<br>Median Total Return: %{z:.4f}<extra></extra>'\n",
    "            ),\n",
    "            row=3, col=1\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title='Median Returns Heatmaps vs Base Length and Hold Days Parameters',\n",
    "            height=1200,\n",
    "            width=1000,\n",
    "            showlegend=False\n",
    "        )\n",
    "        \n",
    "        # Update axis labels\n",
    "        for i in range(1, 4):\n",
    "            fig.update_xaxes(title_text=\"Base Length\", row=i, col=1)\n",
    "            fig.update_yaxes(title_text=\"Hold Days\", row=i, col=1)\n",
    "        \n",
    "        return fig\n",
    "\n",
    "    def create_performance_line_charts(self):\n",
    "        \"\"\"Create interactive line charts for performance metrics\"\"\"\n",
    "        if self.results_df.empty:\n",
    "            print(\"No results to plot. Run optimization first.\")\n",
    "            return None\n",
    "        \n",
    "        # Handle infinite values for better plotting\n",
    "        chart_data = self.results_df.copy()\n",
    "        chart_data['profit_factor'] = chart_data['profit_factor'].replace([np.inf, -np.inf], np.nan)\n",
    "        chart_data['win_loss_ratio'] = chart_data['win_loss_ratio'].replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        # Cap extremely high values for better visualization\n",
    "        chart_data['profit_factor'] = np.clip(chart_data['profit_factor'], 0, 10)\n",
    "        chart_data['win_loss_ratio'] = np.clip(chart_data['win_loss_ratio'], 0, 10)\n",
    "        \n",
    "        # Create parameter combination for x-axis\n",
    "        chart_data['param_combo'] = chart_data['base_length'].astype(str) + '_' + chart_data['hold_days'].astype(str)\n",
    "        chart_data = chart_data.sort_values(['base_length', 'hold_days'])\n",
    "        \n",
    "        # Create subplots - 3 charts in separate rows\n",
    "        fig = make_subplots(\n",
    "            rows=3, cols=1,\n",
    "            subplot_titles=('Profit Factor vs Parameters', \n",
    "                          'Win-Loss Ratio vs Parameters',\n",
    "                          'Max Drawdown vs Parameters'),\n",
    "            vertical_spacing=0.1\n",
    "        )\n",
    "        \n",
    "        # Profit Factor\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=list(range(len(chart_data))), y=chart_data['profit_factor'],\n",
    "                      mode='lines+markers', name='Profit Factor', line=dict(color='green'),\n",
    "                      text=chart_data['param_combo'],\n",
    "                      hovertemplate='Base Length_Hold Days: %{text}<br>Profit Factor: %{y:.3f}<extra></extra>'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Win-Loss Ratio\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=list(range(len(chart_data))), y=chart_data['win_loss_ratio'],\n",
    "                      mode='lines+markers', name='Win-Loss Ratio', line=dict(color='blue'),\n",
    "                      text=chart_data['param_combo'],\n",
    "                      hovertemplate='Base Length_Hold Days: %{text}<br>Win-Loss Ratio: %{y:.3f}<extra></extra>'),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Max Drawdown (make positive for better interpretation)\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=list(range(len(chart_data))), y=-chart_data['max_drawdown'],\n",
    "                      mode='lines+markers', name='Max Drawdown', line=dict(color='red'),\n",
    "                      text=chart_data['param_combo'],\n",
    "                      hovertemplate='Base Length_Hold Days: %{text}<br>Max Drawdown: %{y:.3f}<extra></extra>'),\n",
    "            row=3, col=1\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title='Performance Metrics vs Parameter Combinations',\n",
    "            height=1200,\n",
    "            width=1000,\n",
    "            showlegend=False\n",
    "        )\n",
    "        \n",
    "        # Update x-axis labels\n",
    "        for i in range(1, 4):\n",
    "            fig.update_xaxes(title_text=\"Parameter Combination Index\", row=i, col=1)\n",
    "        \n",
    "        # Update y-axis labels\n",
    "        fig.update_yaxes(title_text=\"Profit Factor\", row=1, col=1)\n",
    "        fig.update_yaxes(title_text=\"Win-Loss Ratio\", row=2, col=1)\n",
    "        fig.update_yaxes(title_text=\"Max Drawdown\", row=3, col=1)\n",
    "        \n",
    "        return fig\n",
    "\n",
    "    def analyze_specific_coins(self, coin_symbols: List[str], base_length: int = 14, hold_days: int = 18):\n",
    "        \"\"\"Analyze specific coins and create interactive charts\"\"\"\n",
    "        charts = {}\n",
    "        \n",
    "        for coin in coin_symbols:\n",
    "            if coin not in self.df_short_h.columns:\n",
    "                print(f\"Coin {coin} not found in data\")\n",
    "                continue\n",
    "                \n",
    "            # Generate signals\n",
    "            buy_signals, sell_signals = self.generate_signals(coin, base_length, hold_days)\n",
    "            price = self.df_short_h[coin]\n",
    "            \n",
    "            # Calculate returns for each signal\n",
    "            buy_signal_dates = buy_signals[buy_signals].index\n",
    "            sell_signal_dates = sell_signals[sell_signals].index\n",
    "            \n",
    "            buy_returns = []\n",
    "            sell_returns = []\n",
    "            buy_dates_with_returns = []\n",
    "            sell_dates_with_returns = []\n",
    "            \n",
    "            # Calculate returns for buy signals\n",
    "            for date in buy_signal_dates:\n",
    "                returns = self.calculate_forward_returns(coin, pd.Series([True], index=[date]), 'buy')\n",
    "                if returns:\n",
    "                    buy_returns.append(returns[0])\n",
    "                    buy_dates_with_returns.append(date)\n",
    "            \n",
    "            # Calculate returns for sell signals\n",
    "            for date in sell_signal_dates:\n",
    "                returns = self.calculate_forward_returns(coin, pd.Series([True], index=[date]), 'sell')\n",
    "                if returns:\n",
    "                    sell_returns.append(returns[0])\n",
    "                    sell_dates_with_returns.append(date)\n",
    "            \n",
    "            # Create subplots for this coin\n",
    "            fig = make_subplots(\n",
    "                rows=2, cols=1,\n",
    "                subplot_titles=(f'{coin} Price with Buy/Sell Signals', \n",
    "                              f'{coin} Signal Returns Over Time'),\n",
    "                specs=[[{\"secondary_y\": False}], [{\"secondary_y\": False}]],\n",
    "                vertical_spacing=0.15\n",
    "            )\n",
    "            \n",
    "            # Price chart with signals\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=price.index, y=price.values,\n",
    "                          mode='lines', name=f'{coin} Price', line=dict(color='black')),\n",
    "                row=1, col=1\n",
    "            )\n",
    "            \n",
    "            # Buy signals\n",
    "            if len(buy_signal_dates) > 0:\n",
    "                buy_prices = [price.loc[date] for date in buy_signal_dates]\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=buy_signal_dates, y=buy_prices,\n",
    "                              mode='markers', name='Buy Signals', \n",
    "                              marker=dict(color='green', size=6, symbol='circle'),\n",
    "                              hovertemplate='Date: %{x}<br>Price: %{y:.4f}<br>Signal: Buy<extra></extra>'),\n",
    "                    row=1, col=1\n",
    "                )\n",
    "            \n",
    "            # Sell signals\n",
    "            if len(sell_signal_dates) > 0:\n",
    "                sell_prices = [price.loc[date] for date in sell_signal_dates]\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=sell_signal_dates, y=sell_prices,\n",
    "                              mode='markers', name='Sell Signals',\n",
    "                              marker=dict(color='red', size=6, symbol='circle'),\n",
    "                              hovertemplate='Date: %{x}<br>Price: %{y:.4f}<br>Signal: Sell<extra></extra>'),\n",
    "                    row=1, col=1\n",
    "                )\n",
    "            \n",
    "            # Returns chart\n",
    "            if buy_returns:\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=buy_dates_with_returns, y=buy_returns,\n",
    "                              mode='markers', name='Buy Returns',\n",
    "                              marker=dict(color='green', size=8, symbol='circle'),\n",
    "                              hovertemplate='Date: %{x}<br>Return: %{y:.4f}<br>Signal: Buy<extra></extra>'),\n",
    "                    row=2, col=1\n",
    "                )\n",
    "            \n",
    "            if sell_returns:\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(x=sell_dates_with_returns, y=sell_returns,\n",
    "                              mode='markers', name='Sell Returns',\n",
    "                              marker=dict(color='red', size=8, symbol='circle'),\n",
    "                              hovertemplate='Date: %{x}<br>Return: %{y:.4f}<br>Signal: Sell<extra></extra>'),\n",
    "                    row=2, col=1\n",
    "                )\n",
    "            \n",
    "            # Add zero line for returns\n",
    "            fig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\", row=2, col=1)\n",
    "            \n",
    "            fig.update_layout(\n",
    "                title=f'{coin} Trading Analysis (Base Length: {base_length}, Hold Days: {hold_days})',\n",
    "                height=800,\n",
    "                width=1200,\n",
    "                showlegend=True\n",
    "            )\n",
    "            \n",
    "            fig.update_xaxes(title_text=\"Date\", row=1, col=1)\n",
    "            fig.update_xaxes(title_text=\"Date\", row=2, col=1)\n",
    "            fig.update_yaxes(title_text=\"Price\", row=1, col=1)\n",
    "            fig.update_yaxes(title_text=\"Return\", row=2, col=1)\n",
    "            \n",
    "            charts[coin] = fig\n",
    "        \n",
    "        return charts\n",
    "\n",
    "    def get_best_parameters(self, metric='median_total_return'):\n",
    "        \"\"\"Get the best parameter combination based on specified metric\"\"\"\n",
    "        if self.results_df.empty:\n",
    "            print(\"No results available. Run optimization first.\")\n",
    "            return None\n",
    "        \n",
    "        if metric not in self.results_df.columns:\n",
    "            print(f\"Metric '{metric}' not found in results.\")\n",
    "            return None\n",
    "        \n",
    "        best_idx = self.results_df[metric].idxmax()\n",
    "        best_params = self.results_df.loc[best_idx]\n",
    "        \n",
    "        print(f\"Best parameters based on {metric}:\")\n",
    "        print(f\"Base Length: {best_params['base_length']}\")\n",
    "        print(f\"Hold Days: {best_params['hold_days']}\")\n",
    "        print(f\"{metric}: {best_params[metric]:.6f}\")\n",
    "        \n",
    "        return best_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6415a7-96e1-4427-b1a2-7f0abf7c3f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31ae7d9-ffc8-417a-899a-9393091ff9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b5f243-ed0d-4274-b34c-f5a5a806cd56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f3020eb-cc3d-449d-ada8-68c1d56690ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Optimization & Charting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f0c1a2-0785-4d1d-a965-41d8dbab8053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize the optimizer\n",
    "optimizer = EnhancedTradingStrategyOptimizer(df_h, filtered_tickers_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96db78b2-abe5-48f2-a3a7-d6b23eea18e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Run parameter optimization\n",
    "base_length_range = range(8, 25, 2)  # 8, 10, 12, 14, 16, 18, 20, 22, 24\n",
    "hold_days_range = range(3, 18, 2)    # 3, 5, 7, 9, 11, 13, 15, 17\n",
    "results_df = optimizer.optimize_parameters(base_length_range, hold_days_range, iterations=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aa3363-a37a-4ab5-a580-4fbe7c5d4005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create and display heatmaps\n",
    "heatmap_fig = optimizer.create_heatmaps()\n",
    "heatmap_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3530c38f-c9b5-4a46-b57b-b6f9732cc21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create and display performance line charts\n",
    "line_charts_fig = optimizer.create_performance_line_charts()\n",
    "line_charts_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e4b9e6-09e4-4c03-b831-032d3d2c41cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Find best parameters\n",
    "best_params = optimizer.get_best_parameters('median_total_return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0dd118-ced1-48ff-b7c7-ce10db5922f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Analyze specific coins\n",
    "coin_symbols = ['WOO']  # Replace with your actual coin symbols\n",
    "coin_charts = optimizer.analyze_specific_coins(coin_symbols, base_length=16, hold_days=6)\n",
    "\n",
    "# Display charts for each coin\n",
    "for coin, chart in coin_charts.items():\n",
    "    chart.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d0b4ae-2be8-45c7-92d8-909209d6c3b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6000b3e-a7ef-4386-aa03-273a226eb327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd3907b-fc13-4cc0-9ff5-3dc6dd7cad37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d384a484-1ff8-4d6a-a5da-20ded9db4798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1743cc3f-da69-4f18-9f04-bec28238d35d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50563c67-7059-4ccb-9089-7e70fd6011bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca8fef7-2c62-42f5-b4c2-612081186dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba417ab5-e010-4f4a-96c2-cc23006719d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d245c93c-d826-4cc8-96cf-26b88e494739",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596f8f48-6ee7-48da-b08e-9c346ffa487d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45eed85-6b86-4714-8ff6-9dce5e583224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b139a-57da-4dbc-979b-b26ae10193b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1efe186d-281a-4a4e-ba2a-5b9f909b12e3",
   "metadata": {},
   "source": [
    "### Return-to-Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d0ba50-9ac9-4db2-8590-cad34a6f8cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeef8c3-d070-403e-b71a-6d5fd33c5df6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b760fb19-5e55-4162-9ad3-2c253c047d05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8a3520-1105-4bfa-83cc-fb83f497404f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0564e3b1-9d7a-4370-8351-d98b0fc99350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af618b9e-84ca-4dc3-9707-a91504b4fe67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c1fff5-e117-4b60-8a8f-0e9c237e0055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ef8b11-cae5-43f3-bc25-7b1019cd8721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78d4b559-8e57-46a3-a639-cbcfdc42e489",
   "metadata": {},
   "source": [
    "## Long/Short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20db93a9-65ef-4d1f-b866-8e5b5ab19655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca38930-32f8-4b53-b867-2cb64b59fbba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5926a82-8a15-4116-824e-b9af5a30dcec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e226823f-ff4f-4fd4-9d4c-b876401bb7f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda0bc1d-ec09-4cb1-a8aa-c60cbe109a94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91c68e44-f0fa-4dc5-a417-69b280fef8b9",
   "metadata": {},
   "source": [
    "## Value Buys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c13c5e-8e6a-4579-aa7f-3e3a25f94281",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fresh Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fa4b18-6010-48bd-b155-76ed0717baf5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Define Function/signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5f375c-d7bc-4e39-910b-d38a62d3c744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fresh_momentum_series(prices_df, lookback_days=42, holding_period=30):\n",
    "    \"\"\"\n",
    "    Computes fresh momentum for each coin across all dates.\n",
    "    \n",
    "    Parameters:\n",
    "    - prices_df: DataFrame with datetime index and coin tickers as columns\n",
    "    - lookback_days: Lookback period for calculating return\n",
    "    - holding_period: Holding period for forward return\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame of fresh momentum values for each date and coin\n",
    "    \"\"\"\n",
    "    # Initialize empty DataFrame to store fresh momentum values\n",
    "    fm_df = pd.DataFrame(index=prices_df.index, columns=prices_df.columns, dtype='float64')\n",
    "\n",
    "    # Loop through each coin\n",
    "    for coin in prices_df.columns:\n",
    "        close_prices = prices_df[coin]\n",
    "        \n",
    "        # Calculate forward return over holding period, offset by lookback\n",
    "        returns = close_prices.pct_change(periods=lookback_days).shift(-holding_period)\n",
    "\n",
    "        # Clip to get positive and negative returns\n",
    "        pos_return = returns.clip(lower=0)\n",
    "        neg_return = returns.clip(upper=0)\n",
    "        \n",
    "        # Final fresh momentum = positive minus negative return\n",
    "        fm = pos_return - neg_return\n",
    "        \n",
    "        fm_df[coin] = fm\n",
    "\n",
    "    return fm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f2d52c-e1c0-4399-8e60-1603e47d76fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fresh_momentum_all = compute_fresh_momentum_series(df_short_h[filtered_tickers_h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7cb7ec-3621-4c88-a0d6-96219bcc74d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where all coin momentum values are NaN\n",
    "fresh_momentum_all_cleaned = fresh_momentum_all.dropna(how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787653bf-697d-4d20-8fd8-1aa94b27c310",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Quintile Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bfa32f-4ea7-4793-b259-3f7c47f076c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_iterations = 500\n",
    "forward_periods = 120  # 30 days of 4-hour intervals\n",
    "num_deciles = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74927063-e191-45e8-a57f-d6f2c615811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize structure to store decile returns\n",
    "decile_returns = {i: [] for i in range(1, num_deciles + 1)}\n",
    "\n",
    "# Drop rows where all values are NaN (if not already cleaned)\n",
    "fresh_momentum_all_cleaned = fresh_momentum_all_cleaned.dropna(how='all')\n",
    "\n",
    "# Use index values that have sufficient forward data\n",
    "valid_indices = fresh_momentum_all_cleaned.index[:-forward_periods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad74f346-a189-4ed1-ac17-71a61d7f3e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 500 random iterations\n",
    "for _ in range(num_iterations):\n",
    "    # Randomly choose a date index\n",
    "    rand_date = np.random.choice(valid_indices)\n",
    "    mom_row = fresh_momentum_all_cleaned.loc[rand_date].dropna()\n",
    "\n",
    "    # Skip if not enough coins to form all deciles\n",
    "    if len(mom_row) < num_deciles:\n",
    "        continue\n",
    "\n",
    "    # Sort coins by fresh momentum (descending)\n",
    "    sorted_coins = mom_row.sort_values(ascending=False)\n",
    "\n",
    "    # Size of each decile\n",
    "    decile_size = len(sorted_coins) // num_deciles\n",
    "\n",
    "    # Create deciles and calculate forward returns\n",
    "    for i in range(num_deciles):\n",
    "        decile_coins = sorted_coins.iloc[i * decile_size : (i + 1) * decile_size].index\n",
    "\n",
    "        try:\n",
    "            # Get current and future prices\n",
    "            current_prices = df_short_h.loc[rand_date, decile_coins]\n",
    "            future_index = df_short_h.index.get_loc(rand_date) + forward_periods\n",
    "            future_date = df_short_h.index[future_index]\n",
    "            future_prices = df_short_h.loc[future_date, decile_coins]\n",
    "\n",
    "            # Compute average return of decile\n",
    "            forward_returns = (future_prices - current_prices) / current_prices\n",
    "            avg_return = forward_returns.mean()\n",
    "\n",
    "            # Save the result\n",
    "            decile_returns[i + 1].append(avg_return)\n",
    "        except (KeyError, IndexError):\n",
    "            continue  # Skip iteration if future date is out of bounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe4dc0c-1af6-4142-9d30-08cf9e446a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average returns per decile\n",
    "avg_decile_returns = {\n",
    "    decile: np.mean(returns) if returns else np.nan\n",
    "    for decile, returns in decile_returns.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf8a447-6c65-445c-b381-92043d8fbe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of average returns\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(avg_decile_returns.keys(), avg_decile_returns.values(), color='skyblue')\n",
    "plt.xlabel('Decile (1 = Highest Momentum)')\n",
    "plt.ylabel('Average Forward 30-Day Return')\n",
    "plt.title('Average Forward Returns by Fresh Momentum Decile (500 Samples)')\n",
    "plt.xticks(range(1, num_deciles + 1))\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bd472b-adf6-4a1f-a142-f9111c58cf73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "371b9ee0-d843-4c6c-8529-def46193397f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Optimization & Charting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaf475c-ef4a-4777-9e21-a7d80b7e7f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236bfd10-14d2-4442-98b6-6b5f9a07d080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fresh_momentum_series(prices_df, lookback_days=42, holding_period=30):\n",
    "    \"\"\"\n",
    "    Computes fresh momentum for each coin across all dates.\n",
    "    \n",
    "    Parameters:\n",
    "    - prices_df: DataFrame with datetime index and coin tickers as columns\n",
    "    - lookback_days: Lookback period for calculating return\n",
    "    - holding_period: Holding period for forward return\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame of fresh momentum values for each date and coin\n",
    "    \"\"\"\n",
    "    # Initialize empty DataFrame to store fresh momentum values\n",
    "    fm_df = pd.DataFrame(index=prices_df.index, columns=prices_df.columns, dtype='float64')\n",
    "    \n",
    "    # Loop through each coin\n",
    "    for coin in prices_df.columns:\n",
    "        close_prices = prices_df[coin]\n",
    "        \n",
    "        # Calculate forward return over holding period, offset by lookback\n",
    "        returns = close_prices.pct_change(periods=lookback_days).shift(-holding_period)\n",
    "        # Clip to get positive and negative returns\n",
    "        pos_return = returns.clip(lower=0)\n",
    "        neg_return = returns.clip(upper=0)\n",
    "        \n",
    "        # Final fresh momentum = positive minus negative return\n",
    "        fm = pos_return - neg_return\n",
    "        \n",
    "        fm_df[coin] = fm\n",
    "    \n",
    "    return fm_df\n",
    "\n",
    "def calculate_forward_returns(prices_df, period=90):\n",
    "    \"\"\"\n",
    "    Calculate forward returns for given period\n",
    "    \"\"\"\n",
    "    return prices_df.pct_change(periods=period).shift(-period)\n",
    "\n",
    "def create_deciles(momentum_series):\n",
    "    \"\"\"\n",
    "    Create deciles based on momentum values\n",
    "    Returns dict with decile number as key and coin list as value\n",
    "    \"\"\"\n",
    "    # Remove NaN values\n",
    "    valid_momentum = momentum_series.dropna()\n",
    "    \n",
    "    if len(valid_momentum) == 0:\n",
    "        return {}\n",
    "    \n",
    "    # Sort by momentum values in descending order\n",
    "    sorted_momentum = valid_momentum.sort_values(ascending=False)\n",
    "    \n",
    "    # Create deciles\n",
    "    deciles = {}\n",
    "    n_coins = len(sorted_momentum)\n",
    "    coins_per_decile = max(1, n_coins // 10)  # At least 1 coin per decile\n",
    "    \n",
    "    for i in range(10):\n",
    "        start_idx = i * coins_per_decile\n",
    "        end_idx = min((i + 1) * coins_per_decile, n_coins)\n",
    "        \n",
    "        if start_idx < n_coins:\n",
    "            decile_coins = sorted_momentum.iloc[start_idx:end_idx].index.tolist()\n",
    "            deciles[i + 1] = decile_coins\n",
    "    \n",
    "    return deciles\n",
    "\n",
    "def calculate_performance_metrics(returns_series):\n",
    "    \"\"\"\n",
    "    Calculate performance metrics for a return series\n",
    "    \"\"\"\n",
    "    returns_series = returns_series.dropna()\n",
    "    \n",
    "    if len(returns_series) == 0:\n",
    "        return {\n",
    "            'total_return': 0,\n",
    "            'profit_factor': 1,\n",
    "            'win_loss_ratio': 1,\n",
    "            'max_drawdown': 0\n",
    "        }\n",
    "    \n",
    "    # Total return\n",
    "    total_return = (1 + returns_series).prod() - 1\n",
    "    \n",
    "    # Win/Loss metrics\n",
    "    positive_returns = returns_series[returns_series > 0]\n",
    "    negative_returns = returns_series[returns_series < 0]\n",
    "    \n",
    "    profit_factor = abs(positive_returns.sum() / negative_returns.sum()) if negative_returns.sum() != 0 else np.inf\n",
    "    win_loss_ratio = len(positive_returns) / len(negative_returns) if len(negative_returns) > 0 else np.inf\n",
    "    \n",
    "    # Max drawdown\n",
    "    cumulative_returns = (1 + returns_series).cumprod()\n",
    "    running_max = cumulative_returns.cummax()\n",
    "    drawdown = (cumulative_returns - running_max) / running_max\n",
    "    max_drawdown = drawdown.min()\n",
    "    \n",
    "    return {\n",
    "        'total_return': total_return,\n",
    "        'profit_factor': profit_factor,\n",
    "        'win_loss_ratio': win_loss_ratio,\n",
    "        'max_drawdown': abs(max_drawdown)\n",
    "    }\n",
    "\n",
    "def run_parameter_optimization(prices_df, filtered_tickers, \n",
    "                             lookback_range=(10, 100, 10), \n",
    "                             holding_range=(10, 60, 10),\n",
    "                             n_iterations=500):\n",
    "    \"\"\"\n",
    "    Run parameter optimization for fresh momentum strategy\n",
    "    \n",
    "    Parameters:\n",
    "    - prices_df: DataFrame with price data\n",
    "    - filtered_tickers: List of tickers to analyze\n",
    "    - lookback_range: (start, stop, step) for lookback_days parameter\n",
    "    - holding_range: (start, stop, step) for holding_period parameter\n",
    "    - n_iterations: Number of random samples per parameter combination\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate parameter combinations\n",
    "    lookback_values = range(lookback_range[0], lookback_range[1] + 1, lookback_range[2])\n",
    "    holding_values = range(holding_range[0], holding_range[1] + 1, holding_range[2])\n",
    "    \n",
    "    # Initialize results storage\n",
    "    performance_metrics = []\n",
    "    \n",
    "    total_combinations = len(lookback_values) * len(holding_values)\n",
    "    \n",
    "    print(f\"Testing {total_combinations} parameter combinations with {n_iterations} iterations each...\")\n",
    "    \n",
    "    for lookback_days in tqdm(lookback_values, desc=\"Lookback Days\"):\n",
    "        for holding_period in tqdm(holding_values, desc=\"Holding Period\", leave=False):\n",
    "            \n",
    "            # Calculate fresh momentum for current parameters\n",
    "            fm_df = compute_fresh_momentum_series(prices_df[filtered_tickers], \n",
    "                                                lookback_days=lookback_days, \n",
    "                                                holding_period=holding_period)\n",
    "            \n",
    "            # Calculate forward returns (15 days = 90 periods for 4-hourly data)\n",
    "            forward_returns = calculate_forward_returns(prices_df[filtered_tickers], period=90)\n",
    "            \n",
    "            # Storage for iteration results\n",
    "            iteration_returns = []\n",
    "            \n",
    "            # Run multiple iterations\n",
    "            for iteration in range(n_iterations):\n",
    "                # Get valid dates where we have both momentum and forward return data\n",
    "                valid_dates = fm_df.index.intersection(forward_returns.index)\n",
    "                valid_dates = valid_dates[~fm_df.loc[valid_dates].isna().all(axis=1)]\n",
    "                valid_dates = valid_dates[~forward_returns.loc[valid_dates].isna().all(axis=1)]\n",
    "                \n",
    "                if len(valid_dates) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Randomly select a date\n",
    "                random_date = np.random.choice(valid_dates)\n",
    "                \n",
    "                # Get momentum values for that date\n",
    "                momentum_values = fm_df.loc[random_date]\n",
    "                \n",
    "                # Create deciles\n",
    "                deciles = create_deciles(momentum_values)\n",
    "                \n",
    "                if 1 not in deciles:  # Top decile\n",
    "                    continue\n",
    "                \n",
    "                # Get forward returns for top decile coins\n",
    "                top_decile_coins = deciles[1]\n",
    "                top_decile_returns = forward_returns.loc[random_date, top_decile_coins]\n",
    "                \n",
    "                # Calculate average return for top decile\n",
    "                avg_return = top_decile_returns.mean()\n",
    "                if not np.isnan(avg_return):\n",
    "                    iteration_returns.append(avg_return)\n",
    "            \n",
    "            # Calculate average return across iterations\n",
    "            if len(iteration_returns) > 0:\n",
    "                avg_return_across_iterations = np.mean(iteration_returns)\n",
    "                \n",
    "                # Calculate performance metrics\n",
    "                returns_series = pd.Series(iteration_returns)\n",
    "                metrics = calculate_performance_metrics(returns_series)\n",
    "                \n",
    "                # Store results\n",
    "                result_row = {\n",
    "                    'lookback_days': lookback_days,\n",
    "                    'holding_period': holding_period,\n",
    "                    'avg_return': avg_return_across_iterations,\n",
    "                    'total_return': metrics['total_return'],\n",
    "                    'profit_factor': metrics['profit_factor'],\n",
    "                    'win_loss_ratio': metrics['win_loss_ratio'],\n",
    "                    'max_drawdown': metrics['max_drawdown']\n",
    "                }\n",
    "                \n",
    "                performance_metrics.append(result_row)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(performance_metrics)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def create_interactive_heatmap(results_df):\n",
    "    \"\"\"\n",
    "    Create interactive heatmap of average returns\n",
    "    \"\"\"\n",
    "    pivot_returns = results_df.pivot(index='holding_period', columns='lookback_days', values='avg_return')\n",
    "    \n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=pivot_returns.values,\n",
    "        x=pivot_returns.columns,\n",
    "        y=pivot_returns.index,\n",
    "        colorscale='RdYlGn',\n",
    "        zmid=0,\n",
    "        text=pivot_returns.values,\n",
    "        texttemplate=\"%{text:.4f}\",\n",
    "        textfont={\"size\": 10},\n",
    "        hoverongaps=False,\n",
    "        hovertemplate='<b>Lookback Days</b>: %{x}<br>' +\n",
    "                     '<b>Holding Period</b>: %{y}<br>' +\n",
    "                     '<b>Average Return</b>: %{z:.4f}<extra></extra>'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Average Returns Heatmap (Top Decile) - Interactive',\n",
    "        xaxis_title='Lookback Days',\n",
    "        yaxis_title='Holding Period',\n",
    "        width=900,\n",
    "        height=700,\n",
    "        font=dict(size=12)\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "def create_interactive_line_charts(results_df):\n",
    "    \"\"\"\n",
    "    Create interactive line charts for all performance metrics in single column layout\n",
    "    \"\"\"\n",
    "    fig = make_subplots(\n",
    "        rows=4, cols=1,  # Changed from 2x2 to 4x1\n",
    "        subplot_titles=('Profit Factor vs Lookback Days', 'Win/Loss Ratio vs Lookback Days', \n",
    "                       'Max Drawdown vs Lookback Days', 'Average Return vs Lookback Days'),\n",
    "        vertical_spacing=0.08,  # Reduced spacing since we have more rows\n",
    "        horizontal_spacing=0.08\n",
    "    )\n",
    "    \n",
    "    colors = px.colors.qualitative.Set1\n",
    "    \n",
    "    for i, holding_period in enumerate(sorted(results_df['holding_period'].unique())):\n",
    "        subset = results_df[results_df['holding_period'] == holding_period]\n",
    "        color = colors[i % len(colors)]\n",
    "        \n",
    "        # Profit Factor - Row 1\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=subset['lookback_days'], y=subset['profit_factor'],\n",
    "                      mode='lines+markers', name=f'Holding={holding_period}',\n",
    "                      line=dict(color=color, width=3), \n",
    "                      marker=dict(size=8),\n",
    "                      legendgroup=f'group{i}',\n",
    "                      hovertemplate=f'<b>Lookback</b>: %{{x}}<br>' +\n",
    "                                   f'<b>Holding</b>: {holding_period}<br>' +\n",
    "                                   '<b>Profit Factor</b>: %{y:.4f}<extra></extra>'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Win/Loss Ratio - Row 2\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=subset['lookback_days'], y=subset['win_loss_ratio'],\n",
    "                      mode='lines+markers', name=f'Holding={holding_period}',\n",
    "                      line=dict(color=color, width=3), \n",
    "                      marker=dict(size=8),\n",
    "                      legendgroup=f'group{i}', showlegend=False,\n",
    "                      hovertemplate=f'<b>Lookback</b>: %{{x}}<br>' +\n",
    "                                   f'<b>Holding</b>: {holding_period}<br>' +\n",
    "                                   '<b>Win/Loss Ratio</b>: %{y:.4f}<extra></extra>'),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Max Drawdown - Row 3\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=subset['lookback_days'], y=subset['max_drawdown'],\n",
    "                      mode='lines+markers', name=f'Holding={holding_period}',\n",
    "                      line=dict(color=color, width=3), \n",
    "                      marker=dict(size=8),\n",
    "                      legendgroup=f'group{i}', showlegend=False,\n",
    "                      hovertemplate=f'<b>Lookback</b>: %{{x}}<br>' +\n",
    "                                   f'<b>Holding</b>: {holding_period}<br>' +\n",
    "                                   '<b>Max Drawdown</b>: %{y:.4f}<extra></extra>'),\n",
    "            row=3, col=1\n",
    "        )\n",
    "        \n",
    "        # Average Return - Row 4\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=subset['lookback_days'], y=subset['avg_return'],\n",
    "                      mode='lines+markers', name=f'Holding={holding_period}',\n",
    "                      line=dict(color=color, width=3), \n",
    "                      marker=dict(size=8),\n",
    "                      legendgroup=f'group{i}', showlegend=False,\n",
    "                      hovertemplate=f'<b>Lookback</b>: %{{x}}<br>' +\n",
    "                                   f'<b>Holding</b>: {holding_period}<br>' +\n",
    "                                   '<b>Average Return</b>: %{y:.4f}<extra></extra>'),\n",
    "            row=4, col=1\n",
    "        )\n",
    "    \n",
    "    # Update x-axis titles for all subplots\n",
    "    fig.update_xaxes(title_text=\"Lookback Days\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Lookback Days\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Lookback Days\", row=3, col=1)\n",
    "    fig.update_xaxes(title_text=\"Lookback Days\", row=4, col=1)\n",
    "    \n",
    "    # Update y-axis titles\n",
    "    fig.update_yaxes(title_text=\"Profit Factor\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Win/Loss Ratio\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Max Drawdown\", row=3, col=1)\n",
    "    fig.update_yaxes(title_text=\"Average Return\", row=4, col=1)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Performance Metrics vs Parameters - Interactive (Single Column)',\n",
    "        height=1200,  # Increased height to accommodate 4 rows\n",
    "        width=1000,   # Reduced width since we only have 1 column\n",
    "        hovermode='closest',\n",
    "        font=dict(size=12)\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "def create_individual_metric_charts(results_df):\n",
    "    \"\"\"\n",
    "    Create separate interactive charts for each metric with better zoom control\n",
    "    \"\"\"\n",
    "    metrics = [\n",
    "        ('avg_return', 'Average Return'),\n",
    "        ('profit_factor', 'Profit Factor'), \n",
    "        ('win_loss_ratio', 'Win/Loss Ratio'),\n",
    "        ('max_drawdown', 'Max Drawdown')\n",
    "    ]\n",
    "    \n",
    "    colors = px.colors.qualitative.Set1\n",
    "    \n",
    "    for metric_col, metric_name in metrics:\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        for i, holding_period in enumerate(sorted(results_df['holding_period'].unique())):\n",
    "            subset = results_df[results_df['holding_period'] == holding_period]\n",
    "            \n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=subset['lookback_days'],\n",
    "                y=subset[metric_col],\n",
    "                mode='lines+markers',\n",
    "                name=f'Holding={holding_period}',\n",
    "                line=dict(color=colors[i % len(colors)], width=4),\n",
    "                marker=dict(size=10),\n",
    "                hovertemplate=f'<b>Lookback Days</b>: %{{x}}<br>' +\n",
    "                            f'<b>Holding Period</b>: {holding_period}<br>' +\n",
    "                            f'<b>{metric_name}</b>: %{{y:.4f}}<extra></extra>'\n",
    "            ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f'{metric_name} vs Lookback Days - Interactive Zoom',\n",
    "            xaxis_title='Lookback Days',\n",
    "            yaxis_title=metric_name,\n",
    "            hovermode='closest',\n",
    "            width=1000,\n",
    "            height=600,\n",
    "            showlegend=True,\n",
    "            font=dict(size=12)\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "\n",
    "def create_visualizations(results_df):\n",
    "    \"\"\"\n",
    "    Create all interactive visualizations\n",
    "    \"\"\"\n",
    "    if results_df.empty:\n",
    "        print(\"No results to visualize\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n=== CREATING INTERACTIVE VISUALIZATIONS ===\")\n",
    "    print(\"Features available:\")\n",
    "    print(\"1. Click and drag to zoom into specific areas\")\n",
    "    print(\"2. Double-click to reset zoom\")\n",
    "    print(\"3. Hover over points/areas to see exact values\")\n",
    "    print(\"4. Use toolbar to pan, zoom, and save charts\")\n",
    "    print(\"5. Click legend items to show/hide specific series\\n\")\n",
    "    \n",
    "    # Create heatmap\n",
    "    print(\"Creating interactive heatmap...\")\n",
    "    create_interactive_heatmap(results_df)\n",
    "    \n",
    "    # Create combined line charts\n",
    "    print(\"Creating combined line charts...\")\n",
    "    create_interactive_line_charts(results_df)\n",
    "    \n",
    "    # Create individual metric charts for detailed analysis\n",
    "    print(\"Creating individual metric charts...\")\n",
    "    create_individual_metric_charts(results_df)\n",
    "\n",
    "def find_optimal_parameters(results_df):\n",
    "    \"\"\"\n",
    "    Find optimal parameters based on different criteria\n",
    "    \"\"\"\n",
    "    if results_df.empty:\n",
    "        print(\"No results to analyze\")\n",
    "        return\n",
    "    \n",
    "    print(\"=== OPTIMAL PARAMETER ANALYSIS ===\\n\")\n",
    "    \n",
    "    # Best by average return\n",
    "    best_return_idx = results_df['avg_return'].idxmax()\n",
    "    best_return = results_df.loc[best_return_idx]\n",
    "    print(f\"Best Average Return: {best_return['avg_return']:.4f}\")\n",
    "    print(f\"Parameters: Lookback={best_return['lookback_days']}, Holding={best_return['holding_period']}\")\n",
    "    print(f\"Profit Factor: {best_return['profit_factor']:.4f}\")\n",
    "    print(f\"Win/Loss Ratio: {best_return['win_loss_ratio']:.4f}\")\n",
    "    print(f\"Max Drawdown: {best_return['max_drawdown']:.4f}\\n\")\n",
    "    \n",
    "    # Best by profit factor\n",
    "    best_pf_idx = results_df['profit_factor'].idxmax()\n",
    "    best_pf = results_df.loc[best_pf_idx]\n",
    "    print(f\"Best Profit Factor: {best_pf['profit_factor']:.4f}\")\n",
    "    print(f\"Parameters: Lookback={best_pf['lookback_days']}, Holding={best_pf['holding_period']}\")\n",
    "    print(f\"Average Return: {best_pf['avg_return']:.4f}\")\n",
    "    print(f\"Win/Loss Ratio: {best_pf['win_loss_ratio']:.4f}\")\n",
    "    print(f\"Max Drawdown: {best_pf['max_drawdown']:.4f}\\n\")\n",
    "    \n",
    "    # Best by Sharpe-like ratio (return/drawdown)\n",
    "    results_df['return_drawdown_ratio'] = results_df['avg_return'] / (results_df['max_drawdown'] + 1e-6)\n",
    "    best_sharpe_idx = results_df['return_drawdown_ratio'].idxmax()\n",
    "    best_sharpe = results_df.loc[best_sharpe_idx]\n",
    "    print(f\"Best Return/Drawdown Ratio: {best_sharpe['return_drawdown_ratio']:.4f}\")\n",
    "    print(f\"Parameters: Lookback={best_sharpe['lookback_days']}, Holding={best_sharpe['holding_period']}\")\n",
    "    print(f\"Average Return: {best_sharpe['avg_return']:.4f}\")\n",
    "    print(f\"Profit Factor: {best_sharpe['profit_factor']:.4f}\")\n",
    "    print(f\"Max Drawdown: {best_sharpe['max_drawdown']:.4f}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3868404a-6d97-451f-9634-a148f84f4712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the parameter optimization\n",
    "results_df = run_parameter_optimization(\n",
    "    prices_df=df_short_h, \n",
    "    filtered_tickers=filtered_tickers_h,\n",
    "    lookback_range=(18, 180, 6),  # Test lookback from 3 to 30 days in steps of 1 day\n",
    "    holding_range=(18, 180, 12),    # Test holding from 3 to 30 days in steps of 2 days\n",
    "    n_iterations=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cc6e7b-8ae7-4830-a981-1d6386bd170e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all interactive visualizations\n",
    "create_visualizations(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0048b7e-51a3-43f4-91e4-957af455b0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal parameters\n",
    "find_optimal_parameters(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1bf624-db9c-406f-a8d0-aae77ac213b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "398992e9-03c3-45e6-a7f6-00b09dd6ef38",
   "metadata": {},
   "source": [
    "## Technical Positioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa8b7a4-fac6-48d4-878c-9b3cd6ab3c28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447e1d09-8e08-4701-ac21-c78b2783b86b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d744a2ce-42b8-4434-830d-536a33a998fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691c4e54-4c4d-46b9-935c-620e2dc1c42a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6716cfdb-fd7e-4d0a-9cfc-ed5de2ba4e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae0e20f7-6531-4593-a037-cb4d3ef93c32",
   "metadata": {},
   "source": [
    "## Fundamental Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8fa516-67a8-4136-acaf-0929face6d16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26880a53-d8c9-4d5e-a524-d8cedbd80c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22fd472-9db8-43fd-88b8-dace2afeafdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a0dcfd-c5f5-4f08-b72f-0149a0b640fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b0d29-58b9-4cee-90fe-d9c692538dec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0c9a50-640f-49b0-a86a-1daf9d67b18e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3869a249-e016-43a7-82da-fdc99567fada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69ce6abf-f015-44b4-ae2b-dfc329f2d941",
   "metadata": {},
   "source": [
    "## Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1889c33-5f04-4a23-961f-dc5eb94c8dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3789e1b3-f0a5-4a8b-82ea-e7acfb13eeb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd19eb9c-2faa-4161-9d5e-168dc2ae1bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fc5973-efb3-4f9c-90db-0c6c46e7fa09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9b787cb-09f9-47d7-b4b8-73946770efd4",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adce065-ce8d-4bc0-bb33-450bcc932618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3de583-8b80-478e-b528-1ad651ae2a49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb5a50b-d848-42f2-aef8-24648d07a10b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaa46ed-e556-4aa7-b1cb-87843b149aad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f15031c-e9bd-408c-9aa3-d54bd6857cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac4ccdc-f84b-482a-a901-07dba87095ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeb6214-dd93-4673-8e46-c1c20b3144a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dca914-990a-46cc-9b91-5bebc403c98e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f7cde0-e653-44e4-9357-8dfd7d52742f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99ef870-e520-4a06-ab42-93b5ce24b223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e515ceaf-61cc-4c1b-9699-835fc7444e90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8324440e-ec73-41e4-aecf-b88af7eb42c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458407c7-228f-47b8-a276-21e83125dcfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3b8533-3168-4c84-9f09-e25a78a5be54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c383046-df04-49aa-a211-d999184cb2da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0c4e52-2ae4-4acd-9362-a4bd8bd4bfaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961c7f22-71a5-48b3-9e72-66ac023d4ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5d1cf1-b451-41cc-80d7-0c64da722359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77156aa-4866-47ce-a851-1e06d8dcd063",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
